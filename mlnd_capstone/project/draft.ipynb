{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Street View House Numbers (SVHN) Dataset\n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. The dataset and more information can be founded [`here`](http://ufldl.stanford.edu/housenumbers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from __future__ import print_function\n",
    "from urllib import urlretrieve\n",
    "from scipy.io import loadmat\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equalization(labels, size=1000):\n",
    "    \"\"\"\n",
    "    Selects random data with equal size for each label in the dataset.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    labels: array of the feature's labels.\n",
    "    size: int, size of the adjusted dataset.\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    list of shuffled indexes numbers.\n",
    "    \"\"\"\n",
    "    idx = None\n",
    "    for i in np.unique(labels):\n",
    "        draw_bucket, _ = np.where(labels == i)\n",
    "        samples = np.random.choice(draw_bucket, size=int(size / len(np.unique(labels))))\n",
    "        if idx is None:\n",
    "            idx = samples\n",
    "        else:\n",
    "            idx = np.concatenate((idx, samples))\n",
    "    np.random.shuffle(idx)\n",
    "    return list(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_sample_images(features, labels, n, test_data=None, normalized=False):\n",
    "    \"\"\"\n",
    "    Displays a set of images from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    features: array of images.\n",
    "    labels: array of the feature's labels.\n",
    "    n: int, number of images to display.\n",
    "    normalized: boolean, if True adjusts the function to work with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i + 1)\n",
    "        image_num = np.random.randint(0, high=len(labels))\n",
    "        if test_data is not None:\n",
    "            image_num = test_data\n",
    "        if normalized:\n",
    "            image_num = np.random.randint(0, high=10)\n",
    "            sample_image = np.squeeze(features[image_num], axis=2)\n",
    "        else:\n",
    "            sample_image = features[:,:,:,image_num]\n",
    "        sample_label = labels[image_num]\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.title('Label: {}\\nFormat: {}\\nMin: {:.2f} | Max: {:.2f}'.format(\n",
    "            sample_label,\n",
    "            sample_image.shape,\n",
    "            sample_image.min(),\n",
    "            sample_image.max()))\n",
    "\n",
    "        plt.imshow(sample_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_images(features, new_size=(224,224)):\n",
    "    \"\"\"\n",
    "    Resizes the images in the dataset.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    features: array of RGB images.\n",
    "    new_size: int, size of the resizes images.\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    array of resized images.\n",
    "    \"\"\"\n",
    "    n = features.shape[3]\n",
    "    resized_images = []\n",
    "    for i in range(n):\n",
    "        image = features[:,:,:,i]\n",
    "        image = resize(image, new_size)\n",
    "        resized_images.append(image)\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    labels: array of the feature's labels.\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(labels)):\n",
    "        result.append([1 if labels[i] == j + 1 else 0 for j in range(10)])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_creator(features, labels, batch_size, flatted=False):\n",
    "    \"\"\"\n",
    "    Creates batches to feed the model.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    features: array of features to use as inputs.\n",
    "    labels: array of the feature's labels.\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    generator of batches.\n",
    "    \"\"\"\n",
    "    if flatted:\n",
    "        for start in range(0, len(features), batch_size):\n",
    "            end = min(start + batch_size, len(features))\n",
    "            yield features[start:end,:], labels[start:end]\n",
    "    else:\n",
    "        for start in range(0, features.shape[3], batch_size):\n",
    "            end = min(start + batch_size, features.shape[3])\n",
    "            yield features[:,:,:,start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and understanding the raw data\n",
    "\n",
    "To create a model that can recognize the digit in the images, it is necessary to understand how this dataset is presented and which transformation will be needed, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVHN Dataset: 182MB [00:10, 17.9MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('../data/train_32x32.mat'):\n",
    "    if not os.path.isdir('../data'):\n",
    "        os.mkdir('../data')\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "            '../data/train_32x32.mat',\n",
    "            pbar.hook)\n",
    "\n",
    "svhn_data = loadmat('../data/train_32x32.mat')\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data info:  ['y', 'X', '__version__', '__header__', '__globals__']\n",
      "Features shape:  (32, 32, 3, 73257)\n",
      "Labels shape:  (73257, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Data info: ', svhn_data.keys())\n",
    "print('Features shape: ', svhn_data['X'].shape)\n",
    "print('Labels shape: ', svhn_data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8bUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f022b13aed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = svhn_data['X']\n",
    "labels = svhn_data['y']\n",
    "count_labels = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "plt.barh(np.arange(10), count_labels.values())\n",
    "plt.yticks(np.arange(10), count_labels.keys())\n",
    "plt.title('Number of images per labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAEPCAYAAABY5XmeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf5GwMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f022b1c6b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample_images(features, labels, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the data has 73000+ RGB images of 32 by 32 pixels, but it has much more examples of 1, 2, 3 and 4. Also the 0 digit is represented as 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing and equalizing data\n",
    "\n",
    "Since the dataset has not similar examples of the different labels, I'm going to randomly sort the images from the dataset keeping the same size for each label. I also going to reduce the data that will be used to 10000 images to train the model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eq_idx = equalization(labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  (32, 32, 3, 10000)\n",
      "Labels:  (10000, 1)\n",
      "Label Counts: {1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000, 10: 1000}\n"
     ]
    }
   ],
   "source": [
    "red_features = features[:,:,:,eq_idx]\n",
    "red_labels = labels[eq_idx]\n",
    "\n",
    "print('Features: ', red_features.shape)\n",
    "print('Labels: ', red_labels.shape)\n",
    "print('Label Counts: {}'.format(dict(zip(*np.unique(red_labels, return_counts=True)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing features with pre trained VGG16\n",
    "\n",
    "To recognize the digit in the images I'll use the VGG16 architecture, but since that model takes a long time to train I'll use a pre trained model and use the first fully connected layer result to train the last two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tensorflow_vgg'...\n",
      "remote: Counting objects: 109, done.\u001b[K\n",
      "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 109\u001b[K\n",
      "Receiving objects: 100% (109/109), 56.61 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (58/58), done.\n",
      "Checking connectivity... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VGG16 Parameters: 553MB [00:08, 62.5MB/s]                              \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow_vvg import vgg16\n",
    "except:\n",
    "    !git clone https://github.com/machrisaa/tensorflow-vgg.git tensorflow_vgg\n",
    "    from tensorflow_vgg import vgg16\n",
    "\n",
    "with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "    urlretrieve(\n",
    "        'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "        './tensorflow_vgg/vgg16.npy',\n",
    "        pbar.hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/carnd/machine-learning/mlnd_capstone/project/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 0s\n",
      "Processing batch #  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/mlcap/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch #  1\n",
      "Processing batch #  2\n",
      "Processing batch #  3\n",
      "Processing batch #  4\n",
      "Processing batch #  5\n",
      "Processing batch #  6\n",
      "Processing batch #  7\n",
      "Processing batch #  8\n",
      "Processing batch #  9\n",
      "Processing batch # 10\n",
      "Processing batch # 11\n",
      "Processing batch # 12\n",
      "Processing batch # 13\n",
      "Processing batch # 14\n",
      "Processing batch # 15\n",
      "Processing batch # 16\n",
      "Processing batch # 17\n",
      "Processing batch # 18\n",
      "Processing batch # 19\n",
      "Processing batch # 20\n",
      "Processing batch # 21\n",
      "Processing batch # 22\n",
      "Processing batch # 23\n",
      "Processing batch # 24\n",
      "Processing batch # 25\n",
      "Processing batch # 26\n",
      "Processing batch # 27\n",
      "Processing batch # 28\n",
      "Processing batch # 29\n",
      "Processing batch # 30\n",
      "Processing batch # 31\n",
      "Processing batch # 32\n",
      "Processing batch # 33\n",
      "Processing batch # 34\n",
      "Processing batch # 35\n",
      "Processing batch # 36\n",
      "Processing batch # 37\n",
      "Processing batch # 38\n",
      "Processing batch # 39\n",
      "Processing batch # 40\n",
      "Processing batch # 41\n",
      "Processing batch # 42\n",
      "Processing batch # 43\n",
      "Processing batch # 44\n",
      "Processing batch # 45\n",
      "Processing batch # 46\n",
      "Processing batch # 47\n",
      "Processing batch # 48\n",
      "Processing batch # 49\n",
      "Processing batch # 50\n",
      "Processing batch # 51\n",
      "Processing batch # 52\n",
      "Processing batch # 53\n",
      "Processing batch # 54\n",
      "Processing batch # 55\n",
      "Processing batch # 56\n",
      "Processing batch # 57\n",
      "Processing batch # 58\n",
      "Processing batch # 59\n",
      "Processing batch # 60\n",
      "Processing batch # 61\n",
      "Processing batch # 62\n",
      "Processing batch # 63\n",
      "Processing batch # 64\n",
      "Processing batch # 65\n",
      "Processing batch # 66\n",
      "Processing batch # 67\n",
      "Processing batch # 68\n",
      "Processing batch # 69\n",
      "Processing batch # 70\n",
      "Processing batch # 71\n",
      "Processing batch # 72\n",
      "Processing batch # 73\n",
      "Processing batch # 74\n",
      "Processing batch # 75\n",
      "Processing batch # 76\n",
      "Processing batch # 77\n",
      "Processing batch # 78\n",
      "Processing batch # 79\n",
      "Processing batch # 80\n",
      "Processing batch # 81\n",
      "Processing batch # 82\n",
      "Processing batch # 83\n",
      "Processing batch # 84\n",
      "Processing batch # 85\n",
      "Processing batch # 86\n",
      "Processing batch # 87\n",
      "Processing batch # 88\n",
      "Processing batch # 89\n",
      "Processing batch # 90\n",
      "Processing batch # 91\n",
      "Processing batch # 92\n",
      "Processing batch # 93\n",
      "Processing batch # 94\n",
      "Processing batch # 95\n",
      "Processing batch # 96\n",
      "Processing batch # 97\n",
      "Processing batch # 98\n",
      "Processing batch # 99\n",
      "Processing batch #100\n",
      "Processing batch #101\n",
      "Processing batch #102\n",
      "Processing batch #103\n",
      "Processing batch #104\n",
      "Processing batch #105\n",
      "Processing batch #106\n",
      "Processing batch #107\n",
      "Processing batch #108\n",
      "Processing batch #109\n",
      "Processing batch #110\n",
      "Processing batch #111\n",
      "Processing batch #112\n",
      "Processing batch #113\n",
      "Processing batch #114\n",
      "Processing batch #115\n",
      "Processing batch #116\n",
      "Processing batch #117\n",
      "Processing batch #118\n",
      "Processing batch #119\n",
      "Processing batch #120\n",
      "Processing batch #121\n",
      "Processing batch #122\n",
      "Processing batch #123\n",
      "Processing batch #124\n",
      "Processing batch #125\n",
      "Processing batch #126\n",
      "Processing batch #127\n",
      "Processing batch #128\n",
      "Processing batch #129\n",
      "Processing batch #130\n",
      "Processing batch #131\n",
      "Processing batch #132\n",
      "Processing batch #133\n",
      "Processing batch #134\n",
      "Processing batch #135\n",
      "Processing batch #136\n",
      "Processing batch #137\n",
      "Processing batch #138\n",
      "Processing batch #139\n",
      "Processing batch #140\n",
      "Processing batch #141\n",
      "Processing batch #142\n",
      "Processing batch #143\n",
      "Processing batch #144\n",
      "Processing batch #145\n",
      "Processing batch #146\n",
      "Processing batch #147\n",
      "Processing batch #148\n",
      "Processing batch #149\n",
      "Processing batch #150\n",
      "Processing batch #151\n",
      "Processing batch #152\n",
      "Processing batch #153\n",
      "Processing batch #154\n",
      "Processing batch #155\n",
      "Processing batch #156\n",
      "Processing batch #157\n",
      "Processing batch #158\n",
      "Processing batch #159\n",
      "Processing batch #160\n",
      "Processing batch #161\n",
      "Processing batch #162\n",
      "Processing batch #163\n",
      "Processing batch #164\n",
      "Processing batch #165\n",
      "Processing batch #166\n",
      "Processing batch #167\n",
      "Processing batch #168\n",
      "Processing batch #169\n",
      "Processing batch #170\n",
      "Processing batch #171\n",
      "Processing batch #172\n",
      "Processing batch #173\n",
      "Processing batch #174\n",
      "Processing batch #175\n",
      "Processing batch #176\n",
      "Processing batch #177\n",
      "Processing batch #178\n",
      "Processing batch #179\n",
      "Processing batch #180\n",
      "Processing batch #181\n",
      "Processing batch #182\n",
      "Processing batch #183\n",
      "Processing batch #184\n",
      "Processing batch #185\n",
      "Processing batch #186\n",
      "Processing batch #187\n",
      "Processing batch #188\n",
      "Processing batch #189\n",
      "Processing batch #190\n",
      "Processing batch #191\n",
      "Processing batch #192\n",
      "Processing batch #193\n",
      "Processing batch #194\n",
      "Processing batch #195\n",
      "Processing batch #196\n",
      "Processing batch #197\n",
      "Processing batch #198\n",
      "Processing batch #199\n",
      "Processing batch #200\n",
      "Processing batch #201\n",
      "Processing batch #202\n",
      "Processing batch #203\n",
      "Processing batch #204\n",
      "Processing batch #205\n",
      "Processing batch #206\n",
      "Processing batch #207\n",
      "Processing batch #208\n",
      "Processing batch #209\n",
      "Processing batch #210\n",
      "Processing batch #211\n",
      "Processing batch #212\n",
      "Processing batch #213\n",
      "Processing batch #214\n",
      "Processing batch #215\n",
      "Processing batch #216\n",
      "Processing batch #217\n",
      "Processing batch #218\n",
      "Processing batch #219\n",
      "Processing batch #220\n",
      "Processing batch #221\n",
      "Processing batch #222\n",
      "Processing batch #223\n",
      "Processing batch #224\n",
      "Processing batch #225\n",
      "Processing batch #226\n",
      "Processing batch #227\n",
      "Processing batch #228\n",
      "Processing batch #229\n",
      "Processing batch #230\n",
      "Processing batch #231\n",
      "Processing batch #232\n",
      "Processing batch #233\n",
      "Processing batch #234\n",
      "Processing batch #235\n",
      "Processing batch #236\n",
      "Processing batch #237\n",
      "Processing batch #238\n",
      "Processing batch #239\n",
      "Processing batch #240\n",
      "Processing batch #241\n",
      "Processing batch #242\n",
      "Processing batch #243\n",
      "Processing batch #244\n",
      "Processing batch #245\n",
      "Processing batch #246\n",
      "Processing batch #247\n",
      "Processing batch #248\n",
      "Processing batch #249\n",
      "Processing batch #250\n",
      "Processing batch #251\n",
      "Processing batch #252\n",
      "Processing batch #253\n",
      "Processing batch #254\n",
      "Processing batch #255\n",
      "Processing batch #256\n",
      "Processing batch #257\n",
      "Processing batch #258\n",
      "Processing batch #259\n",
      "Processing batch #260\n",
      "Processing batch #261\n",
      "Processing batch #262\n",
      "Processing batch #263\n",
      "Processing batch #264\n",
      "Processing batch #265\n",
      "Processing batch #266\n",
      "Processing batch #267\n",
      "Processing batch #268\n",
      "Processing batch #269\n",
      "Processing batch #270\n",
      "Processing batch #271\n",
      "Processing batch #272\n",
      "Processing batch #273\n",
      "Processing batch #274\n",
      "Processing batch #275\n",
      "Processing batch #276\n",
      "Processing batch #277\n",
      "Processing batch #278\n",
      "Processing batch #279\n",
      "Processing batch #280\n",
      "Processing batch #281\n",
      "Processing batch #282\n",
      "Processing batch #283\n",
      "Processing batch #284\n",
      "Processing batch #285\n",
      "Processing batch #286\n",
      "Processing batch #287\n",
      "Processing batch #288\n",
      "Processing batch #289\n",
      "Processing batch #290\n",
      "Processing batch #291\n",
      "Processing batch #292\n",
      "Processing batch #293\n",
      "Processing batch #294\n",
      "Processing batch #295\n",
      "Processing batch #296\n",
      "Processing batch #297\n",
      "Processing batch #298\n",
      "Processing batch #299\n",
      "Processing batch #300\n",
      "Processing batch #301\n",
      "Processing batch #302\n",
      "Processing batch #303\n",
      "Processing batch #304\n",
      "Processing batch #305\n",
      "Processing batch #306\n",
      "Processing batch #307\n",
      "Processing batch #308\n",
      "Processing batch #309\n",
      "Processing batch #310\n",
      "Processing batch #311\n",
      "Processing batch #312\n",
      "Processing batch #313\n",
      "Processing batch #314\n",
      "Processing batch #315\n",
      "Processing batch #316\n",
      "Processing batch #317\n",
      "Processing batch #318\n",
      "Processing batch #319\n",
      "Processing batch #320\n",
      "Processing batch #321\n",
      "Processing batch #322\n",
      "Processing batch #323\n",
      "Processing batch #324\n",
      "Processing batch #325\n",
      "Processing batch #326\n",
      "Processing batch #327\n",
      "Processing batch #328\n",
      "Processing batch #329\n",
      "Processing batch #330\n",
      "Processing batch #331\n",
      "Processing batch #332\n",
      "Processing batch #333\n",
      "Processing batch #334\n",
      "Processing batch #335\n",
      "Processing batch #336\n",
      "Processing batch #337\n",
      "Processing batch #338\n",
      "Processing batch #339\n",
      "Processing batch #340\n",
      "Processing batch #341\n",
      "Processing batch #342\n",
      "Processing batch #343\n",
      "Processing batch #344\n",
      "Processing batch #345\n",
      "Processing batch #346\n",
      "Processing batch #347\n",
      "Processing batch #348\n",
      "Processing batch #349\n",
      "Processing batch #350\n",
      "Processing batch #351\n",
      "Processing batch #352\n",
      "Processing batch #353\n",
      "Processing batch #354\n",
      "Processing batch #355\n",
      "Processing batch #356\n",
      "Processing batch #357\n",
      "Processing batch #358\n",
      "Processing batch #359\n",
      "Processing batch #360\n",
      "Processing batch #361\n",
      "Processing batch #362\n",
      "Processing batch #363\n",
      "Processing batch #364\n",
      "Processing batch #365\n",
      "Processing batch #366\n",
      "Processing batch #367\n",
      "Processing batch #368\n",
      "Processing batch #369\n",
      "Processing batch #370\n",
      "Processing batch #371\n",
      "Processing batch #372\n",
      "Processing batch #373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch #374\n",
      "Processing batch #375\n",
      "Processing batch #376\n",
      "Processing batch #377\n",
      "Processing batch #378\n",
      "Processing batch #379\n",
      "Processing batch #380\n",
      "Processing batch #381\n",
      "Processing batch #382\n",
      "Processing batch #383\n",
      "Processing batch #384\n",
      "Processing batch #385\n",
      "Processing batch #386\n",
      "Processing batch #387\n",
      "Processing batch #388\n",
      "Processing batch #389\n",
      "Processing batch #390\n",
      "Processing batch #391\n",
      "Processing batch #392\n",
      "Processing batch #393\n",
      "Processing batch #394\n",
      "Processing batch #395\n",
      "Processing batch #396\n",
      "Processing batch #397\n",
      "Processing batch #398\n",
      "Processing batch #399\n",
      "Processing batch #400\n",
      "Processing batch #401\n",
      "Processing batch #402\n",
      "Processing batch #403\n",
      "Processing batch #404\n",
      "Processing batch #405\n",
      "Processing batch #406\n",
      "Processing batch #407\n",
      "Processing batch #408\n",
      "Processing batch #409\n",
      "Processing batch #410\n",
      "Processing batch #411\n",
      "Processing batch #412\n",
      "Processing batch #413\n",
      "Processing batch #414\n",
      "Processing batch #415\n",
      "Processing batch #416\n",
      "Processing batch #417\n",
      "Processing batch #418\n",
      "Processing batch #419\n",
      "Processing batch #420\n",
      "Processing batch #421\n",
      "Processing batch #422\n",
      "Processing batch #423\n",
      "Processing batch #424\n",
      "Processing batch #425\n",
      "Processing batch #426\n",
      "Processing batch #427\n",
      "Processing batch #428\n",
      "Processing batch #429\n",
      "Processing batch #430\n",
      "Processing batch #431\n",
      "Processing batch #432\n",
      "Processing batch #433\n",
      "Processing batch #434\n",
      "Processing batch #435\n",
      "Processing batch #436\n",
      "Processing batch #437\n",
      "Processing batch #438\n",
      "Processing batch #439\n",
      "Processing batch #440\n",
      "Processing batch #441\n",
      "Processing batch #442\n",
      "Processing batch #443\n",
      "Processing batch #444\n",
      "Processing batch #445\n",
      "Processing batch #446\n",
      "Processing batch #447\n",
      "Processing batch #448\n",
      "Processing batch #449\n",
      "Processing batch #450\n",
      "Processing batch #451\n",
      "Processing batch #452\n",
      "Processing batch #453\n",
      "Processing batch #454\n",
      "Processing batch #455\n",
      "Processing batch #456\n",
      "Processing batch #457\n",
      "Processing batch #458\n",
      "Processing batch #459\n",
      "Processing batch #460\n",
      "Processing batch #461\n",
      "Processing batch #462\n",
      "Processing batch #463\n",
      "Processing batch #464\n",
      "Processing batch #465\n",
      "Processing batch #466\n",
      "Processing batch #467\n",
      "Processing batch #468\n",
      "Processing batch #469\n",
      "Processing batch #470\n",
      "Processing batch #471\n",
      "Processing batch #472\n",
      "Processing batch #473\n",
      "Processing batch #474\n",
      "Processing batch #475\n",
      "Processing batch #476\n",
      "Processing batch #477\n",
      "Processing batch #478\n",
      "Processing batch #479\n",
      "Processing batch #480\n",
      "Processing batch #481\n",
      "Processing batch #482\n",
      "Processing batch #483\n",
      "Processing batch #484\n",
      "Processing batch #485\n",
      "Processing batch #486\n",
      "Processing batch #487\n",
      "Processing batch #488\n",
      "Processing batch #489\n",
      "Processing batch #490\n",
      "Processing batch #491\n",
      "Processing batch #492\n",
      "Processing batch #493\n",
      "Processing batch #494\n",
      "Processing batch #495\n",
      "Processing batch #496\n",
      "Processing batch #497\n",
      "Processing batch #498\n",
      "Processing batch #499\n",
      "Processing batch #500\n",
      "Processing batch #501\n",
      "Processing batch #502\n",
      "Processing batch #503\n",
      "Processing batch #504\n",
      "Processing batch #505\n",
      "Processing batch #506\n",
      "Processing batch #507\n",
      "Processing batch #508\n",
      "Processing batch #509\n",
      "Processing batch #510\n",
      "Processing batch #511\n",
      "Processing batch #512\n",
      "Processing batch #513\n",
      "Processing batch #514\n",
      "Processing batch #515\n",
      "Processing batch #516\n",
      "Processing batch #517\n",
      "Processing batch #518\n",
      "Processing batch #519\n",
      "Processing batch #520\n",
      "Processing batch #521\n",
      "Processing batch #522\n",
      "Processing batch #523\n",
      "Processing batch #524\n",
      "Processing batch #525\n",
      "Processing batch #526\n",
      "Processing batch #527\n",
      "Processing batch #528\n",
      "Processing batch #529\n",
      "Processing batch #530\n",
      "Processing batch #531\n",
      "Processing batch #532\n",
      "Processing batch #533\n",
      "Processing batch #534\n",
      "Processing batch #535\n",
      "Processing batch #536\n",
      "Processing batch #537\n",
      "Processing batch #538\n",
      "Processing batch #539\n",
      "Processing batch #540\n",
      "Processing batch #541\n",
      "Processing batch #542\n",
      "Processing batch #543\n",
      "Processing batch #544\n",
      "Processing batch #545\n",
      "Processing batch #546\n",
      "Processing batch #547\n",
      "Processing batch #548\n",
      "Processing batch #549\n",
      "Processing batch #550\n",
      "Processing batch #551\n",
      "Processing batch #552\n",
      "Processing batch #553\n",
      "Processing batch #554\n",
      "Processing batch #555\n",
      "Processing batch #556\n",
      "Processing batch #557\n",
      "Processing batch #558\n",
      "Processing batch #559\n",
      "Processing batch #560\n",
      "Processing batch #561\n",
      "Processing batch #562\n",
      "Processing batch #563\n",
      "Processing batch #564\n",
      "Processing batch #565\n",
      "Processing batch #566\n",
      "Processing batch #567\n",
      "Processing batch #568\n",
      "Processing batch #569\n",
      "Processing batch #570\n",
      "Processing batch #571\n",
      "Processing batch #572\n",
      "Processing batch #573\n",
      "Processing batch #574\n",
      "Processing batch #575\n",
      "Processing batch #576\n",
      "Processing batch #577\n",
      "Processing batch #578\n",
      "Processing batch #579\n",
      "Processing batch #580\n",
      "Processing batch #581\n",
      "Processing batch #582\n",
      "Processing batch #583\n",
      "Processing batch #584\n",
      "Processing batch #585\n",
      "Processing batch #586\n",
      "Processing batch #587\n",
      "Processing batch #588\n",
      "Processing batch #589\n",
      "Processing batch #590\n",
      "Processing batch #591\n",
      "Processing batch #592\n",
      "Processing batch #593\n",
      "Processing batch #594\n",
      "Processing batch #595\n",
      "Processing batch #596\n",
      "Processing batch #597\n",
      "Processing batch #598\n",
      "Processing batch #599\n",
      "Processing batch #600\n",
      "Processing batch #601\n",
      "Processing batch #602\n",
      "Processing batch #603\n",
      "Processing batch #604\n",
      "Processing batch #605\n",
      "Processing batch #606\n",
      "Processing batch #607\n",
      "Processing batch #608\n",
      "Processing batch #609\n",
      "Processing batch #610\n",
      "Processing batch #611\n",
      "Processing batch #612\n",
      "Processing batch #613\n",
      "Processing batch #614\n",
      "Processing batch #615\n",
      "Processing batch #616\n",
      "Processing batch #617\n",
      "Processing batch #618\n",
      "Processing batch #619\n",
      "Processing batch #620\n",
      "Processing batch #621\n",
      "Processing batch #622\n",
      "Processing batch #623\n",
      "Processing batch #624\n",
      "Processing batch #625\n",
      "Processing batch #626\n",
      "Processing batch #627\n",
      "Processing batch #628\n",
      "Processing batch #629\n",
      "Processing batch #630\n",
      "Processing batch #631\n",
      "Processing batch #632\n",
      "Processing batch #633\n",
      "Processing batch #634\n",
      "Processing batch #635\n",
      "Processing batch #636\n",
      "Processing batch #637\n",
      "Processing batch #638\n",
      "Processing batch #639\n",
      "Processing batch #640\n",
      "Processing batch #641\n",
      "Processing batch #642\n",
      "Processing batch #643\n",
      "Processing batch #644\n",
      "Processing batch #645\n",
      "Processing batch #646\n",
      "Processing batch #647\n",
      "Processing batch #648\n",
      "Processing batch #649\n",
      "Processing batch #650\n",
      "Processing batch #651\n",
      "Processing batch #652\n",
      "Processing batch #653\n",
      "Processing batch #654\n",
      "Processing batch #655\n",
      "Processing batch #656\n",
      "Processing batch #657\n",
      "Processing batch #658\n",
      "Processing batch #659\n",
      "Processing batch #660\n",
      "Processing batch #661\n",
      "Processing batch #662\n",
      "Processing batch #663\n",
      "Processing batch #664\n",
      "Processing batch #665\n",
      "Processing batch #666\n",
      "Processing batch #667\n",
      "Processing batch #668\n",
      "Processing batch #669\n",
      "Processing batch #670\n",
      "Processing batch #671\n",
      "Processing batch #672\n",
      "Processing batch #673\n",
      "Processing batch #674\n",
      "Processing batch #675\n",
      "Processing batch #676\n",
      "Processing batch #677\n",
      "Processing batch #678\n",
      "Processing batch #679\n",
      "Processing batch #680\n",
      "Processing batch #681\n",
      "Processing batch #682\n",
      "Processing batch #683\n",
      "Processing batch #684\n",
      "Processing batch #685\n",
      "Processing batch #686\n",
      "Processing batch #687\n",
      "Processing batch #688\n",
      "Processing batch #689\n",
      "Processing batch #690\n",
      "Processing batch #691\n",
      "Processing batch #692\n",
      "Processing batch #693\n",
      "Processing batch #694\n",
      "Processing batch #695\n",
      "Processing batch #696\n",
      "Processing batch #697\n",
      "Processing batch #698\n",
      "Processing batch #699\n",
      "Processing batch #700\n",
      "Processing batch #701\n",
      "Processing batch #702\n",
      "Processing batch #703\n",
      "Processing batch #704\n",
      "Processing batch #705\n",
      "Processing batch #706\n",
      "Processing batch #707\n",
      "Processing batch #708\n",
      "Processing batch #709\n",
      "Processing batch #710\n",
      "Processing batch #711\n",
      "Processing batch #712\n",
      "Processing batch #713\n",
      "Processing batch #714\n",
      "Processing batch #715\n",
      "Processing batch #716\n",
      "Processing batch #717\n",
      "Processing batch #718\n",
      "Processing batch #719\n",
      "Processing batch #720\n",
      "Processing batch #721\n",
      "Processing batch #722\n",
      "Processing batch #723\n",
      "Processing batch #724\n",
      "Processing batch #725\n",
      "Processing batch #726\n",
      "Processing batch #727\n",
      "Processing batch #728\n",
      "Processing batch #729\n",
      "Processing batch #730\n",
      "Processing batch #731\n",
      "Processing batch #732\n",
      "Processing batch #733\n",
      "Processing batch #734\n",
      "Processing batch #735\n",
      "Processing batch #736\n",
      "Processing batch #737\n",
      "Processing batch #738\n",
      "Processing batch #739\n",
      "Processing batch #740\n",
      "Processing batch #741\n",
      "Processing batch #742\n",
      "Processing batch #743\n",
      "Processing batch #744\n",
      "Processing batch #745\n",
      "Processing batch #746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch #747\n",
      "Processing batch #748\n",
      "Processing batch #749\n",
      "Processing batch #750\n",
      "Processing batch #751\n",
      "Processing batch #752\n",
      "Processing batch #753\n",
      "Processing batch #754\n",
      "Processing batch #755\n",
      "Processing batch #756\n",
      "Processing batch #757\n",
      "Processing batch #758\n",
      "Processing batch #759\n",
      "Processing batch #760\n",
      "Processing batch #761\n",
      "Processing batch #762\n",
      "Processing batch #763\n",
      "Processing batch #764\n",
      "Processing batch #765\n",
      "Processing batch #766\n",
      "Processing batch #767\n",
      "Processing batch #768\n",
      "Processing batch #769\n",
      "Processing batch #770\n",
      "Processing batch #771\n",
      "Processing batch #772\n",
      "Processing batch #773\n",
      "Processing batch #774\n",
      "Processing batch #775\n",
      "Processing batch #776\n",
      "Processing batch #777\n",
      "Processing batch #778\n",
      "Processing batch #779\n",
      "Processing batch #780\n",
      "Processing batch #781\n",
      "Processing batch #782\n",
      "Processing batch #783\n",
      "Processing batch #784\n",
      "Processing batch #785\n",
      "Processing batch #786\n",
      "Processing batch #787\n",
      "Processing batch #788\n",
      "Processing batch #789\n",
      "Processing batch #790\n",
      "Processing batch #791\n",
      "Processing batch #792\n",
      "Processing batch #793\n",
      "Processing batch #794\n",
      "Processing batch #795\n",
      "Processing batch #796\n",
      "Processing batch #797\n",
      "Processing batch #798\n",
      "Processing batch #799\n",
      "Processing batch #800\n",
      "Processing batch #801\n",
      "Processing batch #802\n",
      "Processing batch #803\n",
      "Processing batch #804\n",
      "Processing batch #805\n",
      "Processing batch #806\n",
      "Processing batch #807\n",
      "Processing batch #808\n",
      "Processing batch #809\n",
      "Processing batch #810\n",
      "Processing batch #811\n",
      "Processing batch #812\n",
      "Processing batch #813\n",
      "Processing batch #814\n",
      "Processing batch #815\n",
      "Processing batch #816\n",
      "Processing batch #817\n",
      "Processing batch #818\n",
      "Processing batch #819\n",
      "Processing batch #820\n",
      "Processing batch #821\n",
      "Processing batch #822\n",
      "Processing batch #823\n",
      "Processing batch #824\n",
      "Processing batch #825\n",
      "Processing batch #826\n",
      "Processing batch #827\n",
      "Processing batch #828\n",
      "Processing batch #829\n",
      "Processing batch #830\n",
      "Processing batch #831\n",
      "Processing batch #832\n",
      "Processing batch #833\n",
      "Processing batch #834\n",
      "Processing batch #835\n",
      "Processing batch #836\n",
      "Processing batch #837\n",
      "Processing batch #838\n",
      "Processing batch #839\n",
      "Processing batch #840\n",
      "Processing batch #841\n",
      "Processing batch #842\n",
      "Processing batch #843\n",
      "Processing batch #844\n",
      "Processing batch #845\n",
      "Processing batch #846\n",
      "Processing batch #847\n",
      "Processing batch #848\n",
      "Processing batch #849\n",
      "Processing batch #850\n",
      "Processing batch #851\n",
      "Processing batch #852\n",
      "Processing batch #853\n",
      "Processing batch #854\n",
      "Processing batch #855\n",
      "Processing batch #856\n",
      "Processing batch #857\n",
      "Processing batch #858\n",
      "Processing batch #859\n",
      "Processing batch #860\n",
      "Processing batch #861\n",
      "Processing batch #862\n",
      "Processing batch #863\n",
      "Processing batch #864\n",
      "Processing batch #865\n",
      "Processing batch #866\n",
      "Processing batch #867\n",
      "Processing batch #868\n",
      "Processing batch #869\n",
      "Processing batch #870\n",
      "Processing batch #871\n",
      "Processing batch #872\n",
      "Processing batch #873\n",
      "Processing batch #874\n",
      "Processing batch #875\n",
      "Processing batch #876\n",
      "Processing batch #877\n",
      "Processing batch #878\n",
      "Processing batch #879\n",
      "Processing batch #880\n",
      "Processing batch #881\n",
      "Processing batch #882\n",
      "Processing batch #883\n",
      "Processing batch #884\n",
      "Processing batch #885\n",
      "Processing batch #886\n",
      "Processing batch #887\n",
      "Processing batch #888\n",
      "Processing batch #889\n",
      "Processing batch #890\n",
      "Processing batch #891\n",
      "Processing batch #892\n",
      "Processing batch #893\n",
      "Processing batch #894\n",
      "Processing batch #895\n",
      "Processing batch #896\n",
      "Processing batch #897\n",
      "Processing batch #898\n",
      "Processing batch #899\n",
      "Processing batch #900\n",
      "Processing batch #901\n",
      "Processing batch #902\n",
      "Processing batch #903\n",
      "Processing batch #904\n",
      "Processing batch #905\n",
      "Processing batch #906\n",
      "Processing batch #907\n",
      "Processing batch #908\n",
      "Processing batch #909\n",
      "Processing batch #910\n",
      "Processing batch #911\n",
      "Processing batch #912\n",
      "Processing batch #913\n",
      "Processing batch #914\n",
      "Processing batch #915\n",
      "Processing batch #916\n",
      "Processing batch #917\n",
      "Processing batch #918\n",
      "Processing batch #919\n",
      "Processing batch #920\n",
      "Processing batch #921\n",
      "Processing batch #922\n",
      "Processing batch #923\n",
      "Processing batch #924\n",
      "Processing batch #925\n",
      "Processing batch #926\n",
      "Processing batch #927\n",
      "Processing batch #928\n",
      "Processing batch #929\n",
      "Processing batch #930\n",
      "Processing batch #931\n",
      "Processing batch #932\n",
      "Processing batch #933\n",
      "Processing batch #934\n",
      "Processing batch #935\n",
      "Processing batch #936\n",
      "Processing batch #937\n",
      "Processing batch #938\n",
      "Processing batch #939\n",
      "Processing batch #940\n",
      "Processing batch #941\n",
      "Processing batch #942\n",
      "Processing batch #943\n",
      "Processing batch #944\n",
      "Processing batch #945\n",
      "Processing batch #946\n",
      "Processing batch #947\n",
      "Processing batch #948\n",
      "Processing batch #949\n",
      "Processing batch #950\n",
      "Processing batch #951\n",
      "Processing batch #952\n",
      "Processing batch #953\n",
      "Processing batch #954\n",
      "Processing batch #955\n",
      "Processing batch #956\n",
      "Processing batch #957\n",
      "Processing batch #958\n",
      "Processing batch #959\n",
      "Processing batch #960\n",
      "Processing batch #961\n",
      "Processing batch #962\n",
      "Processing batch #963\n",
      "Processing batch #964\n",
      "Processing batch #965\n",
      "Processing batch #966\n",
      "Processing batch #967\n",
      "Processing batch #968\n",
      "Processing batch #969\n",
      "Processing batch #970\n",
      "Processing batch #971\n",
      "Processing batch #972\n",
      "Processing batch #973\n",
      "Processing batch #974\n",
      "Processing batch #975\n",
      "Processing batch #976\n",
      "Processing batch #977\n",
      "Processing batch #978\n",
      "Processing batch #979\n",
      "Processing batch #980\n",
      "Processing batch #981\n",
      "Processing batch #982\n",
      "Processing batch #983\n",
      "Processing batch #984\n",
      "Processing batch #985\n",
      "Processing batch #986\n",
      "Processing batch #987\n",
      "Processing batch #988\n",
      "Processing batch #989\n",
      "Processing batch #990\n",
      "Processing batch #991\n",
      "Processing batch #992\n",
      "Processing batch #993\n",
      "Processing batch #994\n",
      "Processing batch #995\n",
      "Processing batch #996\n",
      "Processing batch #997\n",
      "Processing batch #998\n",
      "Processing batch #999\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./code_data.pkl'):\n",
    "    batch_size = 10\n",
    "    iteration = 0\n",
    "    codes = None\n",
    "    with tf.Session() as sess:\n",
    "        vgg = vgg16.Vgg16()\n",
    "        images = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        with tf.name_scope('content_vgg'):\n",
    "            vgg.build(images)\n",
    "    \n",
    "        for batch, _ in batch_creator(red_features, red_labels, batch_size):\n",
    "            print('Processing batch #{:3d}'.format(iteration))\n",
    "            resized_images = resize_images(batch)\n",
    "            feed_dict = {images: resized_images}\n",
    "            codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "            if codes is None:\n",
    "                codes = codes_batch\n",
    "            else:\n",
    "                codes = np.concatenate((codes, codes_batch))\n",
    "            iteration += 1\n",
    "        \n",
    "    with open('./code_data.pkl', 'wb') as f:\n",
    "        pickle.dump(codes, f)\n",
    "else:\n",
    "    with open('./code_data.pkl', 'rb') as f:\n",
    "        codes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(codes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot enconding the labels\n",
    "\n",
    "Also we need to one hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "one_hot_labels = one_hot_encode(red_labels)\n",
    "print(one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data for final training\n",
    "\n",
    "Here I'll split the data in train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, one_hot_labels))\n",
    "\n",
    "half_val_len = int(len(val_idx) / 2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], one_hot_labels[train_idx]\n",
    "val_x, val_y = codes[val_idx], one_hot_labels[val_idx]\n",
    "test_x, test_y = codes[test_idx], one_hot_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (8000, 4096) (8000, 10)\n",
      "Validation shapes (x, y): (1000, 4096) (1000, 10)\n",
      "Test shapes (x, y): (1000, 4096) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the fully connected layers\n",
    "\n",
    "I'll build the last two layers of the model and use the result of the pre trained VGG16 as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, one_hot_labels.shape[1]])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "relu7 = tf.layers.dense(inputs_, codes.shape[1], activation=tf.nn.relu)\n",
    "drop7 = tf.nn.dropout(relu7, keep_prob)\n",
    "\n",
    "logits = tf.layers.dense(drop7, one_hot_labels.shape[1])\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/99 Iteration: 0 Training loss: 4.35376\n",
      "***\n",
      "Epoch: 0/99 Iteration: 0 Validation Acc: 0.1110\n",
      "***\n",
      "Epoch: 0/99 Iteration: 1 Training loss: 44.39518\n",
      "Epoch: 0/99 Iteration: 2 Training loss: 49.34168\n",
      "Epoch: 0/99 Iteration: 3 Training loss: 52.91121\n",
      "Epoch: 0/99 Iteration: 4 Training loss: 49.66009\n",
      "Epoch: 0/99 Iteration: 5 Training loss: 45.84847\n",
      "Epoch: 0/99 Iteration: 6 Training loss: 44.63792\n",
      "Epoch: 0/99 Iteration: 7 Training loss: 37.06716\n",
      "Epoch: 0/99 Iteration: 8 Training loss: 36.01100\n",
      "Epoch: 0/99 Iteration: 9 Training loss: 32.04633\n",
      "Epoch: 0/99 Iteration: 10 Training loss: 25.31068\n",
      "Epoch: 0/99 Iteration: 11 Training loss: 14.30380\n",
      "Epoch: 0/99 Iteration: 12 Training loss: 10.27031\n",
      "Epoch: 0/99 Iteration: 13 Training loss: 12.33575\n",
      "Epoch: 0/99 Iteration: 14 Training loss: 11.13775\n",
      "Epoch: 0/99 Iteration: 15 Training loss: 10.70842\n",
      "Epoch: 0/99 Iteration: 16 Training loss: 8.49321\n",
      "Epoch: 0/99 Iteration: 17 Training loss: 8.81977\n",
      "Epoch: 0/99 Iteration: 18 Training loss: 7.78865\n",
      "Epoch: 0/99 Iteration: 19 Training loss: 5.27176\n",
      "Epoch: 0/99 Iteration: 20 Training loss: 4.97575\n",
      "Epoch: 0/99 Iteration: 21 Training loss: 3.39604\n",
      "Epoch: 0/99 Iteration: 22 Training loss: 4.77999\n",
      "Epoch: 0/99 Iteration: 23 Training loss: 3.56505\n",
      "Epoch: 0/99 Iteration: 24 Training loss: 3.22519\n",
      "Epoch: 0/99 Iteration: 25 Training loss: 3.32151\n",
      "Epoch: 0/99 Iteration: 26 Training loss: 3.14774\n",
      "Epoch: 0/99 Iteration: 27 Training loss: 3.22388\n",
      "Epoch: 0/99 Iteration: 28 Training loss: 2.55386\n",
      "Epoch: 0/99 Iteration: 29 Training loss: 2.42039\n",
      "Epoch: 0/99 Iteration: 30 Training loss: 2.02935\n",
      "Epoch: 0/99 Iteration: 31 Training loss: 2.27002\n",
      "Epoch: 0/99 Iteration: 32 Training loss: 2.10634\n",
      "Epoch: 0/99 Iteration: 33 Training loss: 1.80277\n",
      "Epoch: 0/99 Iteration: 34 Training loss: 1.71992\n",
      "Epoch: 0/99 Iteration: 35 Training loss: 1.83612\n",
      "Epoch: 0/99 Iteration: 36 Training loss: 1.98161\n",
      "Epoch: 0/99 Iteration: 37 Training loss: 1.96730\n",
      "Epoch: 0/99 Iteration: 38 Training loss: 1.59938\n",
      "Epoch: 0/99 Iteration: 39 Training loss: 1.66400\n",
      "Epoch: 0/99 Iteration: 40 Training loss: 1.73014\n",
      "Epoch: 0/99 Iteration: 41 Training loss: 1.61365\n",
      "Epoch: 0/99 Iteration: 42 Training loss: 1.76748\n",
      "Epoch: 0/99 Iteration: 43 Training loss: 1.62813\n",
      "Epoch: 0/99 Iteration: 44 Training loss: 1.52659\n",
      "Epoch: 0/99 Iteration: 45 Training loss: 1.44277\n",
      "Epoch: 0/99 Iteration: 46 Training loss: 1.66968\n",
      "Epoch: 0/99 Iteration: 47 Training loss: 1.56015\n",
      "Epoch: 0/99 Iteration: 48 Training loss: 1.85292\n",
      "Epoch: 0/99 Iteration: 49 Training loss: 1.60374\n",
      "Epoch: 0/99 Iteration: 50 Training loss: 1.34360\n",
      "***\n",
      "Epoch: 0/99 Iteration: 50 Validation Acc: 0.5330\n",
      "***\n",
      "Epoch: 0/99 Iteration: 51 Training loss: 1.58450\n",
      "Epoch: 0/99 Iteration: 52 Training loss: 1.53317\n",
      "Epoch: 0/99 Iteration: 53 Training loss: 1.63942\n",
      "Epoch: 0/99 Iteration: 54 Training loss: 1.52338\n",
      "Epoch: 0/99 Iteration: 55 Training loss: 1.40732\n",
      "Epoch: 0/99 Iteration: 56 Training loss: 1.55694\n",
      "Epoch: 0/99 Iteration: 57 Training loss: 1.47041\n",
      "Epoch: 0/99 Iteration: 58 Training loss: 1.42800\n",
      "Epoch: 0/99 Iteration: 59 Training loss: 1.57189\n",
      "Epoch: 0/99 Iteration: 60 Training loss: 1.38930\n",
      "Epoch: 0/99 Iteration: 61 Training loss: 1.57562\n",
      "Epoch: 0/99 Iteration: 62 Training loss: 1.28623\n",
      "Epoch: 0/99 Iteration: 63 Training loss: 1.41717\n",
      "Epoch: 0/99 Iteration: 64 Training loss: 1.53149\n",
      "Epoch: 0/99 Iteration: 65 Training loss: 1.45476\n",
      "Epoch: 0/99 Iteration: 66 Training loss: 1.36120\n",
      "Epoch: 0/99 Iteration: 67 Training loss: 1.49014\n",
      "Epoch: 0/99 Iteration: 68 Training loss: 1.47953\n",
      "Epoch: 0/99 Iteration: 69 Training loss: 1.34939\n",
      "Epoch: 0/99 Iteration: 70 Training loss: 1.48573\n",
      "Epoch: 0/99 Iteration: 71 Training loss: 1.34228\n",
      "Epoch: 0/99 Iteration: 72 Training loss: 1.43575\n",
      "Epoch: 0/99 Iteration: 73 Training loss: 1.27100\n",
      "Epoch: 0/99 Iteration: 74 Training loss: 1.30553\n",
      "Epoch: 0/99 Iteration: 75 Training loss: 1.42180\n",
      "Epoch: 0/99 Iteration: 76 Training loss: 1.23457\n",
      "Epoch: 0/99 Iteration: 77 Training loss: 1.43461\n",
      "Epoch: 0/99 Iteration: 78 Training loss: 1.50838\n",
      "Epoch: 0/99 Iteration: 79 Training loss: 1.29895\n",
      "Epoch: 1/99 Iteration: 80 Training loss: 1.11291\n",
      "Epoch: 1/99 Iteration: 81 Training loss: 1.26278\n",
      "Epoch: 1/99 Iteration: 82 Training loss: 1.15833\n",
      "Epoch: 1/99 Iteration: 83 Training loss: 1.37490\n",
      "Epoch: 1/99 Iteration: 84 Training loss: 1.15113\n",
      "Epoch: 1/99 Iteration: 85 Training loss: 1.18780\n",
      "Epoch: 1/99 Iteration: 86 Training loss: 1.22531\n",
      "Epoch: 1/99 Iteration: 87 Training loss: 1.33136\n",
      "Epoch: 1/99 Iteration: 88 Training loss: 1.24527\n",
      "Epoch: 1/99 Iteration: 89 Training loss: 1.23034\n",
      "Epoch: 1/99 Iteration: 90 Training loss: 1.18008\n",
      "Epoch: 1/99 Iteration: 91 Training loss: 1.20669\n",
      "Epoch: 1/99 Iteration: 92 Training loss: 1.12808\n",
      "Epoch: 1/99 Iteration: 93 Training loss: 1.06162\n",
      "Epoch: 1/99 Iteration: 94 Training loss: 1.35175\n",
      "Epoch: 1/99 Iteration: 95 Training loss: 1.21334\n",
      "Epoch: 1/99 Iteration: 96 Training loss: 0.91554\n",
      "Epoch: 1/99 Iteration: 97 Training loss: 0.99177\n",
      "Epoch: 1/99 Iteration: 98 Training loss: 1.19163\n",
      "Epoch: 1/99 Iteration: 99 Training loss: 1.20472\n",
      "Epoch: 1/99 Iteration: 100 Training loss: 1.06405\n",
      "***\n",
      "Epoch: 1/99 Iteration: 100 Validation Acc: 0.6170\n",
      "***\n",
      "Epoch: 1/99 Iteration: 101 Training loss: 1.01841\n",
      "Epoch: 1/99 Iteration: 102 Training loss: 1.22929\n",
      "Epoch: 1/99 Iteration: 103 Training loss: 1.24202\n",
      "Epoch: 1/99 Iteration: 104 Training loss: 1.20896\n",
      "Epoch: 1/99 Iteration: 105 Training loss: 1.17289\n",
      "Epoch: 1/99 Iteration: 106 Training loss: 1.11675\n",
      "Epoch: 1/99 Iteration: 107 Training loss: 1.08507\n",
      "Epoch: 1/99 Iteration: 108 Training loss: 1.16238\n",
      "Epoch: 1/99 Iteration: 109 Training loss: 1.37658\n",
      "Epoch: 1/99 Iteration: 110 Training loss: 1.00469\n",
      "Epoch: 1/99 Iteration: 111 Training loss: 1.15602\n",
      "Epoch: 1/99 Iteration: 112 Training loss: 1.17484\n",
      "Epoch: 1/99 Iteration: 113 Training loss: 1.16755\n",
      "Epoch: 1/99 Iteration: 114 Training loss: 1.07300\n",
      "Epoch: 1/99 Iteration: 115 Training loss: 1.12776\n",
      "Epoch: 1/99 Iteration: 116 Training loss: 1.43137\n",
      "Epoch: 1/99 Iteration: 117 Training loss: 1.30542\n",
      "Epoch: 1/99 Iteration: 118 Training loss: 1.18485\n",
      "Epoch: 1/99 Iteration: 119 Training loss: 1.11440\n",
      "Epoch: 1/99 Iteration: 120 Training loss: 1.16492\n",
      "Epoch: 1/99 Iteration: 121 Training loss: 1.16516\n",
      "Epoch: 1/99 Iteration: 122 Training loss: 1.33164\n",
      "Epoch: 1/99 Iteration: 123 Training loss: 1.22281\n",
      "Epoch: 1/99 Iteration: 124 Training loss: 1.15648\n",
      "Epoch: 1/99 Iteration: 125 Training loss: 1.04627\n",
      "Epoch: 1/99 Iteration: 126 Training loss: 1.26560\n",
      "Epoch: 1/99 Iteration: 127 Training loss: 1.18329\n",
      "Epoch: 1/99 Iteration: 128 Training loss: 1.50516\n",
      "Epoch: 1/99 Iteration: 129 Training loss: 1.18126\n",
      "Epoch: 1/99 Iteration: 130 Training loss: 1.15257\n",
      "Epoch: 1/99 Iteration: 131 Training loss: 1.02554\n",
      "Epoch: 1/99 Iteration: 132 Training loss: 1.01872\n",
      "Epoch: 1/99 Iteration: 133 Training loss: 1.25937\n",
      "Epoch: 1/99 Iteration: 134 Training loss: 1.24507\n",
      "Epoch: 1/99 Iteration: 135 Training loss: 1.01615\n",
      "Epoch: 1/99 Iteration: 136 Training loss: 1.22293\n",
      "Epoch: 1/99 Iteration: 137 Training loss: 1.24339\n",
      "Epoch: 1/99 Iteration: 138 Training loss: 1.13480\n",
      "Epoch: 1/99 Iteration: 139 Training loss: 1.11261\n",
      "Epoch: 1/99 Iteration: 140 Training loss: 1.07988\n",
      "Epoch: 1/99 Iteration: 141 Training loss: 1.10691\n",
      "Epoch: 1/99 Iteration: 142 Training loss: 1.02645\n",
      "Epoch: 1/99 Iteration: 143 Training loss: 1.00639\n",
      "Epoch: 1/99 Iteration: 144 Training loss: 1.16372\n",
      "Epoch: 1/99 Iteration: 145 Training loss: 1.07234\n",
      "Epoch: 1/99 Iteration: 146 Training loss: 1.17190\n",
      "Epoch: 1/99 Iteration: 147 Training loss: 1.19443\n",
      "Epoch: 1/99 Iteration: 148 Training loss: 1.14642\n",
      "Epoch: 1/99 Iteration: 149 Training loss: 0.98978\n",
      "Epoch: 1/99 Iteration: 150 Training loss: 1.20493\n",
      "***\n",
      "Epoch: 1/99 Iteration: 150 Validation Acc: 0.6690\n",
      "***\n",
      "Epoch: 1/99 Iteration: 151 Training loss: 1.02870\n",
      "Epoch: 1/99 Iteration: 152 Training loss: 0.96645\n",
      "Epoch: 1/99 Iteration: 153 Training loss: 1.08769\n",
      "Epoch: 1/99 Iteration: 154 Training loss: 1.04634\n",
      "Epoch: 1/99 Iteration: 155 Training loss: 1.00017\n",
      "Epoch: 1/99 Iteration: 156 Training loss: 1.07420\n",
      "Epoch: 1/99 Iteration: 157 Training loss: 1.16023\n",
      "Epoch: 1/99 Iteration: 158 Training loss: 1.08479\n",
      "Epoch: 1/99 Iteration: 159 Training loss: 1.16089\n",
      "Epoch: 2/99 Iteration: 160 Training loss: 0.96022\n",
      "Epoch: 2/99 Iteration: 161 Training loss: 0.95188\n",
      "Epoch: 2/99 Iteration: 162 Training loss: 1.06213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/99 Iteration: 163 Training loss: 1.00181\n",
      "Epoch: 2/99 Iteration: 164 Training loss: 0.94810\n",
      "Epoch: 2/99 Iteration: 165 Training loss: 1.01120\n",
      "Epoch: 2/99 Iteration: 166 Training loss: 0.93516\n",
      "Epoch: 2/99 Iteration: 167 Training loss: 0.96779\n",
      "Epoch: 2/99 Iteration: 168 Training loss: 1.11334\n",
      "Epoch: 2/99 Iteration: 169 Training loss: 0.93222\n",
      "Epoch: 2/99 Iteration: 170 Training loss: 0.93731\n",
      "Epoch: 2/99 Iteration: 171 Training loss: 1.01708\n",
      "Epoch: 2/99 Iteration: 172 Training loss: 1.03271\n",
      "Epoch: 2/99 Iteration: 173 Training loss: 0.95002\n",
      "Epoch: 2/99 Iteration: 174 Training loss: 1.21745\n",
      "Epoch: 2/99 Iteration: 175 Training loss: 0.94850\n",
      "Epoch: 2/99 Iteration: 176 Training loss: 0.75957\n",
      "Epoch: 2/99 Iteration: 177 Training loss: 0.87228\n",
      "Epoch: 2/99 Iteration: 178 Training loss: 0.98388\n",
      "Epoch: 2/99 Iteration: 179 Training loss: 0.96772\n",
      "Epoch: 2/99 Iteration: 180 Training loss: 0.87587\n",
      "Epoch: 2/99 Iteration: 181 Training loss: 0.91102\n",
      "Epoch: 2/99 Iteration: 182 Training loss: 1.05622\n",
      "Epoch: 2/99 Iteration: 183 Training loss: 0.98095\n",
      "Epoch: 2/99 Iteration: 184 Training loss: 0.98070\n",
      "Epoch: 2/99 Iteration: 185 Training loss: 1.04557\n",
      "Epoch: 2/99 Iteration: 186 Training loss: 0.90493\n",
      "Epoch: 2/99 Iteration: 187 Training loss: 0.85887\n",
      "Epoch: 2/99 Iteration: 188 Training loss: 0.91552\n",
      "Epoch: 2/99 Iteration: 189 Training loss: 1.23907\n",
      "Epoch: 2/99 Iteration: 190 Training loss: 0.73048\n",
      "Epoch: 2/99 Iteration: 191 Training loss: 0.81828\n",
      "Epoch: 2/99 Iteration: 192 Training loss: 1.01773\n",
      "Epoch: 2/99 Iteration: 193 Training loss: 0.96054\n",
      "Epoch: 2/99 Iteration: 194 Training loss: 0.95840\n",
      "Epoch: 2/99 Iteration: 195 Training loss: 0.91130\n",
      "Epoch: 2/99 Iteration: 196 Training loss: 1.26951\n",
      "Epoch: 2/99 Iteration: 197 Training loss: 0.95878\n",
      "Epoch: 2/99 Iteration: 198 Training loss: 0.99270\n",
      "Epoch: 2/99 Iteration: 199 Training loss: 0.90704\n",
      "Epoch: 2/99 Iteration: 200 Training loss: 0.95356\n",
      "***\n",
      "Epoch: 2/99 Iteration: 200 Validation Acc: 0.6810\n",
      "***\n",
      "Epoch: 2/99 Iteration: 201 Training loss: 0.90149\n",
      "Epoch: 2/99 Iteration: 202 Training loss: 1.03187\n",
      "Epoch: 2/99 Iteration: 203 Training loss: 0.97970\n",
      "Epoch: 2/99 Iteration: 204 Training loss: 1.15565\n",
      "Epoch: 2/99 Iteration: 205 Training loss: 0.80983\n",
      "Epoch: 2/99 Iteration: 206 Training loss: 0.96476\n",
      "Epoch: 2/99 Iteration: 207 Training loss: 0.87211\n",
      "Epoch: 2/99 Iteration: 208 Training loss: 1.16386\n",
      "Epoch: 2/99 Iteration: 209 Training loss: 0.96622\n",
      "Epoch: 2/99 Iteration: 210 Training loss: 0.94541\n",
      "Epoch: 2/99 Iteration: 211 Training loss: 0.84191\n",
      "Epoch: 2/99 Iteration: 212 Training loss: 0.94948\n",
      "Epoch: 2/99 Iteration: 213 Training loss: 1.07910\n",
      "Epoch: 2/99 Iteration: 214 Training loss: 0.98549\n",
      "Epoch: 2/99 Iteration: 215 Training loss: 0.83531\n",
      "Epoch: 2/99 Iteration: 216 Training loss: 1.03953\n",
      "Epoch: 2/99 Iteration: 217 Training loss: 1.09524\n",
      "Epoch: 2/99 Iteration: 218 Training loss: 1.07185\n",
      "Epoch: 2/99 Iteration: 219 Training loss: 0.97016\n",
      "Epoch: 2/99 Iteration: 220 Training loss: 0.96425\n",
      "Epoch: 2/99 Iteration: 221 Training loss: 0.93803\n",
      "Epoch: 2/99 Iteration: 222 Training loss: 1.00198\n",
      "Epoch: 2/99 Iteration: 223 Training loss: 0.98284\n",
      "Epoch: 2/99 Iteration: 224 Training loss: 0.99137\n",
      "Epoch: 2/99 Iteration: 225 Training loss: 1.11415\n",
      "Epoch: 2/99 Iteration: 226 Training loss: 1.02795\n",
      "Epoch: 2/99 Iteration: 227 Training loss: 1.02168\n",
      "Epoch: 2/99 Iteration: 228 Training loss: 0.87645\n",
      "Epoch: 2/99 Iteration: 229 Training loss: 0.76934\n",
      "Epoch: 2/99 Iteration: 230 Training loss: 1.09178\n",
      "Epoch: 2/99 Iteration: 231 Training loss: 0.86339\n",
      "Epoch: 2/99 Iteration: 232 Training loss: 0.92890\n",
      "Epoch: 2/99 Iteration: 233 Training loss: 0.85692\n",
      "Epoch: 2/99 Iteration: 234 Training loss: 0.90346\n",
      "Epoch: 2/99 Iteration: 235 Training loss: 0.80409\n",
      "Epoch: 2/99 Iteration: 236 Training loss: 0.83209\n",
      "Epoch: 2/99 Iteration: 237 Training loss: 0.98082\n",
      "Epoch: 2/99 Iteration: 238 Training loss: 1.00065\n",
      "Epoch: 2/99 Iteration: 239 Training loss: 1.00327\n",
      "Epoch: 3/99 Iteration: 240 Training loss: 0.80631\n",
      "Epoch: 3/99 Iteration: 241 Training loss: 1.08147\n",
      "Epoch: 3/99 Iteration: 242 Training loss: 0.80259\n",
      "Epoch: 3/99 Iteration: 243 Training loss: 0.80471\n",
      "Epoch: 3/99 Iteration: 244 Training loss: 0.85516\n",
      "Epoch: 3/99 Iteration: 245 Training loss: 0.78981\n",
      "Epoch: 3/99 Iteration: 246 Training loss: 0.83839\n",
      "Epoch: 3/99 Iteration: 247 Training loss: 0.86023\n",
      "Epoch: 3/99 Iteration: 248 Training loss: 0.98683\n",
      "Epoch: 3/99 Iteration: 249 Training loss: 0.82202\n",
      "Epoch: 3/99 Iteration: 250 Training loss: 0.77083\n",
      "***\n",
      "Epoch: 3/99 Iteration: 250 Validation Acc: 0.7260\n",
      "***\n",
      "Epoch: 3/99 Iteration: 251 Training loss: 0.84581\n",
      "Epoch: 3/99 Iteration: 252 Training loss: 0.81469\n",
      "Epoch: 3/99 Iteration: 253 Training loss: 0.81841\n",
      "Epoch: 3/99 Iteration: 254 Training loss: 1.10391\n",
      "Epoch: 3/99 Iteration: 255 Training loss: 0.82976\n",
      "Epoch: 3/99 Iteration: 256 Training loss: 0.68681\n",
      "Epoch: 3/99 Iteration: 257 Training loss: 0.72871\n",
      "Epoch: 3/99 Iteration: 258 Training loss: 0.85229\n",
      "Epoch: 3/99 Iteration: 259 Training loss: 0.82989\n",
      "Epoch: 3/99 Iteration: 260 Training loss: 0.81683\n",
      "Epoch: 3/99 Iteration: 261 Training loss: 0.71361\n",
      "Epoch: 3/99 Iteration: 262 Training loss: 0.90295\n",
      "Epoch: 3/99 Iteration: 263 Training loss: 0.84053\n",
      "Epoch: 3/99 Iteration: 264 Training loss: 0.83181\n",
      "Epoch: 3/99 Iteration: 265 Training loss: 0.85115\n",
      "Epoch: 3/99 Iteration: 266 Training loss: 0.82599\n",
      "Epoch: 3/99 Iteration: 267 Training loss: 0.71816\n",
      "Epoch: 3/99 Iteration: 268 Training loss: 0.85575\n",
      "Epoch: 3/99 Iteration: 269 Training loss: 1.17718\n",
      "Epoch: 3/99 Iteration: 270 Training loss: 0.75418\n",
      "Epoch: 3/99 Iteration: 271 Training loss: 0.68761\n",
      "Epoch: 3/99 Iteration: 272 Training loss: 0.96252\n",
      "Epoch: 3/99 Iteration: 273 Training loss: 0.77417\n",
      "Epoch: 3/99 Iteration: 274 Training loss: 0.72375\n",
      "Epoch: 3/99 Iteration: 275 Training loss: 0.79715\n",
      "Epoch: 3/99 Iteration: 276 Training loss: 1.00030\n",
      "Epoch: 3/99 Iteration: 277 Training loss: 1.03581\n",
      "Epoch: 3/99 Iteration: 278 Training loss: 0.79948\n",
      "Epoch: 3/99 Iteration: 279 Training loss: 0.80929\n",
      "Epoch: 3/99 Iteration: 280 Training loss: 0.86229\n",
      "Epoch: 3/99 Iteration: 281 Training loss: 0.83719\n",
      "Epoch: 3/99 Iteration: 282 Training loss: 0.95694\n",
      "Epoch: 3/99 Iteration: 283 Training loss: 0.79128\n",
      "Epoch: 3/99 Iteration: 284 Training loss: 0.94649\n",
      "Epoch: 3/99 Iteration: 285 Training loss: 0.80233\n",
      "Epoch: 3/99 Iteration: 286 Training loss: 0.86385\n",
      "Epoch: 3/99 Iteration: 287 Training loss: 0.89688\n",
      "Epoch: 3/99 Iteration: 288 Training loss: 1.18674\n",
      "Epoch: 3/99 Iteration: 289 Training loss: 0.70095\n",
      "Epoch: 3/99 Iteration: 290 Training loss: 0.85050\n",
      "Epoch: 3/99 Iteration: 291 Training loss: 0.78669\n",
      "Epoch: 3/99 Iteration: 292 Training loss: 0.78283\n",
      "Epoch: 3/99 Iteration: 293 Training loss: 0.91016\n",
      "Epoch: 3/99 Iteration: 294 Training loss: 0.96966\n",
      "Epoch: 3/99 Iteration: 295 Training loss: 0.77373\n",
      "Epoch: 3/99 Iteration: 296 Training loss: 0.91127\n",
      "Epoch: 3/99 Iteration: 297 Training loss: 1.03048\n",
      "Epoch: 3/99 Iteration: 298 Training loss: 0.91713\n",
      "Epoch: 3/99 Iteration: 299 Training loss: 1.02735\n",
      "Epoch: 3/99 Iteration: 300 Training loss: 0.83284\n",
      "***\n",
      "Epoch: 3/99 Iteration: 300 Validation Acc: 0.7510\n",
      "***\n",
      "Epoch: 3/99 Iteration: 301 Training loss: 0.89040\n",
      "Epoch: 3/99 Iteration: 302 Training loss: 0.77653\n",
      "Epoch: 3/99 Iteration: 303 Training loss: 0.85384\n",
      "Epoch: 3/99 Iteration: 304 Training loss: 0.85976\n",
      "Epoch: 3/99 Iteration: 305 Training loss: 0.95429\n",
      "Epoch: 3/99 Iteration: 306 Training loss: 0.90935\n",
      "Epoch: 3/99 Iteration: 307 Training loss: 0.88371\n",
      "Epoch: 3/99 Iteration: 308 Training loss: 0.88277\n",
      "Epoch: 3/99 Iteration: 309 Training loss: 0.71120\n",
      "Epoch: 3/99 Iteration: 310 Training loss: 0.93088\n",
      "Epoch: 3/99 Iteration: 311 Training loss: 0.83590\n",
      "Epoch: 3/99 Iteration: 312 Training loss: 0.81622\n",
      "Epoch: 3/99 Iteration: 313 Training loss: 0.78437\n",
      "Epoch: 3/99 Iteration: 314 Training loss: 0.84356\n",
      "Epoch: 3/99 Iteration: 315 Training loss: 0.72932\n",
      "Epoch: 3/99 Iteration: 316 Training loss: 0.77234\n",
      "Epoch: 3/99 Iteration: 317 Training loss: 1.04442\n",
      "Epoch: 3/99 Iteration: 318 Training loss: 0.83570\n",
      "Epoch: 3/99 Iteration: 319 Training loss: 0.75155\n",
      "Epoch: 4/99 Iteration: 320 Training loss: 0.66253\n",
      "Epoch: 4/99 Iteration: 321 Training loss: 0.70577\n",
      "Epoch: 4/99 Iteration: 322 Training loss: 0.67915\n",
      "Epoch: 4/99 Iteration: 323 Training loss: 0.79346\n",
      "Epoch: 4/99 Iteration: 324 Training loss: 0.68984\n",
      "Epoch: 4/99 Iteration: 325 Training loss: 0.79116\n",
      "Epoch: 4/99 Iteration: 326 Training loss: 0.73043\n",
      "Epoch: 4/99 Iteration: 327 Training loss: 0.82548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/99 Iteration: 328 Training loss: 0.87261\n",
      "Epoch: 4/99 Iteration: 329 Training loss: 0.76227\n",
      "Epoch: 4/99 Iteration: 330 Training loss: 0.68636\n",
      "Epoch: 4/99 Iteration: 331 Training loss: 0.74201\n",
      "Epoch: 4/99 Iteration: 332 Training loss: 0.78353\n",
      "Epoch: 4/99 Iteration: 333 Training loss: 0.77672\n",
      "Epoch: 4/99 Iteration: 334 Training loss: 0.98424\n",
      "Epoch: 4/99 Iteration: 335 Training loss: 0.76339\n",
      "Epoch: 4/99 Iteration: 336 Training loss: 0.56169\n",
      "Epoch: 4/99 Iteration: 337 Training loss: 0.61945\n",
      "Epoch: 4/99 Iteration: 338 Training loss: 0.91484\n",
      "Epoch: 4/99 Iteration: 339 Training loss: 0.72232\n",
      "Epoch: 4/99 Iteration: 340 Training loss: 0.68283\n",
      "Epoch: 4/99 Iteration: 341 Training loss: 0.76282\n",
      "Epoch: 4/99 Iteration: 342 Training loss: 0.77532\n",
      "Epoch: 4/99 Iteration: 343 Training loss: 0.82613\n",
      "Epoch: 4/99 Iteration: 344 Training loss: 0.72356\n",
      "Epoch: 4/99 Iteration: 345 Training loss: 0.83887\n",
      "Epoch: 4/99 Iteration: 346 Training loss: 0.71537\n",
      "Epoch: 4/99 Iteration: 347 Training loss: 0.60792\n",
      "Epoch: 4/99 Iteration: 348 Training loss: 0.85829\n",
      "Epoch: 4/99 Iteration: 349 Training loss: 0.90923\n",
      "Epoch: 4/99 Iteration: 350 Training loss: 0.63906\n",
      "***\n",
      "Epoch: 4/99 Iteration: 350 Validation Acc: 0.7540\n",
      "***\n",
      "Epoch: 4/99 Iteration: 351 Training loss: 0.63074\n",
      "Epoch: 4/99 Iteration: 352 Training loss: 0.76479\n",
      "Epoch: 4/99 Iteration: 353 Training loss: 0.78640\n",
      "Epoch: 4/99 Iteration: 354 Training loss: 0.76619\n",
      "Epoch: 4/99 Iteration: 355 Training loss: 0.68063\n",
      "Epoch: 4/99 Iteration: 356 Training loss: 0.94504\n",
      "Epoch: 4/99 Iteration: 357 Training loss: 0.75833\n",
      "Epoch: 4/99 Iteration: 358 Training loss: 0.71878\n",
      "Epoch: 4/99 Iteration: 359 Training loss: 0.67913\n",
      "Epoch: 4/99 Iteration: 360 Training loss: 0.77244\n",
      "Epoch: 4/99 Iteration: 361 Training loss: 0.76363\n",
      "Epoch: 4/99 Iteration: 362 Training loss: 0.99389\n",
      "Epoch: 4/99 Iteration: 363 Training loss: 0.80057\n",
      "Epoch: 4/99 Iteration: 364 Training loss: 0.83826\n",
      "Epoch: 4/99 Iteration: 365 Training loss: 0.70798\n",
      "Epoch: 4/99 Iteration: 366 Training loss: 0.78045\n",
      "Epoch: 4/99 Iteration: 367 Training loss: 0.82214\n",
      "Epoch: 4/99 Iteration: 368 Training loss: 1.02506\n",
      "Epoch: 4/99 Iteration: 369 Training loss: 0.68000\n",
      "Epoch: 4/99 Iteration: 370 Training loss: 0.80869\n",
      "Epoch: 4/99 Iteration: 371 Training loss: 0.78634\n",
      "Epoch: 4/99 Iteration: 372 Training loss: 0.75266\n",
      "Epoch: 4/99 Iteration: 373 Training loss: 0.80375\n",
      "Epoch: 4/99 Iteration: 374 Training loss: 0.74135\n",
      "Epoch: 4/99 Iteration: 375 Training loss: 0.68535\n",
      "Epoch: 4/99 Iteration: 376 Training loss: 0.98910\n",
      "Epoch: 4/99 Iteration: 377 Training loss: 0.90003\n",
      "Epoch: 4/99 Iteration: 378 Training loss: 0.75643\n",
      "Epoch: 4/99 Iteration: 379 Training loss: 0.77389\n",
      "Epoch: 4/99 Iteration: 380 Training loss: 0.74274\n",
      "Epoch: 4/99 Iteration: 381 Training loss: 0.88763\n",
      "Epoch: 4/99 Iteration: 382 Training loss: 0.78356\n",
      "Epoch: 4/99 Iteration: 383 Training loss: 0.88403\n",
      "Epoch: 4/99 Iteration: 384 Training loss: 0.81570\n",
      "Epoch: 4/99 Iteration: 385 Training loss: 0.82720\n",
      "Epoch: 4/99 Iteration: 386 Training loss: 0.89779\n",
      "Epoch: 4/99 Iteration: 387 Training loss: 0.92502\n",
      "Epoch: 4/99 Iteration: 388 Training loss: 0.89206\n",
      "Epoch: 4/99 Iteration: 389 Training loss: 0.68336\n",
      "Epoch: 4/99 Iteration: 390 Training loss: 0.94326\n",
      "Epoch: 4/99 Iteration: 391 Training loss: 0.71249\n",
      "Epoch: 4/99 Iteration: 392 Training loss: 0.90830\n",
      "Epoch: 4/99 Iteration: 393 Training loss: 0.79252\n",
      "Epoch: 4/99 Iteration: 394 Training loss: 0.70643\n",
      "Epoch: 4/99 Iteration: 395 Training loss: 0.62828\n",
      "Epoch: 4/99 Iteration: 396 Training loss: 0.79966\n",
      "Epoch: 4/99 Iteration: 397 Training loss: 0.78571\n",
      "Epoch: 4/99 Iteration: 398 Training loss: 0.89791\n",
      "Epoch: 4/99 Iteration: 399 Training loss: 0.86702\n",
      "Epoch: 5/99 Iteration: 400 Training loss: 0.66231\n",
      "***\n",
      "Epoch: 5/99 Iteration: 400 Validation Acc: 0.7560\n",
      "***\n",
      "Epoch: 5/99 Iteration: 401 Training loss: 0.86586\n",
      "Epoch: 5/99 Iteration: 402 Training loss: 0.76727\n",
      "Epoch: 5/99 Iteration: 403 Training loss: 0.70015\n",
      "Epoch: 5/99 Iteration: 404 Training loss: 0.60970\n",
      "Epoch: 5/99 Iteration: 405 Training loss: 0.73676\n",
      "Epoch: 5/99 Iteration: 406 Training loss: 0.68399\n",
      "Epoch: 5/99 Iteration: 407 Training loss: 0.72425\n",
      "Epoch: 5/99 Iteration: 408 Training loss: 0.86600\n",
      "Epoch: 5/99 Iteration: 409 Training loss: 0.71067\n",
      "Epoch: 5/99 Iteration: 410 Training loss: 0.66361\n",
      "Epoch: 5/99 Iteration: 411 Training loss: 0.65840\n",
      "Epoch: 5/99 Iteration: 412 Training loss: 0.79149\n",
      "Epoch: 5/99 Iteration: 413 Training loss: 0.69401\n",
      "Epoch: 5/99 Iteration: 414 Training loss: 0.93361\n",
      "Epoch: 5/99 Iteration: 415 Training loss: 0.70904\n",
      "Epoch: 5/99 Iteration: 416 Training loss: 0.55210\n",
      "Epoch: 5/99 Iteration: 417 Training loss: 0.61898\n",
      "Epoch: 5/99 Iteration: 418 Training loss: 0.82786\n",
      "Epoch: 5/99 Iteration: 419 Training loss: 0.60874\n",
      "Epoch: 5/99 Iteration: 420 Training loss: 0.71320\n",
      "Epoch: 5/99 Iteration: 421 Training loss: 0.64291\n",
      "Epoch: 5/99 Iteration: 422 Training loss: 0.63955\n",
      "Epoch: 5/99 Iteration: 423 Training loss: 0.86391\n",
      "Epoch: 5/99 Iteration: 424 Training loss: 0.61132\n",
      "Epoch: 5/99 Iteration: 425 Training loss: 0.71745\n",
      "Epoch: 5/99 Iteration: 426 Training loss: 0.66131\n",
      "Epoch: 5/99 Iteration: 427 Training loss: 0.56647\n",
      "Epoch: 5/99 Iteration: 428 Training loss: 0.80788\n",
      "Epoch: 5/99 Iteration: 429 Training loss: 1.02105\n",
      "Epoch: 5/99 Iteration: 430 Training loss: 0.56995\n",
      "Epoch: 5/99 Iteration: 431 Training loss: 0.54759\n",
      "Epoch: 5/99 Iteration: 432 Training loss: 0.76108\n",
      "Epoch: 5/99 Iteration: 433 Training loss: 0.66176\n",
      "Epoch: 5/99 Iteration: 434 Training loss: 0.64925\n",
      "Epoch: 5/99 Iteration: 435 Training loss: 0.70642\n",
      "Epoch: 5/99 Iteration: 436 Training loss: 0.92207\n",
      "Epoch: 5/99 Iteration: 437 Training loss: 0.67697\n",
      "Epoch: 5/99 Iteration: 438 Training loss: 0.67321\n",
      "Epoch: 5/99 Iteration: 439 Training loss: 0.61414\n",
      "Epoch: 5/99 Iteration: 440 Training loss: 0.70975\n",
      "Epoch: 5/99 Iteration: 441 Training loss: 0.67422\n",
      "Epoch: 5/99 Iteration: 442 Training loss: 0.71739\n",
      "Epoch: 5/99 Iteration: 443 Training loss: 0.72999\n",
      "Epoch: 5/99 Iteration: 444 Training loss: 0.94310\n",
      "Epoch: 5/99 Iteration: 445 Training loss: 0.67228\n",
      "Epoch: 5/99 Iteration: 446 Training loss: 0.61642\n",
      "Epoch: 5/99 Iteration: 447 Training loss: 0.75833\n",
      "Epoch: 5/99 Iteration: 448 Training loss: 1.01686\n",
      "Epoch: 5/99 Iteration: 449 Training loss: 0.55284\n",
      "Epoch: 5/99 Iteration: 450 Training loss: 0.68074\n",
      "***\n",
      "Epoch: 5/99 Iteration: 450 Validation Acc: 0.7540\n",
      "***\n",
      "Epoch: 5/99 Iteration: 451 Training loss: 0.71341\n",
      "Epoch: 5/99 Iteration: 452 Training loss: 0.71897\n",
      "Epoch: 5/99 Iteration: 453 Training loss: 0.83970\n",
      "Epoch: 5/99 Iteration: 454 Training loss: 0.72653\n",
      "Epoch: 5/99 Iteration: 455 Training loss: 0.61989\n",
      "Epoch: 5/99 Iteration: 456 Training loss: 0.81602\n",
      "Epoch: 5/99 Iteration: 457 Training loss: 0.76031\n",
      "Epoch: 5/99 Iteration: 458 Training loss: 0.69958\n",
      "Epoch: 5/99 Iteration: 459 Training loss: 0.78354\n",
      "Epoch: 5/99 Iteration: 460 Training loss: 0.55655\n",
      "Epoch: 5/99 Iteration: 461 Training loss: 0.77741\n",
      "Epoch: 5/99 Iteration: 462 Training loss: 0.68190\n",
      "Epoch: 5/99 Iteration: 463 Training loss: 0.82153\n",
      "Epoch: 5/99 Iteration: 464 Training loss: 0.68768\n",
      "Epoch: 5/99 Iteration: 465 Training loss: 0.78318\n",
      "Epoch: 5/99 Iteration: 466 Training loss: 0.84168\n",
      "Epoch: 5/99 Iteration: 467 Training loss: 0.91330\n",
      "Epoch: 5/99 Iteration: 468 Training loss: 0.74395\n",
      "Epoch: 5/99 Iteration: 469 Training loss: 0.63921\n",
      "Epoch: 5/99 Iteration: 470 Training loss: 0.76368\n",
      "Epoch: 5/99 Iteration: 471 Training loss: 0.64730\n",
      "Epoch: 5/99 Iteration: 472 Training loss: 0.77767\n",
      "Epoch: 5/99 Iteration: 473 Training loss: 0.70611\n",
      "Epoch: 5/99 Iteration: 474 Training loss: 0.74963\n",
      "Epoch: 5/99 Iteration: 475 Training loss: 0.66341\n",
      "Epoch: 5/99 Iteration: 476 Training loss: 0.65741\n",
      "Epoch: 5/99 Iteration: 477 Training loss: 0.96322\n",
      "Epoch: 5/99 Iteration: 478 Training loss: 0.77290\n",
      "Epoch: 5/99 Iteration: 479 Training loss: 0.82716\n",
      "Epoch: 6/99 Iteration: 480 Training loss: 0.68024\n",
      "Epoch: 6/99 Iteration: 481 Training loss: 0.73083\n",
      "Epoch: 6/99 Iteration: 482 Training loss: 0.69128\n",
      "Epoch: 6/99 Iteration: 483 Training loss: 0.80102\n",
      "Epoch: 6/99 Iteration: 484 Training loss: 0.54105\n",
      "Epoch: 6/99 Iteration: 485 Training loss: 0.56294\n",
      "Epoch: 6/99 Iteration: 486 Training loss: 0.79099\n",
      "Epoch: 6/99 Iteration: 487 Training loss: 0.63323\n",
      "Epoch: 6/99 Iteration: 488 Training loss: 0.83300\n",
      "Epoch: 6/99 Iteration: 489 Training loss: 0.65007\n",
      "Epoch: 6/99 Iteration: 490 Training loss: 0.61248\n",
      "Epoch: 6/99 Iteration: 491 Training loss: 0.52904\n",
      "Epoch: 6/99 Iteration: 492 Training loss: 0.64777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/99 Iteration: 493 Training loss: 0.70774\n",
      "Epoch: 6/99 Iteration: 494 Training loss: 0.82646\n",
      "Epoch: 6/99 Iteration: 495 Training loss: 0.57855\n",
      "Epoch: 6/99 Iteration: 496 Training loss: 0.58452\n",
      "Epoch: 6/99 Iteration: 497 Training loss: 0.59290\n",
      "Epoch: 6/99 Iteration: 498 Training loss: 0.73772\n",
      "Epoch: 6/99 Iteration: 499 Training loss: 0.66159\n",
      "Epoch: 6/99 Iteration: 500 Training loss: 0.66866\n",
      "***\n",
      "Epoch: 6/99 Iteration: 500 Validation Acc: 0.7610\n",
      "***\n",
      "Epoch: 6/99 Iteration: 501 Training loss: 0.63403\n",
      "Epoch: 6/99 Iteration: 502 Training loss: 0.67258\n",
      "Epoch: 6/99 Iteration: 503 Training loss: 0.69707\n",
      "Epoch: 6/99 Iteration: 504 Training loss: 0.62980\n",
      "Epoch: 6/99 Iteration: 505 Training loss: 0.62208\n",
      "Epoch: 6/99 Iteration: 506 Training loss: 0.63786\n",
      "Epoch: 6/99 Iteration: 507 Training loss: 0.57346\n",
      "Epoch: 6/99 Iteration: 508 Training loss: 0.75377\n",
      "Epoch: 6/99 Iteration: 509 Training loss: 0.94896\n",
      "Epoch: 6/99 Iteration: 510 Training loss: 0.57580\n",
      "Epoch: 6/99 Iteration: 511 Training loss: 0.53489\n",
      "Epoch: 6/99 Iteration: 512 Training loss: 0.66122\n",
      "Epoch: 6/99 Iteration: 513 Training loss: 0.76681\n",
      "Epoch: 6/99 Iteration: 514 Training loss: 0.66561\n",
      "Epoch: 6/99 Iteration: 515 Training loss: 0.64338\n",
      "Epoch: 6/99 Iteration: 516 Training loss: 0.80222\n",
      "Epoch: 6/99 Iteration: 517 Training loss: 0.73635\n",
      "Epoch: 6/99 Iteration: 518 Training loss: 0.69678\n",
      "Epoch: 6/99 Iteration: 519 Training loss: 0.66020\n",
      "Epoch: 6/99 Iteration: 520 Training loss: 0.70522\n",
      "Epoch: 6/99 Iteration: 521 Training loss: 0.70125\n",
      "Epoch: 6/99 Iteration: 522 Training loss: 0.73892\n",
      "Epoch: 6/99 Iteration: 523 Training loss: 0.57617\n",
      "Epoch: 6/99 Iteration: 524 Training loss: 0.79019\n",
      "Epoch: 6/99 Iteration: 525 Training loss: 0.56563\n",
      "Epoch: 6/99 Iteration: 526 Training loss: 0.60998\n",
      "Epoch: 6/99 Iteration: 527 Training loss: 0.59973\n",
      "Epoch: 6/99 Iteration: 528 Training loss: 0.90039\n",
      "Epoch: 6/99 Iteration: 529 Training loss: 0.57731\n",
      "Epoch: 6/99 Iteration: 530 Training loss: 0.62722\n",
      "Epoch: 6/99 Iteration: 531 Training loss: 0.59942\n",
      "Epoch: 6/99 Iteration: 532 Training loss: 0.75503\n",
      "Epoch: 6/99 Iteration: 533 Training loss: 0.73248\n",
      "Epoch: 6/99 Iteration: 534 Training loss: 0.81842\n",
      "Epoch: 6/99 Iteration: 535 Training loss: 0.66673\n",
      "Epoch: 6/99 Iteration: 536 Training loss: 0.94926\n",
      "Epoch: 6/99 Iteration: 537 Training loss: 0.83968\n",
      "Epoch: 6/99 Iteration: 538 Training loss: 0.75438\n",
      "Epoch: 6/99 Iteration: 539 Training loss: 0.71153\n",
      "Epoch: 6/99 Iteration: 540 Training loss: 0.64634\n",
      "Epoch: 6/99 Iteration: 541 Training loss: 0.81660\n",
      "Epoch: 6/99 Iteration: 542 Training loss: 0.68582\n",
      "Epoch: 6/99 Iteration: 543 Training loss: 0.80480\n",
      "Epoch: 6/99 Iteration: 544 Training loss: 0.64966\n",
      "Epoch: 6/99 Iteration: 545 Training loss: 0.82989\n",
      "Epoch: 6/99 Iteration: 546 Training loss: 0.79892\n",
      "Epoch: 6/99 Iteration: 547 Training loss: 0.91704\n",
      "Epoch: 6/99 Iteration: 548 Training loss: 0.74161\n",
      "Epoch: 6/99 Iteration: 549 Training loss: 0.52665\n",
      "Epoch: 6/99 Iteration: 550 Training loss: 0.76721\n",
      "***\n",
      "Epoch: 6/99 Iteration: 550 Validation Acc: 0.7860\n",
      "***\n",
      "Epoch: 6/99 Iteration: 551 Training loss: 0.68570\n",
      "Epoch: 6/99 Iteration: 552 Training loss: 0.72931\n",
      "Epoch: 6/99 Iteration: 553 Training loss: 0.63218\n",
      "Epoch: 6/99 Iteration: 554 Training loss: 0.62185\n",
      "Epoch: 6/99 Iteration: 555 Training loss: 0.57270\n",
      "Epoch: 6/99 Iteration: 556 Training loss: 0.72432\n",
      "Epoch: 6/99 Iteration: 557 Training loss: 0.87049\n",
      "Epoch: 6/99 Iteration: 558 Training loss: 0.68962\n",
      "Epoch: 6/99 Iteration: 559 Training loss: 0.62254\n",
      "Epoch: 7/99 Iteration: 560 Training loss: 0.59547\n",
      "Epoch: 7/99 Iteration: 561 Training loss: 0.81394\n",
      "Epoch: 7/99 Iteration: 562 Training loss: 0.65273\n",
      "Epoch: 7/99 Iteration: 563 Training loss: 0.63738\n",
      "Epoch: 7/99 Iteration: 564 Training loss: 0.64043\n",
      "Epoch: 7/99 Iteration: 565 Training loss: 0.72572\n",
      "Epoch: 7/99 Iteration: 566 Training loss: 0.59869\n",
      "Epoch: 7/99 Iteration: 567 Training loss: 0.60075\n",
      "Epoch: 7/99 Iteration: 568 Training loss: 0.82216\n",
      "Epoch: 7/99 Iteration: 569 Training loss: 0.62654\n",
      "Epoch: 7/99 Iteration: 570 Training loss: 0.62783\n",
      "Epoch: 7/99 Iteration: 571 Training loss: 0.63835\n",
      "Epoch: 7/99 Iteration: 572 Training loss: 0.68442\n",
      "Epoch: 7/99 Iteration: 573 Training loss: 0.65456\n",
      "Epoch: 7/99 Iteration: 574 Training loss: 0.87339\n",
      "Epoch: 7/99 Iteration: 575 Training loss: 0.54819\n",
      "Epoch: 7/99 Iteration: 576 Training loss: 0.43497\n",
      "Epoch: 7/99 Iteration: 577 Training loss: 0.47469\n",
      "Epoch: 7/99 Iteration: 578 Training loss: 0.76542\n",
      "Epoch: 7/99 Iteration: 579 Training loss: 0.75110\n",
      "Epoch: 7/99 Iteration: 580 Training loss: 0.63871\n",
      "Epoch: 7/99 Iteration: 581 Training loss: 0.55570\n",
      "Epoch: 7/99 Iteration: 582 Training loss: 0.64659\n",
      "Epoch: 7/99 Iteration: 583 Training loss: 0.63286\n",
      "Epoch: 7/99 Iteration: 584 Training loss: 0.56719\n",
      "Epoch: 7/99 Iteration: 585 Training loss: 0.69210\n",
      "Epoch: 7/99 Iteration: 586 Training loss: 0.68639\n",
      "Epoch: 7/99 Iteration: 587 Training loss: 0.58453\n",
      "Epoch: 7/99 Iteration: 588 Training loss: 0.69506\n",
      "Epoch: 7/99 Iteration: 589 Training loss: 0.86190\n",
      "Epoch: 7/99 Iteration: 590 Training loss: 0.52148\n",
      "Epoch: 7/99 Iteration: 591 Training loss: 0.53539\n",
      "Epoch: 7/99 Iteration: 592 Training loss: 0.69105\n",
      "Epoch: 7/99 Iteration: 593 Training loss: 0.66125\n",
      "Epoch: 7/99 Iteration: 594 Training loss: 0.61671\n",
      "Epoch: 7/99 Iteration: 595 Training loss: 0.66992\n",
      "Epoch: 7/99 Iteration: 596 Training loss: 0.80748\n",
      "Epoch: 7/99 Iteration: 597 Training loss: 0.53028\n",
      "Epoch: 7/99 Iteration: 598 Training loss: 0.56221\n",
      "Epoch: 7/99 Iteration: 599 Training loss: 0.60788\n",
      "Epoch: 7/99 Iteration: 600 Training loss: 0.61994\n",
      "***\n",
      "Epoch: 7/99 Iteration: 600 Validation Acc: 0.8000\n",
      "***\n",
      "Epoch: 7/99 Iteration: 601 Training loss: 0.65346\n",
      "Epoch: 7/99 Iteration: 602 Training loss: 0.75478\n",
      "Epoch: 7/99 Iteration: 603 Training loss: 0.64507\n",
      "Epoch: 7/99 Iteration: 604 Training loss: 0.76003\n",
      "Epoch: 7/99 Iteration: 605 Training loss: 0.62426\n",
      "Epoch: 7/99 Iteration: 606 Training loss: 0.69593\n",
      "Epoch: 7/99 Iteration: 607 Training loss: 0.62790\n",
      "Epoch: 7/99 Iteration: 608 Training loss: 1.00755\n",
      "Epoch: 7/99 Iteration: 609 Training loss: 0.58362\n",
      "Epoch: 7/99 Iteration: 610 Training loss: 0.59957\n",
      "Epoch: 7/99 Iteration: 611 Training loss: 0.57377\n",
      "Epoch: 7/99 Iteration: 612 Training loss: 0.87000\n",
      "Epoch: 7/99 Iteration: 613 Training loss: 0.75673\n",
      "Epoch: 7/99 Iteration: 614 Training loss: 0.79887\n",
      "Epoch: 7/99 Iteration: 615 Training loss: 0.56521\n",
      "Epoch: 7/99 Iteration: 616 Training loss: 0.66731\n",
      "Epoch: 7/99 Iteration: 617 Training loss: 0.95354\n",
      "Epoch: 7/99 Iteration: 618 Training loss: 0.69541\n",
      "Epoch: 7/99 Iteration: 619 Training loss: 0.77524\n",
      "Epoch: 7/99 Iteration: 620 Training loss: 0.56536\n",
      "Epoch: 7/99 Iteration: 621 Training loss: 0.59654\n",
      "Epoch: 7/99 Iteration: 622 Training loss: 0.62790\n",
      "Epoch: 7/99 Iteration: 623 Training loss: 0.70885\n",
      "Epoch: 7/99 Iteration: 624 Training loss: 0.65522\n",
      "Epoch: 7/99 Iteration: 625 Training loss: 0.71133\n",
      "Epoch: 7/99 Iteration: 626 Training loss: 0.76981\n",
      "Epoch: 7/99 Iteration: 627 Training loss: 0.78645\n",
      "Epoch: 7/99 Iteration: 628 Training loss: 0.70294\n",
      "Epoch: 7/99 Iteration: 629 Training loss: 0.59994\n",
      "Epoch: 7/99 Iteration: 630 Training loss: 0.75276\n",
      "Epoch: 7/99 Iteration: 631 Training loss: 0.57335\n",
      "Epoch: 7/99 Iteration: 632 Training loss: 0.65585\n",
      "Epoch: 7/99 Iteration: 633 Training loss: 0.60340\n",
      "Epoch: 7/99 Iteration: 634 Training loss: 0.66726\n",
      "Epoch: 7/99 Iteration: 635 Training loss: 0.58435\n",
      "Epoch: 7/99 Iteration: 636 Training loss: 0.68110\n",
      "Epoch: 7/99 Iteration: 637 Training loss: 0.74631\n",
      "Epoch: 7/99 Iteration: 638 Training loss: 0.74665\n",
      "Epoch: 7/99 Iteration: 639 Training loss: 0.66525\n",
      "Epoch: 8/99 Iteration: 640 Training loss: 0.55772\n",
      "Epoch: 8/99 Iteration: 641 Training loss: 0.69957\n",
      "Epoch: 8/99 Iteration: 642 Training loss: 0.52757\n",
      "Epoch: 8/99 Iteration: 643 Training loss: 0.63045\n",
      "Epoch: 8/99 Iteration: 644 Training loss: 0.55748\n",
      "Epoch: 8/99 Iteration: 645 Training loss: 0.66116\n",
      "Epoch: 8/99 Iteration: 646 Training loss: 0.52174\n",
      "Epoch: 8/99 Iteration: 647 Training loss: 0.67023\n",
      "Epoch: 8/99 Iteration: 648 Training loss: 0.77330\n",
      "Epoch: 8/99 Iteration: 649 Training loss: 0.53704\n",
      "Epoch: 8/99 Iteration: 650 Training loss: 0.48625\n",
      "***\n",
      "Epoch: 8/99 Iteration: 650 Validation Acc: 0.7900\n",
      "***\n",
      "Epoch: 8/99 Iteration: 651 Training loss: 0.48954\n",
      "Epoch: 8/99 Iteration: 652 Training loss: 0.67028\n",
      "Epoch: 8/99 Iteration: 653 Training loss: 0.65514\n",
      "Epoch: 8/99 Iteration: 654 Training loss: 0.74075\n",
      "Epoch: 8/99 Iteration: 655 Training loss: 0.54345\n",
      "Epoch: 8/99 Iteration: 656 Training loss: 0.53504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/99 Iteration: 657 Training loss: 0.54094\n",
      "Epoch: 8/99 Iteration: 658 Training loss: 0.63704\n",
      "Epoch: 8/99 Iteration: 659 Training loss: 0.59740\n",
      "Epoch: 8/99 Iteration: 660 Training loss: 0.62257\n",
      "Epoch: 8/99 Iteration: 661 Training loss: 0.51221\n",
      "Epoch: 8/99 Iteration: 662 Training loss: 0.53026\n",
      "Epoch: 8/99 Iteration: 663 Training loss: 0.64209\n",
      "Epoch: 8/99 Iteration: 664 Training loss: 0.52415\n",
      "Epoch: 8/99 Iteration: 665 Training loss: 0.63555\n",
      "Epoch: 8/99 Iteration: 666 Training loss: 0.60632\n",
      "Epoch: 8/99 Iteration: 667 Training loss: 0.49815\n",
      "Epoch: 8/99 Iteration: 668 Training loss: 0.68738\n",
      "Epoch: 8/99 Iteration: 669 Training loss: 0.78268\n",
      "Epoch: 8/99 Iteration: 670 Training loss: 0.46574\n",
      "Epoch: 8/99 Iteration: 671 Training loss: 0.42322\n",
      "Epoch: 8/99 Iteration: 672 Training loss: 0.60712\n",
      "Epoch: 8/99 Iteration: 673 Training loss: 0.66824\n",
      "Epoch: 8/99 Iteration: 674 Training loss: 0.53857\n",
      "Epoch: 8/99 Iteration: 675 Training loss: 0.56853\n",
      "Epoch: 8/99 Iteration: 676 Training loss: 0.88102\n",
      "Epoch: 8/99 Iteration: 677 Training loss: 0.50078\n",
      "Epoch: 8/99 Iteration: 678 Training loss: 0.50265\n",
      "Epoch: 8/99 Iteration: 679 Training loss: 0.56747\n",
      "Epoch: 8/99 Iteration: 680 Training loss: 0.63939\n",
      "Epoch: 8/99 Iteration: 681 Training loss: 0.68735\n",
      "Epoch: 8/99 Iteration: 682 Training loss: 0.60293\n",
      "Epoch: 8/99 Iteration: 683 Training loss: 0.46558\n",
      "Epoch: 8/99 Iteration: 684 Training loss: 0.80033\n",
      "Epoch: 8/99 Iteration: 685 Training loss: 0.56249\n",
      "Epoch: 8/99 Iteration: 686 Training loss: 0.70642\n",
      "Epoch: 8/99 Iteration: 687 Training loss: 0.66109\n",
      "Epoch: 8/99 Iteration: 688 Training loss: 0.84912\n",
      "Epoch: 8/99 Iteration: 689 Training loss: 0.47560\n",
      "Epoch: 8/99 Iteration: 690 Training loss: 0.66238\n",
      "Epoch: 8/99 Iteration: 691 Training loss: 0.60075\n",
      "Epoch: 8/99 Iteration: 692 Training loss: 0.66708\n",
      "Epoch: 8/99 Iteration: 693 Training loss: 0.67762\n",
      "Epoch: 8/99 Iteration: 694 Training loss: 0.80036\n",
      "Epoch: 8/99 Iteration: 695 Training loss: 0.61973\n",
      "Epoch: 8/99 Iteration: 696 Training loss: 0.79431\n",
      "Epoch: 8/99 Iteration: 697 Training loss: 0.72118\n",
      "Epoch: 8/99 Iteration: 698 Training loss: 0.73999\n",
      "Epoch: 8/99 Iteration: 699 Training loss: 0.76131\n",
      "Epoch: 8/99 Iteration: 700 Training loss: 0.59162\n",
      "***\n",
      "Epoch: 8/99 Iteration: 700 Validation Acc: 0.7450\n",
      "***\n",
      "Epoch: 8/99 Iteration: 701 Training loss: 0.69648\n",
      "Epoch: 8/99 Iteration: 702 Training loss: 0.62105\n",
      "Epoch: 8/99 Iteration: 703 Training loss: 0.72737\n",
      "Epoch: 8/99 Iteration: 704 Training loss: 0.59065\n",
      "Epoch: 8/99 Iteration: 705 Training loss: 0.72977\n",
      "Epoch: 8/99 Iteration: 706 Training loss: 0.66448\n",
      "Epoch: 8/99 Iteration: 707 Training loss: 0.75158\n",
      "Epoch: 8/99 Iteration: 708 Training loss: 0.68621\n",
      "Epoch: 8/99 Iteration: 709 Training loss: 0.49125\n",
      "Epoch: 8/99 Iteration: 710 Training loss: 0.73527\n",
      "Epoch: 8/99 Iteration: 711 Training loss: 0.49057\n",
      "Epoch: 8/99 Iteration: 712 Training loss: 0.62405\n",
      "Epoch: 8/99 Iteration: 713 Training loss: 0.67081\n",
      "Epoch: 8/99 Iteration: 714 Training loss: 0.65989\n",
      "Epoch: 8/99 Iteration: 715 Training loss: 0.57490\n",
      "Epoch: 8/99 Iteration: 716 Training loss: 0.63275\n",
      "Epoch: 8/99 Iteration: 717 Training loss: 0.88809\n",
      "Epoch: 8/99 Iteration: 718 Training loss: 0.65841\n",
      "Epoch: 8/99 Iteration: 719 Training loss: 0.72388\n",
      "Epoch: 9/99 Iteration: 720 Training loss: 0.49242\n",
      "Epoch: 9/99 Iteration: 721 Training loss: 0.64399\n",
      "Epoch: 9/99 Iteration: 722 Training loss: 0.65951\n",
      "Epoch: 9/99 Iteration: 723 Training loss: 0.57682\n",
      "Epoch: 9/99 Iteration: 724 Training loss: 0.55995\n",
      "Epoch: 9/99 Iteration: 725 Training loss: 0.62523\n",
      "Epoch: 9/99 Iteration: 726 Training loss: 0.67650\n",
      "Epoch: 9/99 Iteration: 727 Training loss: 0.63611\n",
      "Epoch: 9/99 Iteration: 728 Training loss: 0.73480\n",
      "Epoch: 9/99 Iteration: 729 Training loss: 0.58343\n",
      "Epoch: 9/99 Iteration: 730 Training loss: 0.65424\n",
      "Epoch: 9/99 Iteration: 731 Training loss: 0.57744\n",
      "Epoch: 9/99 Iteration: 732 Training loss: 0.58527\n",
      "Epoch: 9/99 Iteration: 733 Training loss: 0.63680\n",
      "Epoch: 9/99 Iteration: 734 Training loss: 0.75488\n",
      "Epoch: 9/99 Iteration: 735 Training loss: 0.53186\n",
      "Epoch: 9/99 Iteration: 736 Training loss: 0.51358\n",
      "Epoch: 9/99 Iteration: 737 Training loss: 0.50447\n",
      "Epoch: 9/99 Iteration: 738 Training loss: 0.64121\n",
      "Epoch: 9/99 Iteration: 739 Training loss: 0.61250\n",
      "Epoch: 9/99 Iteration: 740 Training loss: 0.64855\n",
      "Epoch: 9/99 Iteration: 741 Training loss: 0.55954\n",
      "Epoch: 9/99 Iteration: 742 Training loss: 0.54718\n",
      "Epoch: 9/99 Iteration: 743 Training loss: 0.55683\n",
      "Epoch: 9/99 Iteration: 744 Training loss: 0.59306\n",
      "Epoch: 9/99 Iteration: 745 Training loss: 0.65612\n",
      "Epoch: 9/99 Iteration: 746 Training loss: 0.55817\n",
      "Epoch: 9/99 Iteration: 747 Training loss: 0.55215\n",
      "Epoch: 9/99 Iteration: 748 Training loss: 0.66315\n",
      "Epoch: 9/99 Iteration: 749 Training loss: 0.80401\n",
      "Epoch: 9/99 Iteration: 750 Training loss: 0.48628\n",
      "***\n",
      "Epoch: 9/99 Iteration: 750 Validation Acc: 0.8020\n",
      "***\n",
      "Epoch: 9/99 Iteration: 751 Training loss: 0.50641\n",
      "Epoch: 9/99 Iteration: 752 Training loss: 0.62553\n",
      "Epoch: 9/99 Iteration: 753 Training loss: 0.72488\n",
      "Epoch: 9/99 Iteration: 754 Training loss: 0.53957\n",
      "Epoch: 9/99 Iteration: 755 Training loss: 0.51655\n",
      "Epoch: 9/99 Iteration: 756 Training loss: 0.95822\n",
      "Epoch: 9/99 Iteration: 757 Training loss: 0.59515\n",
      "Epoch: 9/99 Iteration: 758 Training loss: 0.54403\n",
      "Epoch: 9/99 Iteration: 759 Training loss: 0.56670\n",
      "Epoch: 9/99 Iteration: 760 Training loss: 0.54911\n",
      "Epoch: 9/99 Iteration: 761 Training loss: 0.69825\n",
      "Epoch: 9/99 Iteration: 762 Training loss: 0.63532\n",
      "Epoch: 9/99 Iteration: 763 Training loss: 0.55997\n",
      "Epoch: 9/99 Iteration: 764 Training loss: 0.65002\n",
      "Epoch: 9/99 Iteration: 765 Training loss: 0.55820\n",
      "Epoch: 9/99 Iteration: 766 Training loss: 0.64307\n",
      "Epoch: 9/99 Iteration: 767 Training loss: 0.53752\n",
      "Epoch: 9/99 Iteration: 768 Training loss: 0.88613\n",
      "Epoch: 9/99 Iteration: 769 Training loss: 0.56196\n",
      "Epoch: 9/99 Iteration: 770 Training loss: 0.62161\n",
      "Epoch: 9/99 Iteration: 771 Training loss: 0.60372\n",
      "Epoch: 9/99 Iteration: 772 Training loss: 0.60160\n",
      "Epoch: 9/99 Iteration: 773 Training loss: 0.73540\n",
      "Epoch: 9/99 Iteration: 774 Training loss: 0.69112\n",
      "Epoch: 9/99 Iteration: 775 Training loss: 0.50987\n",
      "Epoch: 9/99 Iteration: 776 Training loss: 0.60914\n",
      "Epoch: 9/99 Iteration: 777 Training loss: 0.75913\n",
      "Epoch: 9/99 Iteration: 778 Training loss: 0.77455\n",
      "Epoch: 9/99 Iteration: 779 Training loss: 0.73579\n",
      "Epoch: 9/99 Iteration: 780 Training loss: 0.60544\n",
      "Epoch: 9/99 Iteration: 781 Training loss: 0.61890\n",
      "Epoch: 9/99 Iteration: 782 Training loss: 0.58546\n",
      "Epoch: 9/99 Iteration: 783 Training loss: 0.75270\n",
      "Epoch: 9/99 Iteration: 784 Training loss: 0.73982\n",
      "Epoch: 9/99 Iteration: 785 Training loss: 0.67983\n",
      "Epoch: 9/99 Iteration: 786 Training loss: 0.69541\n",
      "Epoch: 9/99 Iteration: 787 Training loss: 0.74128\n",
      "Epoch: 9/99 Iteration: 788 Training loss: 0.63679\n",
      "Epoch: 9/99 Iteration: 789 Training loss: 0.55394\n",
      "Epoch: 9/99 Iteration: 790 Training loss: 0.70481\n",
      "Epoch: 9/99 Iteration: 791 Training loss: 0.60890\n",
      "Epoch: 9/99 Iteration: 792 Training loss: 0.74995\n",
      "Epoch: 9/99 Iteration: 793 Training loss: 0.64347\n",
      "Epoch: 9/99 Iteration: 794 Training loss: 0.54139\n",
      "Epoch: 9/99 Iteration: 795 Training loss: 0.55677\n",
      "Epoch: 9/99 Iteration: 796 Training loss: 0.67024\n",
      "Epoch: 9/99 Iteration: 797 Training loss: 0.86860\n",
      "Epoch: 9/99 Iteration: 798 Training loss: 0.70468\n",
      "Epoch: 9/99 Iteration: 799 Training loss: 0.50808\n",
      "Epoch: 10/99 Iteration: 800 Training loss: 0.59812\n",
      "***\n",
      "Epoch: 10/99 Iteration: 800 Validation Acc: 0.7880\n",
      "***\n",
      "Epoch: 10/99 Iteration: 801 Training loss: 0.68532\n",
      "Epoch: 10/99 Iteration: 802 Training loss: 0.58625\n",
      "Epoch: 10/99 Iteration: 803 Training loss: 0.57498\n",
      "Epoch: 10/99 Iteration: 804 Training loss: 0.57446\n",
      "Epoch: 10/99 Iteration: 805 Training loss: 0.62263\n",
      "Epoch: 10/99 Iteration: 806 Training loss: 0.52108\n",
      "Epoch: 10/99 Iteration: 807 Training loss: 0.57850\n",
      "Epoch: 10/99 Iteration: 808 Training loss: 0.72382\n",
      "Epoch: 10/99 Iteration: 809 Training loss: 0.59647\n",
      "Epoch: 10/99 Iteration: 810 Training loss: 0.54832\n",
      "Epoch: 10/99 Iteration: 811 Training loss: 0.62020\n",
      "Epoch: 10/99 Iteration: 812 Training loss: 0.60407\n",
      "Epoch: 10/99 Iteration: 813 Training loss: 0.70923\n",
      "Epoch: 10/99 Iteration: 814 Training loss: 0.69626\n",
      "Epoch: 10/99 Iteration: 815 Training loss: 0.55451\n",
      "Epoch: 10/99 Iteration: 816 Training loss: 0.51375\n",
      "Epoch: 10/99 Iteration: 817 Training loss: 0.54588\n",
      "Epoch: 10/99 Iteration: 818 Training loss: 0.65828\n",
      "Epoch: 10/99 Iteration: 819 Training loss: 0.59179\n",
      "Epoch: 10/99 Iteration: 820 Training loss: 0.59290\n",
      "Epoch: 10/99 Iteration: 821 Training loss: 0.60403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/99 Iteration: 822 Training loss: 0.66868\n",
      "Epoch: 10/99 Iteration: 823 Training loss: 0.61303\n",
      "Epoch: 10/99 Iteration: 824 Training loss: 0.46198\n",
      "Epoch: 10/99 Iteration: 825 Training loss: 0.56877\n",
      "Epoch: 10/99 Iteration: 826 Training loss: 0.61189\n",
      "Epoch: 10/99 Iteration: 827 Training loss: 0.51822\n",
      "Epoch: 10/99 Iteration: 828 Training loss: 0.68249\n",
      "Epoch: 10/99 Iteration: 829 Training loss: 0.78428\n",
      "Epoch: 10/99 Iteration: 830 Training loss: 0.50193\n",
      "Epoch: 10/99 Iteration: 831 Training loss: 0.46784\n",
      "Epoch: 10/99 Iteration: 832 Training loss: 0.60191\n",
      "Epoch: 10/99 Iteration: 833 Training loss: 0.62569\n",
      "Epoch: 10/99 Iteration: 834 Training loss: 0.56264\n",
      "Epoch: 10/99 Iteration: 835 Training loss: 0.49594\n",
      "Epoch: 10/99 Iteration: 836 Training loss: 0.71858\n",
      "Epoch: 10/99 Iteration: 837 Training loss: 0.54529\n",
      "Epoch: 10/99 Iteration: 838 Training loss: 0.66146\n",
      "Epoch: 10/99 Iteration: 839 Training loss: 0.60061\n",
      "Epoch: 10/99 Iteration: 840 Training loss: 0.53982\n",
      "Epoch: 10/99 Iteration: 841 Training loss: 0.57062\n",
      "Epoch: 10/99 Iteration: 842 Training loss: 0.63221\n",
      "Epoch: 10/99 Iteration: 843 Training loss: 0.55273\n",
      "Epoch: 10/99 Iteration: 844 Training loss: 0.68040\n",
      "Epoch: 10/99 Iteration: 845 Training loss: 0.58675\n",
      "Epoch: 10/99 Iteration: 846 Training loss: 0.51430\n",
      "Epoch: 10/99 Iteration: 847 Training loss: 0.59270\n",
      "Epoch: 10/99 Iteration: 848 Training loss: 0.82264\n",
      "Epoch: 10/99 Iteration: 849 Training loss: 0.53829\n",
      "Epoch: 10/99 Iteration: 850 Training loss: 0.63599\n",
      "***\n",
      "Epoch: 10/99 Iteration: 850 Validation Acc: 0.7920\n",
      "***\n",
      "Epoch: 10/99 Iteration: 851 Training loss: 0.53312\n",
      "Epoch: 10/99 Iteration: 852 Training loss: 0.58683\n",
      "Epoch: 10/99 Iteration: 853 Training loss: 0.64104\n",
      "Epoch: 10/99 Iteration: 854 Training loss: 0.72244\n",
      "Epoch: 10/99 Iteration: 855 Training loss: 0.56794\n",
      "Epoch: 10/99 Iteration: 856 Training loss: 0.60589\n",
      "Epoch: 10/99 Iteration: 857 Training loss: 0.63609\n",
      "Epoch: 10/99 Iteration: 858 Training loss: 0.67601\n",
      "Epoch: 10/99 Iteration: 859 Training loss: 0.78539\n",
      "Epoch: 10/99 Iteration: 860 Training loss: 0.57957\n",
      "Epoch: 10/99 Iteration: 861 Training loss: 0.62717\n",
      "Epoch: 10/99 Iteration: 862 Training loss: 0.50770\n",
      "Epoch: 10/99 Iteration: 863 Training loss: 0.60463\n",
      "Epoch: 10/99 Iteration: 864 Training loss: 0.65606\n",
      "Epoch: 10/99 Iteration: 865 Training loss: 0.68515\n",
      "Epoch: 10/99 Iteration: 866 Training loss: 0.68862\n",
      "Epoch: 10/99 Iteration: 867 Training loss: 0.75601\n",
      "Epoch: 10/99 Iteration: 868 Training loss: 0.57477\n",
      "Epoch: 10/99 Iteration: 869 Training loss: 0.57145\n",
      "Epoch: 10/99 Iteration: 870 Training loss: 0.88272\n",
      "Epoch: 10/99 Iteration: 871 Training loss: 0.55126\n",
      "Epoch: 10/99 Iteration: 872 Training loss: 0.64702\n",
      "Epoch: 10/99 Iteration: 873 Training loss: 0.63784\n",
      "Epoch: 10/99 Iteration: 874 Training loss: 0.56311\n",
      "Epoch: 10/99 Iteration: 875 Training loss: 0.48609\n",
      "Epoch: 10/99 Iteration: 876 Training loss: 0.59374\n",
      "Epoch: 10/99 Iteration: 877 Training loss: 0.79589\n",
      "Epoch: 10/99 Iteration: 878 Training loss: 0.66329\n",
      "Epoch: 10/99 Iteration: 879 Training loss: 0.65158\n",
      "Epoch: 11/99 Iteration: 880 Training loss: 0.50277\n",
      "Epoch: 11/99 Iteration: 881 Training loss: 0.63712\n",
      "Epoch: 11/99 Iteration: 882 Training loss: 0.60971\n",
      "Epoch: 11/99 Iteration: 883 Training loss: 0.55672\n",
      "Epoch: 11/99 Iteration: 884 Training loss: 0.55628\n",
      "Epoch: 11/99 Iteration: 885 Training loss: 0.59725\n",
      "Epoch: 11/99 Iteration: 886 Training loss: 0.72252\n",
      "Epoch: 11/99 Iteration: 887 Training loss: 0.61749\n",
      "Epoch: 11/99 Iteration: 888 Training loss: 0.70982\n",
      "Epoch: 11/99 Iteration: 889 Training loss: 0.51445\n",
      "Epoch: 11/99 Iteration: 890 Training loss: 0.42483\n",
      "Epoch: 11/99 Iteration: 891 Training loss: 0.46856\n",
      "Epoch: 11/99 Iteration: 892 Training loss: 0.61790\n",
      "Epoch: 11/99 Iteration: 893 Training loss: 0.72477\n",
      "Epoch: 11/99 Iteration: 894 Training loss: 0.76983\n",
      "Epoch: 11/99 Iteration: 895 Training loss: 0.50285\n",
      "Epoch: 11/99 Iteration: 896 Training loss: 0.47896\n",
      "Epoch: 11/99 Iteration: 897 Training loss: 0.45330\n",
      "Epoch: 11/99 Iteration: 898 Training loss: 0.45075\n",
      "Epoch: 11/99 Iteration: 899 Training loss: 0.51815\n",
      "Epoch: 11/99 Iteration: 900 Training loss: 0.66619\n",
      "***\n",
      "Epoch: 11/99 Iteration: 900 Validation Acc: 0.7880\n",
      "***\n",
      "Epoch: 11/99 Iteration: 901 Training loss: 0.51126\n",
      "Epoch: 11/99 Iteration: 902 Training loss: 0.59833\n",
      "Epoch: 11/99 Iteration: 903 Training loss: 0.60804\n",
      "Epoch: 11/99 Iteration: 904 Training loss: 0.41359\n",
      "Epoch: 11/99 Iteration: 905 Training loss: 0.59265\n",
      "Epoch: 11/99 Iteration: 906 Training loss: 0.61679\n",
      "Epoch: 11/99 Iteration: 907 Training loss: 0.39373\n",
      "Epoch: 11/99 Iteration: 908 Training loss: 0.67725\n",
      "Epoch: 11/99 Iteration: 909 Training loss: 0.70225\n",
      "Epoch: 11/99 Iteration: 910 Training loss: 0.39884\n",
      "Epoch: 11/99 Iteration: 911 Training loss: 0.38087\n",
      "Epoch: 11/99 Iteration: 912 Training loss: 0.56623\n",
      "Epoch: 11/99 Iteration: 913 Training loss: 0.69982\n",
      "Epoch: 11/99 Iteration: 914 Training loss: 0.54877\n",
      "Epoch: 11/99 Iteration: 915 Training loss: 0.50998\n",
      "Epoch: 11/99 Iteration: 916 Training loss: 0.64543\n",
      "Epoch: 11/99 Iteration: 917 Training loss: 0.60586\n",
      "Epoch: 11/99 Iteration: 918 Training loss: 0.46901\n",
      "Epoch: 11/99 Iteration: 919 Training loss: 0.49522\n",
      "Epoch: 11/99 Iteration: 920 Training loss: 0.53548\n",
      "Epoch: 11/99 Iteration: 921 Training loss: 0.42957\n",
      "Epoch: 11/99 Iteration: 922 Training loss: 0.52792\n",
      "Epoch: 11/99 Iteration: 923 Training loss: 0.44213\n",
      "Epoch: 11/99 Iteration: 924 Training loss: 0.67674\n",
      "Epoch: 11/99 Iteration: 925 Training loss: 0.53599\n",
      "Epoch: 11/99 Iteration: 926 Training loss: 0.51876\n",
      "Epoch: 11/99 Iteration: 927 Training loss: 0.62979\n",
      "Epoch: 11/99 Iteration: 928 Training loss: 0.81825\n",
      "Epoch: 11/99 Iteration: 929 Training loss: 0.46526\n",
      "Epoch: 11/99 Iteration: 930 Training loss: 0.48755\n",
      "Epoch: 11/99 Iteration: 931 Training loss: 0.48386\n",
      "Epoch: 11/99 Iteration: 932 Training loss: 0.56364\n",
      "Epoch: 11/99 Iteration: 933 Training loss: 0.70621\n",
      "Epoch: 11/99 Iteration: 934 Training loss: 0.68075\n",
      "Epoch: 11/99 Iteration: 935 Training loss: 0.69610\n",
      "Epoch: 11/99 Iteration: 936 Training loss: 0.58643\n",
      "Epoch: 11/99 Iteration: 937 Training loss: 0.58769\n",
      "Epoch: 11/99 Iteration: 938 Training loss: 0.55348\n",
      "Epoch: 11/99 Iteration: 939 Training loss: 0.78169\n",
      "Epoch: 11/99 Iteration: 940 Training loss: 0.49626\n",
      "Epoch: 11/99 Iteration: 941 Training loss: 0.59651\n",
      "Epoch: 11/99 Iteration: 942 Training loss: 0.49646\n",
      "Epoch: 11/99 Iteration: 943 Training loss: 0.59790\n",
      "Epoch: 11/99 Iteration: 944 Training loss: 0.49399\n",
      "Epoch: 11/99 Iteration: 945 Training loss: 0.63106\n",
      "Epoch: 11/99 Iteration: 946 Training loss: 0.72794\n",
      "Epoch: 11/99 Iteration: 947 Training loss: 0.77665\n",
      "Epoch: 11/99 Iteration: 948 Training loss: 0.60121\n",
      "Epoch: 11/99 Iteration: 949 Training loss: 0.50874\n",
      "Epoch: 11/99 Iteration: 950 Training loss: 0.65020\n",
      "***\n",
      "Epoch: 11/99 Iteration: 950 Validation Acc: 0.8150\n",
      "***\n",
      "Epoch: 11/99 Iteration: 951 Training loss: 0.47373\n",
      "Epoch: 11/99 Iteration: 952 Training loss: 0.56061\n",
      "Epoch: 11/99 Iteration: 953 Training loss: 0.54265\n",
      "Epoch: 11/99 Iteration: 954 Training loss: 0.56180\n",
      "Epoch: 11/99 Iteration: 955 Training loss: 0.46765\n",
      "Epoch: 11/99 Iteration: 956 Training loss: 0.50271\n",
      "Epoch: 11/99 Iteration: 957 Training loss: 0.74477\n",
      "Epoch: 11/99 Iteration: 958 Training loss: 0.70594\n",
      "Epoch: 11/99 Iteration: 959 Training loss: 0.52708\n",
      "Epoch: 12/99 Iteration: 960 Training loss: 0.49630\n",
      "Epoch: 12/99 Iteration: 961 Training loss: 0.58533\n",
      "Epoch: 12/99 Iteration: 962 Training loss: 0.59659\n",
      "Epoch: 12/99 Iteration: 963 Training loss: 0.54173\n",
      "Epoch: 12/99 Iteration: 964 Training loss: 0.47218\n",
      "Epoch: 12/99 Iteration: 965 Training loss: 0.49045\n",
      "Epoch: 12/99 Iteration: 966 Training loss: 0.46627\n",
      "Epoch: 12/99 Iteration: 967 Training loss: 0.47699\n",
      "Epoch: 12/99 Iteration: 968 Training loss: 0.62479\n",
      "Epoch: 12/99 Iteration: 969 Training loss: 0.48421\n",
      "Epoch: 12/99 Iteration: 970 Training loss: 0.51483\n",
      "Epoch: 12/99 Iteration: 971 Training loss: 0.56837\n",
      "Epoch: 12/99 Iteration: 972 Training loss: 0.52564\n",
      "Epoch: 12/99 Iteration: 973 Training loss: 0.61635\n",
      "Epoch: 12/99 Iteration: 974 Training loss: 0.72028\n",
      "Epoch: 12/99 Iteration: 975 Training loss: 0.52056\n",
      "Epoch: 12/99 Iteration: 976 Training loss: 0.41869\n",
      "Epoch: 12/99 Iteration: 977 Training loss: 0.48759\n",
      "Epoch: 12/99 Iteration: 978 Training loss: 0.57585\n",
      "Epoch: 12/99 Iteration: 979 Training loss: 0.61380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/99 Iteration: 980 Training loss: 0.60285\n",
      "Epoch: 12/99 Iteration: 981 Training loss: 0.51275\n",
      "Epoch: 12/99 Iteration: 982 Training loss: 0.58344\n",
      "Epoch: 12/99 Iteration: 983 Training loss: 0.52368\n",
      "Epoch: 12/99 Iteration: 984 Training loss: 0.53380\n",
      "Epoch: 12/99 Iteration: 985 Training loss: 0.56838\n",
      "Epoch: 12/99 Iteration: 986 Training loss: 0.65917\n",
      "Epoch: 12/99 Iteration: 987 Training loss: 0.48929\n",
      "Epoch: 12/99 Iteration: 988 Training loss: 0.62937\n",
      "Epoch: 12/99 Iteration: 989 Training loss: 0.60670\n",
      "Epoch: 12/99 Iteration: 990 Training loss: 0.49073\n",
      "Epoch: 12/99 Iteration: 991 Training loss: 0.55758\n",
      "Epoch: 12/99 Iteration: 992 Training loss: 0.61940\n",
      "Epoch: 12/99 Iteration: 993 Training loss: 0.55297\n",
      "Epoch: 12/99 Iteration: 994 Training loss: 0.47776\n",
      "Epoch: 12/99 Iteration: 995 Training loss: 0.50915\n",
      "Epoch: 12/99 Iteration: 996 Training loss: 0.75522\n",
      "Epoch: 12/99 Iteration: 997 Training loss: 0.48180\n",
      "Epoch: 12/99 Iteration: 998 Training loss: 0.49024\n",
      "Epoch: 12/99 Iteration: 999 Training loss: 0.55295\n",
      "Epoch: 12/99 Iteration: 1000 Training loss: 0.46996\n",
      "***\n",
      "Epoch: 12/99 Iteration: 1000 Validation Acc: 0.8090\n",
      "***\n",
      "Epoch: 12/99 Iteration: 1001 Training loss: 0.52481\n",
      "Epoch: 12/99 Iteration: 1002 Training loss: 0.53776\n",
      "Epoch: 12/99 Iteration: 1003 Training loss: 0.53862\n",
      "Epoch: 12/99 Iteration: 1004 Training loss: 0.68672\n",
      "Epoch: 12/99 Iteration: 1005 Training loss: 0.45344\n",
      "Epoch: 12/99 Iteration: 1006 Training loss: 0.71397\n",
      "Epoch: 12/99 Iteration: 1007 Training loss: 0.52903\n",
      "Epoch: 12/99 Iteration: 1008 Training loss: 0.80411\n",
      "Epoch: 12/99 Iteration: 1009 Training loss: 0.53150\n",
      "Epoch: 12/99 Iteration: 1010 Training loss: 0.55967\n",
      "Epoch: 12/99 Iteration: 1011 Training loss: 0.64796\n",
      "Epoch: 12/99 Iteration: 1012 Training loss: 0.61114\n",
      "Epoch: 12/99 Iteration: 1013 Training loss: 0.67098\n",
      "Epoch: 12/99 Iteration: 1014 Training loss: 0.77563\n",
      "Epoch: 12/99 Iteration: 1015 Training loss: 0.47727\n",
      "Epoch: 12/99 Iteration: 1016 Training loss: 0.67322\n",
      "Epoch: 12/99 Iteration: 1017 Training loss: 0.71122\n",
      "Epoch: 12/99 Iteration: 1018 Training loss: 0.66985\n",
      "Epoch: 12/99 Iteration: 1019 Training loss: 0.72752\n",
      "Epoch: 12/99 Iteration: 1020 Training loss: 0.53249\n",
      "Epoch: 12/99 Iteration: 1021 Training loss: 0.53676\n",
      "Epoch: 12/99 Iteration: 1022 Training loss: 0.50549\n",
      "Epoch: 12/99 Iteration: 1023 Training loss: 0.66953\n",
      "Epoch: 12/99 Iteration: 1024 Training loss: 0.60352\n",
      "Epoch: 12/99 Iteration: 1025 Training loss: 0.54035\n",
      "Epoch: 12/99 Iteration: 1026 Training loss: 0.57897\n",
      "Epoch: 12/99 Iteration: 1027 Training loss: 0.70137\n",
      "Epoch: 12/99 Iteration: 1028 Training loss: 0.67568\n",
      "Epoch: 12/99 Iteration: 1029 Training loss: 0.52526\n",
      "Epoch: 12/99 Iteration: 1030 Training loss: 0.63048\n",
      "Epoch: 12/99 Iteration: 1031 Training loss: 0.59982\n",
      "Epoch: 12/99 Iteration: 1032 Training loss: 0.53742\n",
      "Epoch: 12/99 Iteration: 1033 Training loss: 0.53592\n",
      "Epoch: 12/99 Iteration: 1034 Training loss: 0.53190\n",
      "Epoch: 12/99 Iteration: 1035 Training loss: 0.43701\n",
      "Epoch: 12/99 Iteration: 1036 Training loss: 0.62061\n",
      "Epoch: 12/99 Iteration: 1037 Training loss: 0.66318\n",
      "Epoch: 12/99 Iteration: 1038 Training loss: 0.54481\n",
      "Epoch: 12/99 Iteration: 1039 Training loss: 0.49441\n",
      "Epoch: 13/99 Iteration: 1040 Training loss: 0.61557\n",
      "Epoch: 13/99 Iteration: 1041 Training loss: 0.55899\n",
      "Epoch: 13/99 Iteration: 1042 Training loss: 0.58514\n",
      "Epoch: 13/99 Iteration: 1043 Training loss: 0.62005\n",
      "Epoch: 13/99 Iteration: 1044 Training loss: 0.45722\n",
      "Epoch: 13/99 Iteration: 1045 Training loss: 0.57934\n",
      "Epoch: 13/99 Iteration: 1046 Training loss: 0.61514\n",
      "Epoch: 13/99 Iteration: 1047 Training loss: 0.47937\n",
      "Epoch: 13/99 Iteration: 1048 Training loss: 0.68490\n",
      "Epoch: 13/99 Iteration: 1049 Training loss: 0.51979\n",
      "Epoch: 13/99 Iteration: 1050 Training loss: 0.50613\n",
      "***\n",
      "Epoch: 13/99 Iteration: 1050 Validation Acc: 0.8130\n",
      "***\n",
      "Epoch: 13/99 Iteration: 1051 Training loss: 0.54967\n",
      "Epoch: 13/99 Iteration: 1052 Training loss: 0.66093\n",
      "Epoch: 13/99 Iteration: 1053 Training loss: 0.63478\n",
      "Epoch: 13/99 Iteration: 1054 Training loss: 0.68481\n",
      "Epoch: 13/99 Iteration: 1055 Training loss: 0.40177\n",
      "Epoch: 13/99 Iteration: 1056 Training loss: 0.39327\n",
      "Epoch: 13/99 Iteration: 1057 Training loss: 0.36018\n",
      "Epoch: 13/99 Iteration: 1058 Training loss: 0.50803\n",
      "Epoch: 13/99 Iteration: 1059 Training loss: 0.52874\n",
      "Epoch: 13/99 Iteration: 1060 Training loss: 0.52388\n",
      "Epoch: 13/99 Iteration: 1061 Training loss: 0.58476\n",
      "Epoch: 13/99 Iteration: 1062 Training loss: 0.51792\n",
      "Epoch: 13/99 Iteration: 1063 Training loss: 0.54041\n",
      "Epoch: 13/99 Iteration: 1064 Training loss: 0.50266\n",
      "Epoch: 13/99 Iteration: 1065 Training loss: 0.48714\n",
      "Epoch: 13/99 Iteration: 1066 Training loss: 0.58658\n",
      "Epoch: 13/99 Iteration: 1067 Training loss: 0.33599\n",
      "Epoch: 13/99 Iteration: 1068 Training loss: 0.60631\n",
      "Epoch: 13/99 Iteration: 1069 Training loss: 0.70515\n",
      "Epoch: 13/99 Iteration: 1070 Training loss: 0.44165\n",
      "Epoch: 13/99 Iteration: 1071 Training loss: 0.51836\n",
      "Epoch: 13/99 Iteration: 1072 Training loss: 0.57893\n",
      "Epoch: 13/99 Iteration: 1073 Training loss: 0.51537\n",
      "Epoch: 13/99 Iteration: 1074 Training loss: 0.54880\n",
      "Epoch: 13/99 Iteration: 1075 Training loss: 0.55061\n",
      "Epoch: 13/99 Iteration: 1076 Training loss: 0.63020\n",
      "Epoch: 13/99 Iteration: 1077 Training loss: 0.47326\n",
      "Epoch: 13/99 Iteration: 1078 Training loss: 0.48016\n",
      "Epoch: 13/99 Iteration: 1079 Training loss: 0.50787\n",
      "Epoch: 13/99 Iteration: 1080 Training loss: 0.51137\n",
      "Epoch: 13/99 Iteration: 1081 Training loss: 0.48509\n",
      "Epoch: 13/99 Iteration: 1082 Training loss: 0.65620\n",
      "Epoch: 13/99 Iteration: 1083 Training loss: 0.43820\n",
      "Epoch: 13/99 Iteration: 1084 Training loss: 0.68166\n",
      "Epoch: 13/99 Iteration: 1085 Training loss: 0.51822\n",
      "Epoch: 13/99 Iteration: 1086 Training loss: 0.55415\n",
      "Epoch: 13/99 Iteration: 1087 Training loss: 0.64720\n",
      "Epoch: 13/99 Iteration: 1088 Training loss: 0.82426\n",
      "Epoch: 13/99 Iteration: 1089 Training loss: 0.42172\n",
      "Epoch: 13/99 Iteration: 1090 Training loss: 0.69257\n",
      "Epoch: 13/99 Iteration: 1091 Training loss: 0.57488\n",
      "Epoch: 13/99 Iteration: 1092 Training loss: 0.56986\n",
      "Epoch: 13/99 Iteration: 1093 Training loss: 0.65617\n",
      "Epoch: 13/99 Iteration: 1094 Training loss: 0.68978\n",
      "Epoch: 13/99 Iteration: 1095 Training loss: 0.46402\n",
      "Epoch: 13/99 Iteration: 1096 Training loss: 0.72836\n",
      "Epoch: 13/99 Iteration: 1097 Training loss: 0.69628\n",
      "Epoch: 13/99 Iteration: 1098 Training loss: 0.55940\n",
      "Epoch: 13/99 Iteration: 1099 Training loss: 0.67084\n",
      "Epoch: 13/99 Iteration: 1100 Training loss: 0.49536\n",
      "***\n",
      "Epoch: 13/99 Iteration: 1100 Validation Acc: 0.8170\n",
      "***\n",
      "Epoch: 13/99 Iteration: 1101 Training loss: 0.60099\n",
      "Epoch: 13/99 Iteration: 1102 Training loss: 0.54128\n",
      "Epoch: 13/99 Iteration: 1103 Training loss: 0.55770\n",
      "Epoch: 13/99 Iteration: 1104 Training loss: 0.57118\n",
      "Epoch: 13/99 Iteration: 1105 Training loss: 0.69847\n",
      "Epoch: 13/99 Iteration: 1106 Training loss: 0.64132\n",
      "Epoch: 13/99 Iteration: 1107 Training loss: 0.71787\n",
      "Epoch: 13/99 Iteration: 1108 Training loss: 0.54488\n",
      "Epoch: 13/99 Iteration: 1109 Training loss: 0.46725\n",
      "Epoch: 13/99 Iteration: 1110 Training loss: 0.59395\n",
      "Epoch: 13/99 Iteration: 1111 Training loss: 0.43203\n",
      "Epoch: 13/99 Iteration: 1112 Training loss: 0.54446\n",
      "Epoch: 13/99 Iteration: 1113 Training loss: 0.63360\n",
      "Epoch: 13/99 Iteration: 1114 Training loss: 0.46946\n",
      "Epoch: 13/99 Iteration: 1115 Training loss: 0.43962\n",
      "Epoch: 13/99 Iteration: 1116 Training loss: 0.43489\n",
      "Epoch: 13/99 Iteration: 1117 Training loss: 0.68241\n",
      "Epoch: 13/99 Iteration: 1118 Training loss: 0.62617\n",
      "Epoch: 13/99 Iteration: 1119 Training loss: 0.57164\n",
      "Epoch: 14/99 Iteration: 1120 Training loss: 0.44768\n",
      "Epoch: 14/99 Iteration: 1121 Training loss: 0.62420\n",
      "Epoch: 14/99 Iteration: 1122 Training loss: 0.73079\n",
      "Epoch: 14/99 Iteration: 1123 Training loss: 0.48228\n",
      "Epoch: 14/99 Iteration: 1124 Training loss: 0.54500\n",
      "Epoch: 14/99 Iteration: 1125 Training loss: 0.42077\n",
      "Epoch: 14/99 Iteration: 1126 Training loss: 0.51617\n",
      "Epoch: 14/99 Iteration: 1127 Training loss: 0.43859\n",
      "Epoch: 14/99 Iteration: 1128 Training loss: 0.70232\n",
      "Epoch: 14/99 Iteration: 1129 Training loss: 0.56466\n",
      "Epoch: 14/99 Iteration: 1130 Training loss: 0.53551\n",
      "Epoch: 14/99 Iteration: 1131 Training loss: 0.56454\n",
      "Epoch: 14/99 Iteration: 1132 Training loss: 0.54956\n",
      "Epoch: 14/99 Iteration: 1133 Training loss: 0.48727\n",
      "Epoch: 14/99 Iteration: 1134 Training loss: 0.77735\n",
      "Epoch: 14/99 Iteration: 1135 Training loss: 0.43075\n",
      "Epoch: 14/99 Iteration: 1136 Training loss: 0.43560\n",
      "Epoch: 14/99 Iteration: 1137 Training loss: 0.37205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/99 Iteration: 1138 Training loss: 0.49874\n",
      "Epoch: 14/99 Iteration: 1139 Training loss: 0.45835\n",
      "Epoch: 14/99 Iteration: 1140 Training loss: 0.61753\n",
      "Epoch: 14/99 Iteration: 1141 Training loss: 0.44287\n",
      "Epoch: 14/99 Iteration: 1142 Training loss: 0.56069\n",
      "Epoch: 14/99 Iteration: 1143 Training loss: 0.60306\n",
      "Epoch: 14/99 Iteration: 1144 Training loss: 0.47669\n",
      "Epoch: 14/99 Iteration: 1145 Training loss: 0.45848\n",
      "Epoch: 14/99 Iteration: 1146 Training loss: 0.52084\n",
      "Epoch: 14/99 Iteration: 1147 Training loss: 0.38961\n",
      "Epoch: 14/99 Iteration: 1148 Training loss: 0.52953\n",
      "Epoch: 14/99 Iteration: 1149 Training loss: 0.76411\n",
      "Epoch: 14/99 Iteration: 1150 Training loss: 0.43552\n",
      "***\n",
      "Epoch: 14/99 Iteration: 1150 Validation Acc: 0.8160\n",
      "***\n",
      "Epoch: 14/99 Iteration: 1151 Training loss: 0.31313\n",
      "Epoch: 14/99 Iteration: 1152 Training loss: 0.50626\n",
      "Epoch: 14/99 Iteration: 1153 Training loss: 0.70039\n",
      "Epoch: 14/99 Iteration: 1154 Training loss: 0.48064\n",
      "Epoch: 14/99 Iteration: 1155 Training loss: 0.53311\n",
      "Epoch: 14/99 Iteration: 1156 Training loss: 0.63457\n",
      "Epoch: 14/99 Iteration: 1157 Training loss: 0.59727\n",
      "Epoch: 14/99 Iteration: 1158 Training loss: 0.48230\n",
      "Epoch: 14/99 Iteration: 1159 Training loss: 0.46328\n",
      "Epoch: 14/99 Iteration: 1160 Training loss: 0.45935\n",
      "Epoch: 14/99 Iteration: 1161 Training loss: 0.50997\n",
      "Epoch: 14/99 Iteration: 1162 Training loss: 0.58500\n",
      "Epoch: 14/99 Iteration: 1163 Training loss: 0.45570\n",
      "Epoch: 14/99 Iteration: 1164 Training loss: 0.74658\n",
      "Epoch: 14/99 Iteration: 1165 Training loss: 0.47621\n",
      "Epoch: 14/99 Iteration: 1166 Training loss: 0.54233\n",
      "Epoch: 14/99 Iteration: 1167 Training loss: 0.45230\n",
      "Epoch: 14/99 Iteration: 1168 Training loss: 0.77181\n",
      "Epoch: 14/99 Iteration: 1169 Training loss: 0.57359\n",
      "Epoch: 14/99 Iteration: 1170 Training loss: 0.58542\n",
      "Epoch: 14/99 Iteration: 1171 Training loss: 0.48229\n",
      "Epoch: 14/99 Iteration: 1172 Training loss: 0.63479\n",
      "Epoch: 14/99 Iteration: 1173 Training loss: 0.59262\n",
      "Epoch: 14/99 Iteration: 1174 Training loss: 0.54327\n",
      "Epoch: 14/99 Iteration: 1175 Training loss: 0.39881\n",
      "Epoch: 14/99 Iteration: 1176 Training loss: 0.67209\n",
      "Epoch: 14/99 Iteration: 1177 Training loss: 0.60131\n",
      "Epoch: 14/99 Iteration: 1178 Training loss: 0.69154\n",
      "Epoch: 14/99 Iteration: 1179 Training loss: 0.63174\n",
      "Epoch: 14/99 Iteration: 1180 Training loss: 0.49649\n",
      "Epoch: 14/99 Iteration: 1181 Training loss: 0.53178\n",
      "Epoch: 14/99 Iteration: 1182 Training loss: 0.43513\n",
      "Epoch: 14/99 Iteration: 1183 Training loss: 0.57339\n",
      "Epoch: 14/99 Iteration: 1184 Training loss: 0.65248\n",
      "Epoch: 14/99 Iteration: 1185 Training loss: 0.66589\n",
      "Epoch: 14/99 Iteration: 1186 Training loss: 0.71373\n",
      "Epoch: 14/99 Iteration: 1187 Training loss: 0.70366\n",
      "Epoch: 14/99 Iteration: 1188 Training loss: 0.40419\n",
      "Epoch: 14/99 Iteration: 1189 Training loss: 0.47293\n",
      "Epoch: 14/99 Iteration: 1190 Training loss: 0.77974\n",
      "Epoch: 14/99 Iteration: 1191 Training loss: 0.49721\n",
      "Epoch: 14/99 Iteration: 1192 Training loss: 0.57034\n",
      "Epoch: 14/99 Iteration: 1193 Training loss: 0.55388\n",
      "Epoch: 14/99 Iteration: 1194 Training loss: 0.49060\n",
      "Epoch: 14/99 Iteration: 1195 Training loss: 0.44935\n",
      "Epoch: 14/99 Iteration: 1196 Training loss: 0.64143\n",
      "Epoch: 14/99 Iteration: 1197 Training loss: 0.64661\n",
      "Epoch: 14/99 Iteration: 1198 Training loss: 0.62839\n",
      "Epoch: 14/99 Iteration: 1199 Training loss: 0.62375\n",
      "Epoch: 15/99 Iteration: 1200 Training loss: 0.46071\n",
      "***\n",
      "Epoch: 15/99 Iteration: 1200 Validation Acc: 0.8210\n",
      "***\n",
      "Epoch: 15/99 Iteration: 1201 Training loss: 0.56530\n",
      "Epoch: 15/99 Iteration: 1202 Training loss: 0.62129\n",
      "Epoch: 15/99 Iteration: 1203 Training loss: 0.54612\n",
      "Epoch: 15/99 Iteration: 1204 Training loss: 0.43819\n",
      "Epoch: 15/99 Iteration: 1205 Training loss: 0.48083\n",
      "Epoch: 15/99 Iteration: 1206 Training loss: 0.48297\n",
      "Epoch: 15/99 Iteration: 1207 Training loss: 0.48700\n",
      "Epoch: 15/99 Iteration: 1208 Training loss: 0.67686\n",
      "Epoch: 15/99 Iteration: 1209 Training loss: 0.47518\n",
      "Epoch: 15/99 Iteration: 1210 Training loss: 0.38068\n",
      "Epoch: 15/99 Iteration: 1211 Training loss: 0.44276\n",
      "Epoch: 15/99 Iteration: 1212 Training loss: 0.42555\n",
      "Epoch: 15/99 Iteration: 1213 Training loss: 0.51264\n",
      "Epoch: 15/99 Iteration: 1214 Training loss: 0.62647\n",
      "Epoch: 15/99 Iteration: 1215 Training loss: 0.41926\n",
      "Epoch: 15/99 Iteration: 1216 Training loss: 0.37887\n",
      "Epoch: 15/99 Iteration: 1217 Training loss: 0.44480\n",
      "Epoch: 15/99 Iteration: 1218 Training loss: 0.53387\n",
      "Epoch: 15/99 Iteration: 1219 Training loss: 0.50111\n",
      "Epoch: 15/99 Iteration: 1220 Training loss: 0.47320\n",
      "Epoch: 15/99 Iteration: 1221 Training loss: 0.43098\n",
      "Epoch: 15/99 Iteration: 1222 Training loss: 0.38715\n",
      "Epoch: 15/99 Iteration: 1223 Training loss: 0.51010\n",
      "Epoch: 15/99 Iteration: 1224 Training loss: 0.46089\n",
      "Epoch: 15/99 Iteration: 1225 Training loss: 0.43403\n",
      "Epoch: 15/99 Iteration: 1226 Training loss: 0.53954\n",
      "Epoch: 15/99 Iteration: 1227 Training loss: 0.41092\n",
      "Epoch: 15/99 Iteration: 1228 Training loss: 0.55574\n",
      "Epoch: 15/99 Iteration: 1229 Training loss: 0.67428\n",
      "Epoch: 15/99 Iteration: 1230 Training loss: 0.37076\n",
      "Epoch: 15/99 Iteration: 1231 Training loss: 0.43879\n",
      "Epoch: 15/99 Iteration: 1232 Training loss: 0.51970\n",
      "Epoch: 15/99 Iteration: 1233 Training loss: 0.34502\n",
      "Epoch: 15/99 Iteration: 1234 Training loss: 0.40482\n",
      "Epoch: 15/99 Iteration: 1235 Training loss: 0.49534\n",
      "Epoch: 15/99 Iteration: 1236 Training loss: 0.63104\n",
      "Epoch: 15/99 Iteration: 1237 Training loss: 0.37924\n",
      "Epoch: 15/99 Iteration: 1238 Training loss: 0.41643\n",
      "Epoch: 15/99 Iteration: 1239 Training loss: 0.46582\n",
      "Epoch: 15/99 Iteration: 1240 Training loss: 0.52279\n",
      "Epoch: 15/99 Iteration: 1241 Training loss: 0.40893\n",
      "Epoch: 15/99 Iteration: 1242 Training loss: 0.48596\n",
      "Epoch: 15/99 Iteration: 1243 Training loss: 0.33506\n",
      "Epoch: 15/99 Iteration: 1244 Training loss: 0.63934\n",
      "Epoch: 15/99 Iteration: 1245 Training loss: 0.43105\n",
      "Epoch: 15/99 Iteration: 1246 Training loss: 0.56931\n",
      "Epoch: 15/99 Iteration: 1247 Training loss: 0.53970\n",
      "Epoch: 15/99 Iteration: 1248 Training loss: 0.71598\n",
      "Epoch: 15/99 Iteration: 1249 Training loss: 0.41133\n",
      "Epoch: 15/99 Iteration: 1250 Training loss: 0.46642\n",
      "***\n",
      "Epoch: 15/99 Iteration: 1250 Validation Acc: 0.8250\n",
      "***\n",
      "Epoch: 15/99 Iteration: 1251 Training loss: 0.51371\n",
      "Epoch: 15/99 Iteration: 1252 Training loss: 0.51974\n",
      "Epoch: 15/99 Iteration: 1253 Training loss: 0.54519\n",
      "Epoch: 15/99 Iteration: 1254 Training loss: 0.51907\n",
      "Epoch: 15/99 Iteration: 1255 Training loss: 0.46409\n",
      "Epoch: 15/99 Iteration: 1256 Training loss: 0.58791\n",
      "Epoch: 15/99 Iteration: 1257 Training loss: 0.53733\n",
      "Epoch: 15/99 Iteration: 1258 Training loss: 0.50419\n",
      "Epoch: 15/99 Iteration: 1259 Training loss: 0.53547\n",
      "Epoch: 15/99 Iteration: 1260 Training loss: 0.44198\n",
      "Epoch: 15/99 Iteration: 1261 Training loss: 0.62000\n",
      "Epoch: 15/99 Iteration: 1262 Training loss: 0.45091\n",
      "Epoch: 15/99 Iteration: 1263 Training loss: 0.51637\n",
      "Epoch: 15/99 Iteration: 1264 Training loss: 0.58355\n",
      "Epoch: 15/99 Iteration: 1265 Training loss: 0.67469\n",
      "Epoch: 15/99 Iteration: 1266 Training loss: 0.58338\n",
      "Epoch: 15/99 Iteration: 1267 Training loss: 0.53400\n",
      "Epoch: 15/99 Iteration: 1268 Training loss: 0.64973\n",
      "Epoch: 15/99 Iteration: 1269 Training loss: 0.42489\n",
      "Epoch: 15/99 Iteration: 1270 Training loss: 0.62442\n",
      "Epoch: 15/99 Iteration: 1271 Training loss: 0.59123\n",
      "Epoch: 15/99 Iteration: 1272 Training loss: 0.50994\n",
      "Epoch: 15/99 Iteration: 1273 Training loss: 0.41335\n",
      "Epoch: 15/99 Iteration: 1274 Training loss: 0.48181\n",
      "Epoch: 15/99 Iteration: 1275 Training loss: 0.49277\n",
      "Epoch: 15/99 Iteration: 1276 Training loss: 0.65287\n",
      "Epoch: 15/99 Iteration: 1277 Training loss: 0.75644\n",
      "Epoch: 15/99 Iteration: 1278 Training loss: 0.49421\n",
      "Epoch: 15/99 Iteration: 1279 Training loss: 0.57126\n",
      "Epoch: 16/99 Iteration: 1280 Training loss: 0.50532\n",
      "Epoch: 16/99 Iteration: 1281 Training loss: 0.61671\n",
      "Epoch: 16/99 Iteration: 1282 Training loss: 0.50398\n",
      "Epoch: 16/99 Iteration: 1283 Training loss: 0.62915\n",
      "Epoch: 16/99 Iteration: 1284 Training loss: 0.57394\n",
      "Epoch: 16/99 Iteration: 1285 Training loss: 0.46467\n",
      "Epoch: 16/99 Iteration: 1286 Training loss: 0.43261\n",
      "Epoch: 16/99 Iteration: 1287 Training loss: 0.47795\n",
      "Epoch: 16/99 Iteration: 1288 Training loss: 0.63693\n",
      "Epoch: 16/99 Iteration: 1289 Training loss: 0.47278\n",
      "Epoch: 16/99 Iteration: 1290 Training loss: 0.43772\n",
      "Epoch: 16/99 Iteration: 1291 Training loss: 0.44711\n",
      "Epoch: 16/99 Iteration: 1292 Training loss: 0.48918\n",
      "Epoch: 16/99 Iteration: 1293 Training loss: 0.62466\n",
      "Epoch: 16/99 Iteration: 1294 Training loss: 0.68886\n",
      "Epoch: 16/99 Iteration: 1295 Training loss: 0.59804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/99 Iteration: 1296 Training loss: 0.44282\n",
      "Epoch: 16/99 Iteration: 1297 Training loss: 0.42464\n",
      "Epoch: 16/99 Iteration: 1298 Training loss: 0.59031\n",
      "Epoch: 16/99 Iteration: 1299 Training loss: 0.52783\n",
      "Epoch: 16/99 Iteration: 1300 Training loss: 0.54240\n",
      "***\n",
      "Epoch: 16/99 Iteration: 1300 Validation Acc: 0.7990\n",
      "***\n",
      "Epoch: 16/99 Iteration: 1301 Training loss: 0.48531\n",
      "Epoch: 16/99 Iteration: 1302 Training loss: 0.48221\n",
      "Epoch: 16/99 Iteration: 1303 Training loss: 0.57746\n",
      "Epoch: 16/99 Iteration: 1304 Training loss: 0.47550\n",
      "Epoch: 16/99 Iteration: 1305 Training loss: 0.55596\n",
      "Epoch: 16/99 Iteration: 1306 Training loss: 0.51853\n",
      "Epoch: 16/99 Iteration: 1307 Training loss: 0.42860\n",
      "Epoch: 16/99 Iteration: 1308 Training loss: 0.58889\n",
      "Epoch: 16/99 Iteration: 1309 Training loss: 0.69740\n",
      "Epoch: 16/99 Iteration: 1310 Training loss: 0.38362\n",
      "Epoch: 16/99 Iteration: 1311 Training loss: 0.52178\n",
      "Epoch: 16/99 Iteration: 1312 Training loss: 0.60752\n",
      "Epoch: 16/99 Iteration: 1313 Training loss: 0.47334\n",
      "Epoch: 16/99 Iteration: 1314 Training loss: 0.44932\n",
      "Epoch: 16/99 Iteration: 1315 Training loss: 0.58270\n",
      "Epoch: 16/99 Iteration: 1316 Training loss: 0.64582\n",
      "Epoch: 16/99 Iteration: 1317 Training loss: 0.55826\n",
      "Epoch: 16/99 Iteration: 1318 Training loss: 0.48846\n",
      "Epoch: 16/99 Iteration: 1319 Training loss: 0.38358\n",
      "Epoch: 16/99 Iteration: 1320 Training loss: 0.47108\n",
      "Epoch: 16/99 Iteration: 1321 Training loss: 0.51504\n",
      "Epoch: 16/99 Iteration: 1322 Training loss: 0.55164\n",
      "Epoch: 16/99 Iteration: 1323 Training loss: 0.51022\n",
      "Epoch: 16/99 Iteration: 1324 Training loss: 0.51237\n",
      "Epoch: 16/99 Iteration: 1325 Training loss: 0.52622\n",
      "Epoch: 16/99 Iteration: 1326 Training loss: 0.53911\n",
      "Epoch: 16/99 Iteration: 1327 Training loss: 0.54701\n",
      "Epoch: 16/99 Iteration: 1328 Training loss: 0.81793\n",
      "Epoch: 16/99 Iteration: 1329 Training loss: 0.45399\n",
      "Epoch: 16/99 Iteration: 1330 Training loss: 0.56098\n",
      "Epoch: 16/99 Iteration: 1331 Training loss: 0.53575\n",
      "Epoch: 16/99 Iteration: 1332 Training loss: 0.52910\n",
      "Epoch: 16/99 Iteration: 1333 Training loss: 0.58141\n",
      "Epoch: 16/99 Iteration: 1334 Training loss: 0.63058\n",
      "Epoch: 16/99 Iteration: 1335 Training loss: 0.43022\n",
      "Epoch: 16/99 Iteration: 1336 Training loss: 0.68102\n",
      "Epoch: 16/99 Iteration: 1337 Training loss: 0.64287\n",
      "Epoch: 16/99 Iteration: 1338 Training loss: 0.59867\n",
      "Epoch: 16/99 Iteration: 1339 Training loss: 0.66740\n",
      "Epoch: 16/99 Iteration: 1340 Training loss: 0.48735\n",
      "Epoch: 16/99 Iteration: 1341 Training loss: 0.58181\n",
      "Epoch: 16/99 Iteration: 1342 Training loss: 0.46141\n",
      "Epoch: 16/99 Iteration: 1343 Training loss: 0.51350\n",
      "Epoch: 16/99 Iteration: 1344 Training loss: 0.51937\n",
      "Epoch: 16/99 Iteration: 1345 Training loss: 0.55532\n",
      "Epoch: 16/99 Iteration: 1346 Training loss: 0.74603\n",
      "Epoch: 16/99 Iteration: 1347 Training loss: 0.71334\n",
      "Epoch: 16/99 Iteration: 1348 Training loss: 0.55925\n",
      "Epoch: 16/99 Iteration: 1349 Training loss: 0.57583\n",
      "Epoch: 16/99 Iteration: 1350 Training loss: 0.66690\n",
      "***\n",
      "Epoch: 16/99 Iteration: 1350 Validation Acc: 0.8050\n",
      "***\n",
      "Epoch: 16/99 Iteration: 1351 Training loss: 0.45614\n",
      "Epoch: 16/99 Iteration: 1352 Training loss: 0.58544\n",
      "Epoch: 16/99 Iteration: 1353 Training loss: 0.45927\n",
      "Epoch: 16/99 Iteration: 1354 Training loss: 0.51051\n",
      "Epoch: 16/99 Iteration: 1355 Training loss: 0.50261\n",
      "Epoch: 16/99 Iteration: 1356 Training loss: 0.60260\n",
      "Epoch: 16/99 Iteration: 1357 Training loss: 0.72847\n",
      "Epoch: 16/99 Iteration: 1358 Training loss: 0.49683\n",
      "Epoch: 16/99 Iteration: 1359 Training loss: 0.61658\n",
      "Epoch: 17/99 Iteration: 1360 Training loss: 0.51204\n",
      "Epoch: 17/99 Iteration: 1361 Training loss: 0.65599\n",
      "Epoch: 17/99 Iteration: 1362 Training loss: 0.66725\n",
      "Epoch: 17/99 Iteration: 1363 Training loss: 0.50288\n",
      "Epoch: 17/99 Iteration: 1364 Training loss: 0.42199\n",
      "Epoch: 17/99 Iteration: 1365 Training loss: 0.41868\n",
      "Epoch: 17/99 Iteration: 1366 Training loss: 0.56486\n",
      "Epoch: 17/99 Iteration: 1367 Training loss: 0.57996\n",
      "Epoch: 17/99 Iteration: 1368 Training loss: 0.72058\n",
      "Epoch: 17/99 Iteration: 1369 Training loss: 0.48516\n",
      "Epoch: 17/99 Iteration: 1370 Training loss: 0.42713\n",
      "Epoch: 17/99 Iteration: 1371 Training loss: 0.53347\n",
      "Epoch: 17/99 Iteration: 1372 Training loss: 0.48437\n",
      "Epoch: 17/99 Iteration: 1373 Training loss: 0.59134\n",
      "Epoch: 17/99 Iteration: 1374 Training loss: 0.58192\n",
      "Epoch: 17/99 Iteration: 1375 Training loss: 0.45429\n",
      "Epoch: 17/99 Iteration: 1376 Training loss: 0.49880\n",
      "Epoch: 17/99 Iteration: 1377 Training loss: 0.48048\n",
      "Epoch: 17/99 Iteration: 1378 Training loss: 0.50919\n",
      "Epoch: 17/99 Iteration: 1379 Training loss: 0.51161\n",
      "Epoch: 17/99 Iteration: 1380 Training loss: 0.45980\n",
      "Epoch: 17/99 Iteration: 1381 Training loss: 0.44563\n",
      "Epoch: 17/99 Iteration: 1382 Training loss: 0.50831\n",
      "Epoch: 17/99 Iteration: 1383 Training loss: 0.57302\n",
      "Epoch: 17/99 Iteration: 1384 Training loss: 0.48012\n",
      "Epoch: 17/99 Iteration: 1385 Training loss: 0.49181\n",
      "Epoch: 17/99 Iteration: 1386 Training loss: 0.54874\n",
      "Epoch: 17/99 Iteration: 1387 Training loss: 0.48460\n",
      "Epoch: 17/99 Iteration: 1388 Training loss: 0.61660\n",
      "Epoch: 17/99 Iteration: 1389 Training loss: 0.69892\n",
      "Epoch: 17/99 Iteration: 1390 Training loss: 0.49472\n",
      "Epoch: 17/99 Iteration: 1391 Training loss: 0.39635\n",
      "Epoch: 17/99 Iteration: 1392 Training loss: 0.58232\n",
      "Epoch: 17/99 Iteration: 1393 Training loss: 0.61394\n",
      "Epoch: 17/99 Iteration: 1394 Training loss: 0.63604\n",
      "Epoch: 17/99 Iteration: 1395 Training loss: 0.58986\n",
      "Epoch: 17/99 Iteration: 1396 Training loss: 0.61674\n",
      "Epoch: 17/99 Iteration: 1397 Training loss: 0.46219\n",
      "Epoch: 17/99 Iteration: 1398 Training loss: 0.56946\n",
      "Epoch: 17/99 Iteration: 1399 Training loss: 0.45946\n",
      "Epoch: 17/99 Iteration: 1400 Training loss: 0.48794\n",
      "***\n",
      "Epoch: 17/99 Iteration: 1400 Validation Acc: 0.8020\n",
      "***\n",
      "Epoch: 17/99 Iteration: 1401 Training loss: 0.70101\n",
      "Epoch: 17/99 Iteration: 1402 Training loss: 0.54227\n",
      "Epoch: 17/99 Iteration: 1403 Training loss: 0.39963\n",
      "Epoch: 17/99 Iteration: 1404 Training loss: 0.64936\n",
      "Epoch: 17/99 Iteration: 1405 Training loss: 0.44120\n",
      "Epoch: 17/99 Iteration: 1406 Training loss: 0.56757\n",
      "Epoch: 17/99 Iteration: 1407 Training loss: 0.44483\n",
      "Epoch: 17/99 Iteration: 1408 Training loss: 0.76414\n",
      "Epoch: 17/99 Iteration: 1409 Training loss: 0.44265\n",
      "Epoch: 17/99 Iteration: 1410 Training loss: 0.44849\n",
      "Epoch: 17/99 Iteration: 1411 Training loss: 0.53345\n",
      "Epoch: 17/99 Iteration: 1412 Training loss: 0.50858\n",
      "Epoch: 17/99 Iteration: 1413 Training loss: 0.61722\n",
      "Epoch: 17/99 Iteration: 1414 Training loss: 0.67604\n",
      "Epoch: 17/99 Iteration: 1415 Training loss: 0.50433\n",
      "Epoch: 17/99 Iteration: 1416 Training loss: 0.66606\n",
      "Epoch: 17/99 Iteration: 1417 Training loss: 0.57385\n",
      "Epoch: 17/99 Iteration: 1418 Training loss: 0.54309\n",
      "Epoch: 17/99 Iteration: 1419 Training loss: 0.78036\n",
      "Epoch: 17/99 Iteration: 1420 Training loss: 0.50449\n",
      "Epoch: 17/99 Iteration: 1421 Training loss: 0.65492\n",
      "Epoch: 17/99 Iteration: 1422 Training loss: 0.41822\n",
      "Epoch: 17/99 Iteration: 1423 Training loss: 0.59756\n",
      "Epoch: 17/99 Iteration: 1424 Training loss: 0.68151\n",
      "Epoch: 17/99 Iteration: 1425 Training loss: 0.64614\n",
      "Epoch: 17/99 Iteration: 1426 Training loss: 0.67031\n",
      "Epoch: 17/99 Iteration: 1427 Training loss: 0.68520\n",
      "Epoch: 17/99 Iteration: 1428 Training loss: 0.60248\n",
      "Epoch: 17/99 Iteration: 1429 Training loss: 0.52482\n",
      "Epoch: 17/99 Iteration: 1430 Training loss: 0.71039\n",
      "Epoch: 17/99 Iteration: 1431 Training loss: 0.53642\n",
      "Epoch: 17/99 Iteration: 1432 Training loss: 0.56672\n",
      "Epoch: 17/99 Iteration: 1433 Training loss: 0.51992\n",
      "Epoch: 17/99 Iteration: 1434 Training loss: 0.50803\n",
      "Epoch: 17/99 Iteration: 1435 Training loss: 0.52718\n",
      "Epoch: 17/99 Iteration: 1436 Training loss: 0.54932\n",
      "Epoch: 17/99 Iteration: 1437 Training loss: 0.78871\n",
      "Epoch: 17/99 Iteration: 1438 Training loss: 0.63979\n",
      "Epoch: 17/99 Iteration: 1439 Training loss: 0.50829\n",
      "Epoch: 18/99 Iteration: 1440 Training loss: 0.44805\n",
      "Epoch: 18/99 Iteration: 1441 Training loss: 0.69316\n",
      "Epoch: 18/99 Iteration: 1442 Training loss: 0.56393\n",
      "Epoch: 18/99 Iteration: 1443 Training loss: 0.59580\n",
      "Epoch: 18/99 Iteration: 1444 Training loss: 0.47155\n",
      "Epoch: 18/99 Iteration: 1445 Training loss: 0.59594\n",
      "Epoch: 18/99 Iteration: 1446 Training loss: 0.58241\n",
      "Epoch: 18/99 Iteration: 1447 Training loss: 0.54586\n",
      "Epoch: 18/99 Iteration: 1448 Training loss: 0.74724\n",
      "Epoch: 18/99 Iteration: 1449 Training loss: 0.53851\n",
      "Epoch: 18/99 Iteration: 1450 Training loss: 0.42564\n",
      "***\n",
      "Epoch: 18/99 Iteration: 1450 Validation Acc: 0.8220\n",
      "***\n",
      "Epoch: 18/99 Iteration: 1451 Training loss: 0.40427\n",
      "Epoch: 18/99 Iteration: 1452 Training loss: 0.50175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/99 Iteration: 1453 Training loss: 0.54596\n",
      "Epoch: 18/99 Iteration: 1454 Training loss: 0.75706\n",
      "Epoch: 18/99 Iteration: 1455 Training loss: 0.49159\n",
      "Epoch: 18/99 Iteration: 1456 Training loss: 0.41068\n",
      "Epoch: 18/99 Iteration: 1457 Training loss: 0.41195\n",
      "Epoch: 18/99 Iteration: 1458 Training loss: 0.52984\n",
      "Epoch: 18/99 Iteration: 1459 Training loss: 0.53392\n",
      "Epoch: 18/99 Iteration: 1460 Training loss: 0.50538\n",
      "Epoch: 18/99 Iteration: 1461 Training loss: 0.38656\n",
      "Epoch: 18/99 Iteration: 1462 Training loss: 0.44862\n",
      "Epoch: 18/99 Iteration: 1463 Training loss: 0.46348\n",
      "Epoch: 18/99 Iteration: 1464 Training loss: 0.48375\n",
      "Epoch: 18/99 Iteration: 1465 Training loss: 0.48071\n",
      "Epoch: 18/99 Iteration: 1466 Training loss: 0.55029\n",
      "Epoch: 18/99 Iteration: 1467 Training loss: 0.45413\n",
      "Epoch: 18/99 Iteration: 1468 Training loss: 0.52037\n",
      "Epoch: 18/99 Iteration: 1469 Training loss: 0.64453\n",
      "Epoch: 18/99 Iteration: 1470 Training loss: 0.39448\n",
      "Epoch: 18/99 Iteration: 1471 Training loss: 0.39844\n",
      "Epoch: 18/99 Iteration: 1472 Training loss: 0.66884\n",
      "Epoch: 18/99 Iteration: 1473 Training loss: 0.51019\n",
      "Epoch: 18/99 Iteration: 1474 Training loss: 0.45553\n",
      "Epoch: 18/99 Iteration: 1475 Training loss: 0.59900\n",
      "Epoch: 18/99 Iteration: 1476 Training loss: 0.68008\n",
      "Epoch: 18/99 Iteration: 1477 Training loss: 0.46822\n",
      "Epoch: 18/99 Iteration: 1478 Training loss: 0.57734\n",
      "Epoch: 18/99 Iteration: 1479 Training loss: 0.41032\n",
      "Epoch: 18/99 Iteration: 1480 Training loss: 0.51973\n",
      "Epoch: 18/99 Iteration: 1481 Training loss: 0.53519\n",
      "Epoch: 18/99 Iteration: 1482 Training loss: 0.65517\n",
      "Epoch: 18/99 Iteration: 1483 Training loss: 0.40078\n",
      "Epoch: 18/99 Iteration: 1484 Training loss: 0.61058\n",
      "Epoch: 18/99 Iteration: 1485 Training loss: 0.46242\n",
      "Epoch: 18/99 Iteration: 1486 Training loss: 0.62196\n",
      "Epoch: 18/99 Iteration: 1487 Training loss: 0.44953\n",
      "Epoch: 18/99 Iteration: 1488 Training loss: 0.71811\n",
      "Epoch: 18/99 Iteration: 1489 Training loss: 0.37794\n",
      "Epoch: 18/99 Iteration: 1490 Training loss: 0.45724\n",
      "Epoch: 18/99 Iteration: 1491 Training loss: 0.41716\n",
      "Epoch: 18/99 Iteration: 1492 Training loss: 0.43425\n",
      "Epoch: 18/99 Iteration: 1493 Training loss: 0.70693\n",
      "Epoch: 18/99 Iteration: 1494 Training loss: 0.61276\n",
      "Epoch: 18/99 Iteration: 1495 Training loss: 0.57313\n",
      "Epoch: 18/99 Iteration: 1496 Training loss: 0.53586\n",
      "Epoch: 18/99 Iteration: 1497 Training loss: 0.58692\n",
      "Epoch: 18/99 Iteration: 1498 Training loss: 0.45165\n",
      "Epoch: 18/99 Iteration: 1499 Training loss: 0.60241\n",
      "Epoch: 18/99 Iteration: 1500 Training loss: 0.50231\n",
      "***\n",
      "Epoch: 18/99 Iteration: 1500 Validation Acc: 0.8100\n",
      "***\n",
      "Epoch: 18/99 Iteration: 1501 Training loss: 0.62111\n",
      "Epoch: 18/99 Iteration: 1502 Training loss: 0.52818\n",
      "Epoch: 18/99 Iteration: 1503 Training loss: 0.54816\n",
      "Epoch: 18/99 Iteration: 1504 Training loss: 0.47641\n",
      "Epoch: 18/99 Iteration: 1505 Training loss: 0.57010\n",
      "Epoch: 18/99 Iteration: 1506 Training loss: 0.64701\n",
      "Epoch: 18/99 Iteration: 1507 Training loss: 0.62366\n",
      "Epoch: 18/99 Iteration: 1508 Training loss: 0.49824\n",
      "Epoch: 18/99 Iteration: 1509 Training loss: 0.51586\n",
      "Epoch: 18/99 Iteration: 1510 Training loss: 0.71597\n",
      "Epoch: 18/99 Iteration: 1511 Training loss: 0.54248\n",
      "Epoch: 18/99 Iteration: 1512 Training loss: 0.53847\n",
      "Epoch: 18/99 Iteration: 1513 Training loss: 0.41832\n",
      "Epoch: 18/99 Iteration: 1514 Training loss: 0.50999\n",
      "Epoch: 18/99 Iteration: 1515 Training loss: 0.43446\n",
      "Epoch: 18/99 Iteration: 1516 Training loss: 0.55680\n",
      "Epoch: 18/99 Iteration: 1517 Training loss: 0.70502\n",
      "Epoch: 18/99 Iteration: 1518 Training loss: 0.59934\n",
      "Epoch: 18/99 Iteration: 1519 Training loss: 0.48548\n",
      "Epoch: 19/99 Iteration: 1520 Training loss: 0.43191\n",
      "Epoch: 19/99 Iteration: 1521 Training loss: 0.64430\n",
      "Epoch: 19/99 Iteration: 1522 Training loss: 0.62471\n",
      "Epoch: 19/99 Iteration: 1523 Training loss: 0.43144\n",
      "Epoch: 19/99 Iteration: 1524 Training loss: 0.48669\n",
      "Epoch: 19/99 Iteration: 1525 Training loss: 0.49550\n",
      "Epoch: 19/99 Iteration: 1526 Training loss: 0.54357\n",
      "Epoch: 19/99 Iteration: 1527 Training loss: 0.50276\n",
      "Epoch: 19/99 Iteration: 1528 Training loss: 0.54739\n",
      "Epoch: 19/99 Iteration: 1529 Training loss: 0.36439\n",
      "Epoch: 19/99 Iteration: 1530 Training loss: 0.53649\n",
      "Epoch: 19/99 Iteration: 1531 Training loss: 0.55031\n",
      "Epoch: 19/99 Iteration: 1532 Training loss: 0.54584\n",
      "Epoch: 19/99 Iteration: 1533 Training loss: 0.55856\n",
      "Epoch: 19/99 Iteration: 1534 Training loss: 0.74800\n",
      "Epoch: 19/99 Iteration: 1535 Training loss: 0.46926\n",
      "Epoch: 19/99 Iteration: 1536 Training loss: 0.36119\n",
      "Epoch: 19/99 Iteration: 1537 Training loss: 0.42330\n",
      "Epoch: 19/99 Iteration: 1538 Training loss: 0.55684\n",
      "Epoch: 19/99 Iteration: 1539 Training loss: 0.42177\n",
      "Epoch: 19/99 Iteration: 1540 Training loss: 0.56750\n",
      "Epoch: 19/99 Iteration: 1541 Training loss: 0.48709\n",
      "Epoch: 19/99 Iteration: 1542 Training loss: 0.66506\n",
      "Epoch: 19/99 Iteration: 1543 Training loss: 0.64608\n",
      "Epoch: 19/99 Iteration: 1544 Training loss: 0.43397\n",
      "Epoch: 19/99 Iteration: 1545 Training loss: 0.44315\n",
      "Epoch: 19/99 Iteration: 1546 Training loss: 0.60900\n",
      "Epoch: 19/99 Iteration: 1547 Training loss: 0.36765\n",
      "Epoch: 19/99 Iteration: 1548 Training loss: 0.58194\n",
      "Epoch: 19/99 Iteration: 1549 Training loss: 0.63909\n",
      "Epoch: 19/99 Iteration: 1550 Training loss: 0.40567\n",
      "***\n",
      "Epoch: 19/99 Iteration: 1550 Validation Acc: 0.8200\n",
      "***\n",
      "Epoch: 19/99 Iteration: 1551 Training loss: 0.36313\n",
      "Epoch: 19/99 Iteration: 1552 Training loss: 0.53417\n",
      "Epoch: 19/99 Iteration: 1553 Training loss: 0.48342\n",
      "Epoch: 19/99 Iteration: 1554 Training loss: 0.51085\n",
      "Epoch: 19/99 Iteration: 1555 Training loss: 0.46659\n",
      "Epoch: 19/99 Iteration: 1556 Training loss: 0.71510\n",
      "Epoch: 19/99 Iteration: 1557 Training loss: 0.37947\n",
      "Epoch: 19/99 Iteration: 1558 Training loss: 0.46360\n",
      "Epoch: 19/99 Iteration: 1559 Training loss: 0.52501\n",
      "Epoch: 19/99 Iteration: 1560 Training loss: 0.45504\n",
      "Epoch: 19/99 Iteration: 1561 Training loss: 0.44005\n",
      "Epoch: 19/99 Iteration: 1562 Training loss: 0.59555\n",
      "Epoch: 19/99 Iteration: 1563 Training loss: 0.43429\n",
      "Epoch: 19/99 Iteration: 1564 Training loss: 0.56301\n",
      "Epoch: 19/99 Iteration: 1565 Training loss: 0.42833\n",
      "Epoch: 19/99 Iteration: 1566 Training loss: 0.58587\n",
      "Epoch: 19/99 Iteration: 1567 Training loss: 0.42944\n",
      "Epoch: 19/99 Iteration: 1568 Training loss: 0.68451\n",
      "Epoch: 19/99 Iteration: 1569 Training loss: 0.52796\n",
      "Epoch: 19/99 Iteration: 1570 Training loss: 0.51992\n",
      "Epoch: 19/99 Iteration: 1571 Training loss: 0.40855\n",
      "Epoch: 19/99 Iteration: 1572 Training loss: 0.53005\n",
      "Epoch: 19/99 Iteration: 1573 Training loss: 0.76291\n",
      "Epoch: 19/99 Iteration: 1574 Training loss: 0.63408\n",
      "Epoch: 19/99 Iteration: 1575 Training loss: 0.52919\n",
      "Epoch: 19/99 Iteration: 1576 Training loss: 0.62255\n",
      "Epoch: 19/99 Iteration: 1577 Training loss: 0.65175\n",
      "Epoch: 19/99 Iteration: 1578 Training loss: 0.62434\n",
      "Epoch: 19/99 Iteration: 1579 Training loss: 0.60682\n",
      "Epoch: 19/99 Iteration: 1580 Training loss: 0.47635\n",
      "Epoch: 19/99 Iteration: 1581 Training loss: 0.67491\n",
      "Epoch: 19/99 Iteration: 1582 Training loss: 0.55993\n",
      "Epoch: 19/99 Iteration: 1583 Training loss: 0.52207\n",
      "Epoch: 19/99 Iteration: 1584 Training loss: 0.54034\n",
      "Epoch: 19/99 Iteration: 1585 Training loss: 0.67549\n",
      "Epoch: 19/99 Iteration: 1586 Training loss: 0.59705\n",
      "Epoch: 19/99 Iteration: 1587 Training loss: 0.79800\n",
      "Epoch: 19/99 Iteration: 1588 Training loss: 0.62943\n",
      "Epoch: 19/99 Iteration: 1589 Training loss: 0.51064\n",
      "Epoch: 19/99 Iteration: 1590 Training loss: 0.66139\n",
      "Epoch: 19/99 Iteration: 1591 Training loss: 0.56906\n",
      "Epoch: 19/99 Iteration: 1592 Training loss: 0.66212\n",
      "Epoch: 19/99 Iteration: 1593 Training loss: 0.45113\n",
      "Epoch: 19/99 Iteration: 1594 Training loss: 0.43845\n",
      "Epoch: 19/99 Iteration: 1595 Training loss: 0.44554\n",
      "Epoch: 19/99 Iteration: 1596 Training loss: 0.60377\n",
      "Epoch: 19/99 Iteration: 1597 Training loss: 0.76555\n",
      "Epoch: 19/99 Iteration: 1598 Training loss: 0.57450\n",
      "Epoch: 19/99 Iteration: 1599 Training loss: 0.53315\n",
      "Epoch: 20/99 Iteration: 1600 Training loss: 0.53351\n",
      "***\n",
      "Epoch: 20/99 Iteration: 1600 Validation Acc: 0.7980\n",
      "***\n",
      "Epoch: 20/99 Iteration: 1601 Training loss: 0.74688\n",
      "Epoch: 20/99 Iteration: 1602 Training loss: 0.58530\n",
      "Epoch: 20/99 Iteration: 1603 Training loss: 0.54873\n",
      "Epoch: 20/99 Iteration: 1604 Training loss: 0.53053\n",
      "Epoch: 20/99 Iteration: 1605 Training loss: 0.37511\n",
      "Epoch: 20/99 Iteration: 1606 Training loss: 0.46985\n",
      "Epoch: 20/99 Iteration: 1607 Training loss: 0.50012\n",
      "Epoch: 20/99 Iteration: 1608 Training loss: 0.72366\n",
      "Epoch: 20/99 Iteration: 1609 Training loss: 0.64113\n",
      "Epoch: 20/99 Iteration: 1610 Training loss: 0.50770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/99 Iteration: 1611 Training loss: 0.47864\n",
      "Epoch: 20/99 Iteration: 1612 Training loss: 0.48170\n",
      "Epoch: 20/99 Iteration: 1613 Training loss: 0.58366\n",
      "Epoch: 20/99 Iteration: 1614 Training loss: 0.54639\n",
      "Epoch: 20/99 Iteration: 1615 Training loss: 0.47710\n",
      "Epoch: 20/99 Iteration: 1616 Training loss: 0.51743\n",
      "Epoch: 20/99 Iteration: 1617 Training loss: 0.46420\n",
      "Epoch: 20/99 Iteration: 1618 Training loss: 0.54484\n",
      "Epoch: 20/99 Iteration: 1619 Training loss: 0.42814\n",
      "Epoch: 20/99 Iteration: 1620 Training loss: 0.51199\n",
      "Epoch: 20/99 Iteration: 1621 Training loss: 0.46205\n",
      "Epoch: 20/99 Iteration: 1622 Training loss: 0.59882\n",
      "Epoch: 20/99 Iteration: 1623 Training loss: 0.56463\n",
      "Epoch: 20/99 Iteration: 1624 Training loss: 0.43899\n",
      "Epoch: 20/99 Iteration: 1625 Training loss: 0.56951\n",
      "Epoch: 20/99 Iteration: 1626 Training loss: 0.55147\n",
      "Epoch: 20/99 Iteration: 1627 Training loss: 0.41653\n",
      "Epoch: 20/99 Iteration: 1628 Training loss: 0.57943\n",
      "Epoch: 20/99 Iteration: 1629 Training loss: 0.65693\n",
      "Epoch: 20/99 Iteration: 1630 Training loss: 0.47174\n",
      "Epoch: 20/99 Iteration: 1631 Training loss: 0.48632\n",
      "Epoch: 20/99 Iteration: 1632 Training loss: 0.53803\n",
      "Epoch: 20/99 Iteration: 1633 Training loss: 0.57848\n",
      "Epoch: 20/99 Iteration: 1634 Training loss: 0.50941\n",
      "Epoch: 20/99 Iteration: 1635 Training loss: 0.59852\n",
      "Epoch: 20/99 Iteration: 1636 Training loss: 0.67059\n",
      "Epoch: 20/99 Iteration: 1637 Training loss: 0.62778\n",
      "Epoch: 20/99 Iteration: 1638 Training loss: 0.47990\n",
      "Epoch: 20/99 Iteration: 1639 Training loss: 0.49767\n",
      "Epoch: 20/99 Iteration: 1640 Training loss: 0.50710\n",
      "Epoch: 20/99 Iteration: 1641 Training loss: 0.61328\n",
      "Epoch: 20/99 Iteration: 1642 Training loss: 0.55970\n",
      "Epoch: 20/99 Iteration: 1643 Training loss: 0.51727\n",
      "Epoch: 20/99 Iteration: 1644 Training loss: 0.63148\n",
      "Epoch: 20/99 Iteration: 1645 Training loss: 0.43431\n",
      "Epoch: 20/99 Iteration: 1646 Training loss: 0.57314\n",
      "Epoch: 20/99 Iteration: 1647 Training loss: 0.57542\n",
      "Epoch: 20/99 Iteration: 1648 Training loss: 0.68866\n",
      "Epoch: 20/99 Iteration: 1649 Training loss: 0.47407\n",
      "Epoch: 20/99 Iteration: 1650 Training loss: 0.46495\n",
      "***\n",
      "Epoch: 20/99 Iteration: 1650 Validation Acc: 0.8250\n",
      "***\n",
      "Epoch: 20/99 Iteration: 1651 Training loss: 0.46312\n",
      "Epoch: 20/99 Iteration: 1652 Training loss: 0.43646\n",
      "Epoch: 20/99 Iteration: 1653 Training loss: 0.62044\n",
      "Epoch: 20/99 Iteration: 1654 Training loss: 0.52469\n",
      "Epoch: 20/99 Iteration: 1655 Training loss: 0.50503\n",
      "Epoch: 20/99 Iteration: 1656 Training loss: 0.50801\n",
      "Epoch: 20/99 Iteration: 1657 Training loss: 0.50897\n",
      "Epoch: 20/99 Iteration: 1658 Training loss: 0.53765\n",
      "Epoch: 20/99 Iteration: 1659 Training loss: 0.67639\n",
      "Epoch: 20/99 Iteration: 1660 Training loss: 0.39735\n",
      "Epoch: 20/99 Iteration: 1661 Training loss: 0.55600\n",
      "Epoch: 20/99 Iteration: 1662 Training loss: 0.47642\n",
      "Epoch: 20/99 Iteration: 1663 Training loss: 0.58549\n",
      "Epoch: 20/99 Iteration: 1664 Training loss: 0.57533\n",
      "Epoch: 20/99 Iteration: 1665 Training loss: 0.58208\n",
      "Epoch: 20/99 Iteration: 1666 Training loss: 0.70041\n",
      "Epoch: 20/99 Iteration: 1667 Training loss: 0.70259\n",
      "Epoch: 20/99 Iteration: 1668 Training loss: 0.46410\n",
      "Epoch: 20/99 Iteration: 1669 Training loss: 0.48866\n",
      "Epoch: 20/99 Iteration: 1670 Training loss: 0.59061\n",
      "Epoch: 20/99 Iteration: 1671 Training loss: 0.49012\n",
      "Epoch: 20/99 Iteration: 1672 Training loss: 0.63293\n",
      "Epoch: 20/99 Iteration: 1673 Training loss: 0.57062\n",
      "Epoch: 20/99 Iteration: 1674 Training loss: 0.42316\n",
      "Epoch: 20/99 Iteration: 1675 Training loss: 0.42311\n",
      "Epoch: 20/99 Iteration: 1676 Training loss: 0.51050\n",
      "Epoch: 20/99 Iteration: 1677 Training loss: 0.69227\n",
      "Epoch: 20/99 Iteration: 1678 Training loss: 0.54722\n",
      "Epoch: 20/99 Iteration: 1679 Training loss: 0.54241\n",
      "Epoch: 21/99 Iteration: 1680 Training loss: 0.35672\n",
      "Epoch: 21/99 Iteration: 1681 Training loss: 0.57701\n",
      "Epoch: 21/99 Iteration: 1682 Training loss: 0.51991\n",
      "Epoch: 21/99 Iteration: 1683 Training loss: 0.52646\n",
      "Epoch: 21/99 Iteration: 1684 Training loss: 0.46131\n",
      "Epoch: 21/99 Iteration: 1685 Training loss: 0.51445\n",
      "Epoch: 21/99 Iteration: 1686 Training loss: 0.51852\n",
      "Epoch: 21/99 Iteration: 1687 Training loss: 0.53945\n",
      "Epoch: 21/99 Iteration: 1688 Training loss: 0.72599\n",
      "Epoch: 21/99 Iteration: 1689 Training loss: 0.49742\n",
      "Epoch: 21/99 Iteration: 1690 Training loss: 0.56403\n",
      "Epoch: 21/99 Iteration: 1691 Training loss: 0.48977\n",
      "Epoch: 21/99 Iteration: 1692 Training loss: 0.58513\n",
      "Epoch: 21/99 Iteration: 1693 Training loss: 0.56650\n",
      "Epoch: 21/99 Iteration: 1694 Training loss: 0.68752\n",
      "Epoch: 21/99 Iteration: 1695 Training loss: 0.49735\n",
      "Epoch: 21/99 Iteration: 1696 Training loss: 0.48386\n",
      "Epoch: 21/99 Iteration: 1697 Training loss: 0.51319\n",
      "Epoch: 21/99 Iteration: 1698 Training loss: 0.54753\n",
      "Epoch: 21/99 Iteration: 1699 Training loss: 0.41735\n",
      "Epoch: 21/99 Iteration: 1700 Training loss: 0.49707\n",
      "***\n",
      "Epoch: 21/99 Iteration: 1700 Validation Acc: 0.7990\n",
      "***\n",
      "Epoch: 21/99 Iteration: 1701 Training loss: 0.49926\n",
      "Epoch: 21/99 Iteration: 1702 Training loss: 0.59178\n",
      "Epoch: 21/99 Iteration: 1703 Training loss: 0.52807\n",
      "Epoch: 21/99 Iteration: 1704 Training loss: 0.53036\n",
      "Epoch: 21/99 Iteration: 1705 Training loss: 0.52649\n",
      "Epoch: 21/99 Iteration: 1706 Training loss: 0.65358\n",
      "Epoch: 21/99 Iteration: 1707 Training loss: 0.53311\n",
      "Epoch: 21/99 Iteration: 1708 Training loss: 0.62025\n",
      "Epoch: 21/99 Iteration: 1709 Training loss: 0.71037\n",
      "Epoch: 21/99 Iteration: 1710 Training loss: 0.44668\n",
      "Epoch: 21/99 Iteration: 1711 Training loss: 0.43751\n",
      "Epoch: 21/99 Iteration: 1712 Training loss: 0.50810\n",
      "Epoch: 21/99 Iteration: 1713 Training loss: 0.49150\n",
      "Epoch: 21/99 Iteration: 1714 Training loss: 0.47154\n",
      "Epoch: 21/99 Iteration: 1715 Training loss: 0.52558\n",
      "Epoch: 21/99 Iteration: 1716 Training loss: 0.57255\n",
      "Epoch: 21/99 Iteration: 1717 Training loss: 0.41102\n",
      "Epoch: 21/99 Iteration: 1718 Training loss: 0.56337\n",
      "Epoch: 21/99 Iteration: 1719 Training loss: 0.57597\n",
      "Epoch: 21/99 Iteration: 1720 Training loss: 0.55289\n",
      "Epoch: 21/99 Iteration: 1721 Training loss: 0.60345\n",
      "Epoch: 21/99 Iteration: 1722 Training loss: 0.68725\n",
      "Epoch: 21/99 Iteration: 1723 Training loss: 0.41309\n",
      "Epoch: 21/99 Iteration: 1724 Training loss: 0.66239\n",
      "Epoch: 21/99 Iteration: 1725 Training loss: 0.54814\n",
      "Epoch: 21/99 Iteration: 1726 Training loss: 0.61377\n",
      "Epoch: 21/99 Iteration: 1727 Training loss: 0.43594\n",
      "Epoch: 21/99 Iteration: 1728 Training loss: 0.78681\n",
      "Epoch: 21/99 Iteration: 1729 Training loss: 0.49223\n",
      "Epoch: 21/99 Iteration: 1730 Training loss: 0.56216\n",
      "Epoch: 21/99 Iteration: 1731 Training loss: 0.51965\n",
      "Epoch: 21/99 Iteration: 1732 Training loss: 0.57726\n",
      "Epoch: 21/99 Iteration: 1733 Training loss: 0.48117\n",
      "Epoch: 21/99 Iteration: 1734 Training loss: 0.72592\n",
      "Epoch: 21/99 Iteration: 1735 Training loss: 0.35293\n",
      "Epoch: 21/99 Iteration: 1736 Training loss: 0.56924\n",
      "Epoch: 21/99 Iteration: 1737 Training loss: 0.68140\n",
      "Epoch: 21/99 Iteration: 1738 Training loss: 0.53226\n",
      "Epoch: 21/99 Iteration: 1739 Training loss: 0.54191\n",
      "Epoch: 21/99 Iteration: 1740 Training loss: 0.53463\n",
      "Epoch: 21/99 Iteration: 1741 Training loss: 0.63148\n",
      "Epoch: 21/99 Iteration: 1742 Training loss: 0.67762\n",
      "Epoch: 21/99 Iteration: 1743 Training loss: 0.63816\n",
      "Epoch: 21/99 Iteration: 1744 Training loss: 0.53128\n",
      "Epoch: 21/99 Iteration: 1745 Training loss: 0.65676\n",
      "Epoch: 21/99 Iteration: 1746 Training loss: 0.71107\n",
      "Epoch: 21/99 Iteration: 1747 Training loss: 0.53222\n",
      "Epoch: 21/99 Iteration: 1748 Training loss: 0.58301\n",
      "Epoch: 21/99 Iteration: 1749 Training loss: 0.51913\n",
      "Epoch: 21/99 Iteration: 1750 Training loss: 0.72453\n",
      "***\n",
      "Epoch: 21/99 Iteration: 1750 Validation Acc: 0.8210\n",
      "***\n",
      "Epoch: 21/99 Iteration: 1751 Training loss: 0.42620\n",
      "Epoch: 21/99 Iteration: 1752 Training loss: 0.64698\n",
      "Epoch: 21/99 Iteration: 1753 Training loss: 0.45218\n",
      "Epoch: 21/99 Iteration: 1754 Training loss: 0.54016\n",
      "Epoch: 21/99 Iteration: 1755 Training loss: 0.51650\n",
      "Epoch: 21/99 Iteration: 1756 Training loss: 0.53194\n",
      "Epoch: 21/99 Iteration: 1757 Training loss: 0.61657\n",
      "Epoch: 21/99 Iteration: 1758 Training loss: 0.51445\n",
      "Epoch: 21/99 Iteration: 1759 Training loss: 0.49503\n",
      "Epoch: 22/99 Iteration: 1760 Training loss: 0.41103\n",
      "Epoch: 22/99 Iteration: 1761 Training loss: 0.69167\n",
      "Epoch: 22/99 Iteration: 1762 Training loss: 0.52680\n",
      "Epoch: 22/99 Iteration: 1763 Training loss: 0.56494\n",
      "Epoch: 22/99 Iteration: 1764 Training loss: 0.63938\n",
      "Epoch: 22/99 Iteration: 1765 Training loss: 0.49304\n",
      "Epoch: 22/99 Iteration: 1766 Training loss: 0.50154\n",
      "Epoch: 22/99 Iteration: 1767 Training loss: 0.48740\n",
      "Epoch: 22/99 Iteration: 1768 Training loss: 0.58727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/99 Iteration: 1769 Training loss: 0.40881\n",
      "Epoch: 22/99 Iteration: 1770 Training loss: 0.49870\n",
      "Epoch: 22/99 Iteration: 1771 Training loss: 0.42655\n",
      "Epoch: 22/99 Iteration: 1772 Training loss: 0.50266\n",
      "Epoch: 22/99 Iteration: 1773 Training loss: 0.53253\n",
      "Epoch: 22/99 Iteration: 1774 Training loss: 0.54539\n",
      "Epoch: 22/99 Iteration: 1775 Training loss: 0.47298\n",
      "Epoch: 22/99 Iteration: 1776 Training loss: 0.43183\n",
      "Epoch: 22/99 Iteration: 1777 Training loss: 0.39587\n",
      "Epoch: 22/99 Iteration: 1778 Training loss: 0.47451\n",
      "Epoch: 22/99 Iteration: 1779 Training loss: 0.43864\n",
      "Epoch: 22/99 Iteration: 1780 Training loss: 0.53373\n",
      "Epoch: 22/99 Iteration: 1781 Training loss: 0.48158\n",
      "Epoch: 22/99 Iteration: 1782 Training loss: 0.48717\n",
      "Epoch: 22/99 Iteration: 1783 Training loss: 0.52623\n",
      "Epoch: 22/99 Iteration: 1784 Training loss: 0.42795\n",
      "Epoch: 22/99 Iteration: 1785 Training loss: 0.48298\n",
      "Epoch: 22/99 Iteration: 1786 Training loss: 0.40752\n",
      "Epoch: 22/99 Iteration: 1787 Training loss: 0.33925\n",
      "Epoch: 22/99 Iteration: 1788 Training loss: 0.69455\n",
      "Epoch: 22/99 Iteration: 1789 Training loss: 0.77961\n",
      "Epoch: 22/99 Iteration: 1790 Training loss: 0.43649\n",
      "Epoch: 22/99 Iteration: 1791 Training loss: 0.40281\n",
      "Epoch: 22/99 Iteration: 1792 Training loss: 0.41527\n",
      "Epoch: 22/99 Iteration: 1793 Training loss: 0.48438\n",
      "Epoch: 22/99 Iteration: 1794 Training loss: 0.52028\n",
      "Epoch: 22/99 Iteration: 1795 Training loss: 0.44284\n",
      "Epoch: 22/99 Iteration: 1796 Training loss: 0.58625\n",
      "Epoch: 22/99 Iteration: 1797 Training loss: 0.40545\n",
      "Epoch: 22/99 Iteration: 1798 Training loss: 0.47418\n",
      "Epoch: 22/99 Iteration: 1799 Training loss: 0.46651\n",
      "Epoch: 22/99 Iteration: 1800 Training loss: 0.66912\n",
      "***\n",
      "Epoch: 22/99 Iteration: 1800 Validation Acc: 0.8060\n",
      "***\n",
      "Epoch: 22/99 Iteration: 1801 Training loss: 0.57962\n",
      "Epoch: 22/99 Iteration: 1802 Training loss: 0.48316\n",
      "Epoch: 22/99 Iteration: 1803 Training loss: 0.36648\n",
      "Epoch: 22/99 Iteration: 1804 Training loss: 0.78679\n",
      "Epoch: 22/99 Iteration: 1805 Training loss: 0.58239\n",
      "Epoch: 22/99 Iteration: 1806 Training loss: 0.55442\n",
      "Epoch: 22/99 Iteration: 1807 Training loss: 0.54346\n",
      "Epoch: 22/99 Iteration: 1808 Training loss: 0.76076\n",
      "Epoch: 22/99 Iteration: 1809 Training loss: 0.39525\n",
      "Epoch: 22/99 Iteration: 1810 Training loss: 0.52445\n",
      "Epoch: 22/99 Iteration: 1811 Training loss: 0.49193\n",
      "Epoch: 22/99 Iteration: 1812 Training loss: 0.55378\n",
      "Epoch: 22/99 Iteration: 1813 Training loss: 0.63239\n",
      "Epoch: 22/99 Iteration: 1814 Training loss: 0.65997\n",
      "Epoch: 22/99 Iteration: 1815 Training loss: 0.51754\n",
      "Epoch: 22/99 Iteration: 1816 Training loss: 0.72642\n",
      "Epoch: 22/99 Iteration: 1817 Training loss: 0.56485\n",
      "Epoch: 22/99 Iteration: 1818 Training loss: 0.56517\n",
      "Epoch: 22/99 Iteration: 1819 Training loss: 0.56207\n",
      "Epoch: 22/99 Iteration: 1820 Training loss: 0.46434\n",
      "Epoch: 22/99 Iteration: 1821 Training loss: 0.50611\n",
      "Epoch: 22/99 Iteration: 1822 Training loss: 0.42420\n",
      "Epoch: 22/99 Iteration: 1823 Training loss: 0.48161\n",
      "Epoch: 22/99 Iteration: 1824 Training loss: 0.58288\n",
      "Epoch: 22/99 Iteration: 1825 Training loss: 0.77344\n",
      "Epoch: 22/99 Iteration: 1826 Training loss: 0.67296\n",
      "Epoch: 22/99 Iteration: 1827 Training loss: 0.57238\n",
      "Epoch: 22/99 Iteration: 1828 Training loss: 0.50510\n",
      "Epoch: 22/99 Iteration: 1829 Training loss: 0.50075\n",
      "Epoch: 22/99 Iteration: 1830 Training loss: 0.66040\n",
      "Epoch: 22/99 Iteration: 1831 Training loss: 0.43374\n",
      "Epoch: 22/99 Iteration: 1832 Training loss: 0.48605\n",
      "Epoch: 22/99 Iteration: 1833 Training loss: 0.60557\n",
      "Epoch: 22/99 Iteration: 1834 Training loss: 0.46118\n",
      "Epoch: 22/99 Iteration: 1835 Training loss: 0.49237\n",
      "Epoch: 22/99 Iteration: 1836 Training loss: 0.56960\n",
      "Epoch: 22/99 Iteration: 1837 Training loss: 0.56716\n",
      "Epoch: 22/99 Iteration: 1838 Training loss: 0.53877\n",
      "Epoch: 22/99 Iteration: 1839 Training loss: 0.42930\n",
      "Epoch: 23/99 Iteration: 1840 Training loss: 0.54096\n",
      "Epoch: 23/99 Iteration: 1841 Training loss: 0.70342\n",
      "Epoch: 23/99 Iteration: 1842 Training loss: 0.46521\n",
      "Epoch: 23/99 Iteration: 1843 Training loss: 0.50756\n",
      "Epoch: 23/99 Iteration: 1844 Training loss: 0.31441\n",
      "Epoch: 23/99 Iteration: 1845 Training loss: 0.41857\n",
      "Epoch: 23/99 Iteration: 1846 Training loss: 0.47374\n",
      "Epoch: 23/99 Iteration: 1847 Training loss: 0.39373\n",
      "Epoch: 23/99 Iteration: 1848 Training loss: 0.61038\n",
      "Epoch: 23/99 Iteration: 1849 Training loss: 0.59447\n",
      "Epoch: 23/99 Iteration: 1850 Training loss: 0.47606\n",
      "***\n",
      "Epoch: 23/99 Iteration: 1850 Validation Acc: 0.8280\n",
      "***\n",
      "Epoch: 23/99 Iteration: 1851 Training loss: 0.47006\n",
      "Epoch: 23/99 Iteration: 1852 Training loss: 0.50671\n",
      "Epoch: 23/99 Iteration: 1853 Training loss: 0.47428\n",
      "Epoch: 23/99 Iteration: 1854 Training loss: 0.58366\n",
      "Epoch: 23/99 Iteration: 1855 Training loss: 0.46798\n",
      "Epoch: 23/99 Iteration: 1856 Training loss: 0.34082\n",
      "Epoch: 23/99 Iteration: 1857 Training loss: 0.44978\n",
      "Epoch: 23/99 Iteration: 1858 Training loss: 0.48001\n",
      "Epoch: 23/99 Iteration: 1859 Training loss: 0.49190\n",
      "Epoch: 23/99 Iteration: 1860 Training loss: 0.43891\n",
      "Epoch: 23/99 Iteration: 1861 Training loss: 0.53875\n",
      "Epoch: 23/99 Iteration: 1862 Training loss: 0.41799\n",
      "Epoch: 23/99 Iteration: 1863 Training loss: 0.52182\n",
      "Epoch: 23/99 Iteration: 1864 Training loss: 0.46489\n",
      "Epoch: 23/99 Iteration: 1865 Training loss: 0.48973\n",
      "Epoch: 23/99 Iteration: 1866 Training loss: 0.46986\n",
      "Epoch: 23/99 Iteration: 1867 Training loss: 0.40047\n",
      "Epoch: 23/99 Iteration: 1868 Training loss: 0.45753\n",
      "Epoch: 23/99 Iteration: 1869 Training loss: 0.84076\n",
      "Epoch: 23/99 Iteration: 1870 Training loss: 0.42482\n",
      "Epoch: 23/99 Iteration: 1871 Training loss: 0.53152\n",
      "Epoch: 23/99 Iteration: 1872 Training loss: 0.49400\n",
      "Epoch: 23/99 Iteration: 1873 Training loss: 0.42553\n",
      "Epoch: 23/99 Iteration: 1874 Training loss: 0.53960\n",
      "Epoch: 23/99 Iteration: 1875 Training loss: 0.54461\n",
      "Epoch: 23/99 Iteration: 1876 Training loss: 0.60447\n",
      "Epoch: 23/99 Iteration: 1877 Training loss: 0.45156\n",
      "Epoch: 23/99 Iteration: 1878 Training loss: 0.37046\n",
      "Epoch: 23/99 Iteration: 1879 Training loss: 0.49653\n",
      "Epoch: 23/99 Iteration: 1880 Training loss: 0.56377\n",
      "Epoch: 23/99 Iteration: 1881 Training loss: 0.50660\n",
      "Epoch: 23/99 Iteration: 1882 Training loss: 0.54832\n",
      "Epoch: 23/99 Iteration: 1883 Training loss: 0.42642\n",
      "Epoch: 23/99 Iteration: 1884 Training loss: 0.59421\n",
      "Epoch: 23/99 Iteration: 1885 Training loss: 0.33929\n",
      "Epoch: 23/99 Iteration: 1886 Training loss: 0.43337\n",
      "Epoch: 23/99 Iteration: 1887 Training loss: 0.49311\n",
      "Epoch: 23/99 Iteration: 1888 Training loss: 0.79233\n",
      "Epoch: 23/99 Iteration: 1889 Training loss: 0.43171\n",
      "Epoch: 23/99 Iteration: 1890 Training loss: 0.40361\n",
      "Epoch: 23/99 Iteration: 1891 Training loss: 0.39175\n",
      "Epoch: 23/99 Iteration: 1892 Training loss: 0.35483\n",
      "Epoch: 23/99 Iteration: 1893 Training loss: 0.65512\n",
      "Epoch: 23/99 Iteration: 1894 Training loss: 0.63609\n",
      "Epoch: 23/99 Iteration: 1895 Training loss: 0.35448\n",
      "Epoch: 23/99 Iteration: 1896 Training loss: 0.44884\n",
      "Epoch: 23/99 Iteration: 1897 Training loss: 0.48219\n",
      "Epoch: 23/99 Iteration: 1898 Training loss: 0.53056\n",
      "Epoch: 23/99 Iteration: 1899 Training loss: 0.46596\n",
      "Epoch: 23/99 Iteration: 1900 Training loss: 0.47274\n",
      "***\n",
      "Epoch: 23/99 Iteration: 1900 Validation Acc: 0.8140\n",
      "***\n",
      "Epoch: 23/99 Iteration: 1901 Training loss: 0.51647\n",
      "Epoch: 23/99 Iteration: 1902 Training loss: 0.42472\n",
      "Epoch: 23/99 Iteration: 1903 Training loss: 0.53067\n",
      "Epoch: 23/99 Iteration: 1904 Training loss: 0.44753\n",
      "Epoch: 23/99 Iteration: 1905 Training loss: 0.57458\n",
      "Epoch: 23/99 Iteration: 1906 Training loss: 0.54404\n",
      "Epoch: 23/99 Iteration: 1907 Training loss: 0.54815\n",
      "Epoch: 23/99 Iteration: 1908 Training loss: 0.46478\n",
      "Epoch: 23/99 Iteration: 1909 Training loss: 0.39018\n",
      "Epoch: 23/99 Iteration: 1910 Training loss: 0.56535\n",
      "Epoch: 23/99 Iteration: 1911 Training loss: 0.41711\n",
      "Epoch: 23/99 Iteration: 1912 Training loss: 0.62981\n",
      "Epoch: 23/99 Iteration: 1913 Training loss: 0.56421\n",
      "Epoch: 23/99 Iteration: 1914 Training loss: 0.46183\n",
      "Epoch: 23/99 Iteration: 1915 Training loss: 0.32566\n",
      "Epoch: 23/99 Iteration: 1916 Training loss: 0.37814\n",
      "Epoch: 23/99 Iteration: 1917 Training loss: 0.44906\n",
      "Epoch: 23/99 Iteration: 1918 Training loss: 0.48967\n",
      "Epoch: 23/99 Iteration: 1919 Training loss: 0.45186\n",
      "Epoch: 24/99 Iteration: 1920 Training loss: 0.40827\n",
      "Epoch: 24/99 Iteration: 1921 Training loss: 0.51083\n",
      "Epoch: 24/99 Iteration: 1922 Training loss: 0.64524\n",
      "Epoch: 24/99 Iteration: 1923 Training loss: 0.47191\n",
      "Epoch: 24/99 Iteration: 1924 Training loss: 0.50079\n",
      "Epoch: 24/99 Iteration: 1925 Training loss: 0.47973\n",
      "Epoch: 24/99 Iteration: 1926 Training loss: 0.45028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/99 Iteration: 1927 Training loss: 0.44129\n",
      "Epoch: 24/99 Iteration: 1928 Training loss: 0.52790\n",
      "Epoch: 24/99 Iteration: 1929 Training loss: 0.46295\n",
      "Epoch: 24/99 Iteration: 1930 Training loss: 0.47927\n",
      "Epoch: 24/99 Iteration: 1931 Training loss: 0.44300\n",
      "Epoch: 24/99 Iteration: 1932 Training loss: 0.42157\n",
      "Epoch: 24/99 Iteration: 1933 Training loss: 0.49599\n",
      "Epoch: 24/99 Iteration: 1934 Training loss: 0.51371\n",
      "Epoch: 24/99 Iteration: 1935 Training loss: 0.36454\n",
      "Epoch: 24/99 Iteration: 1936 Training loss: 0.35920\n",
      "Epoch: 24/99 Iteration: 1937 Training loss: 0.30905\n",
      "Epoch: 24/99 Iteration: 1938 Training loss: 0.47825\n",
      "Epoch: 24/99 Iteration: 1939 Training loss: 0.39674\n",
      "Epoch: 24/99 Iteration: 1940 Training loss: 0.46975\n",
      "Epoch: 24/99 Iteration: 1941 Training loss: 0.62270\n",
      "Epoch: 24/99 Iteration: 1942 Training loss: 0.41162\n",
      "Epoch: 24/99 Iteration: 1943 Training loss: 0.60178\n",
      "Epoch: 24/99 Iteration: 1944 Training loss: 0.45176\n",
      "Epoch: 24/99 Iteration: 1945 Training loss: 0.58388\n",
      "Epoch: 24/99 Iteration: 1946 Training loss: 0.50993\n",
      "Epoch: 24/99 Iteration: 1947 Training loss: 0.31355\n",
      "Epoch: 24/99 Iteration: 1948 Training loss: 0.48174\n",
      "Epoch: 24/99 Iteration: 1949 Training loss: 0.62498\n",
      "Epoch: 24/99 Iteration: 1950 Training loss: 0.42628\n",
      "***\n",
      "Epoch: 24/99 Iteration: 1950 Validation Acc: 0.8160\n",
      "***\n",
      "Epoch: 24/99 Iteration: 1951 Training loss: 0.36188\n",
      "Epoch: 24/99 Iteration: 1952 Training loss: 0.52955\n",
      "Epoch: 24/99 Iteration: 1953 Training loss: 0.52224\n",
      "Epoch: 24/99 Iteration: 1954 Training loss: 0.35746\n",
      "Epoch: 24/99 Iteration: 1955 Training loss: 0.35780\n",
      "Epoch: 24/99 Iteration: 1956 Training loss: 0.76654\n",
      "Epoch: 24/99 Iteration: 1957 Training loss: 0.43199\n",
      "Epoch: 24/99 Iteration: 1958 Training loss: 0.46330\n",
      "Epoch: 24/99 Iteration: 1959 Training loss: 0.39596\n",
      "Epoch: 24/99 Iteration: 1960 Training loss: 0.51338\n",
      "Epoch: 24/99 Iteration: 1961 Training loss: 0.44466\n",
      "Epoch: 24/99 Iteration: 1962 Training loss: 0.49589\n",
      "Epoch: 24/99 Iteration: 1963 Training loss: 0.35859\n",
      "Epoch: 24/99 Iteration: 1964 Training loss: 0.75714\n",
      "Epoch: 24/99 Iteration: 1965 Training loss: 0.42985\n",
      "Epoch: 24/99 Iteration: 1966 Training loss: 0.54005\n",
      "Epoch: 24/99 Iteration: 1967 Training loss: 0.47017\n",
      "Epoch: 24/99 Iteration: 1968 Training loss: 0.66833\n",
      "Epoch: 24/99 Iteration: 1969 Training loss: 0.46813\n",
      "Epoch: 24/99 Iteration: 1970 Training loss: 0.51127\n",
      "Epoch: 24/99 Iteration: 1971 Training loss: 0.57839\n",
      "Epoch: 24/99 Iteration: 1972 Training loss: 0.48902\n",
      "Epoch: 24/99 Iteration: 1973 Training loss: 0.46846\n",
      "Epoch: 24/99 Iteration: 1974 Training loss: 0.54060\n",
      "Epoch: 24/99 Iteration: 1975 Training loss: 0.34709\n",
      "Epoch: 24/99 Iteration: 1976 Training loss: 0.46754\n",
      "Epoch: 24/99 Iteration: 1977 Training loss: 0.52410\n",
      "Epoch: 24/99 Iteration: 1978 Training loss: 0.41306\n",
      "Epoch: 24/99 Iteration: 1979 Training loss: 0.61981\n",
      "Epoch: 24/99 Iteration: 1980 Training loss: 0.45167\n",
      "Epoch: 24/99 Iteration: 1981 Training loss: 0.45207\n",
      "Epoch: 24/99 Iteration: 1982 Training loss: 0.41225\n",
      "Epoch: 24/99 Iteration: 1983 Training loss: 0.59260\n",
      "Epoch: 24/99 Iteration: 1984 Training loss: 0.45060\n",
      "Epoch: 24/99 Iteration: 1985 Training loss: 0.51322\n",
      "Epoch: 24/99 Iteration: 1986 Training loss: 0.52362\n",
      "Epoch: 24/99 Iteration: 1987 Training loss: 0.60007\n",
      "Epoch: 24/99 Iteration: 1988 Training loss: 0.47491\n",
      "Epoch: 24/99 Iteration: 1989 Training loss: 0.60762\n",
      "Epoch: 24/99 Iteration: 1990 Training loss: 0.65094\n",
      "Epoch: 24/99 Iteration: 1991 Training loss: 0.44933\n",
      "Epoch: 24/99 Iteration: 1992 Training loss: 0.45031\n",
      "Epoch: 24/99 Iteration: 1993 Training loss: 0.39754\n",
      "Epoch: 24/99 Iteration: 1994 Training loss: 0.49435\n",
      "Epoch: 24/99 Iteration: 1995 Training loss: 0.37884\n",
      "Epoch: 24/99 Iteration: 1996 Training loss: 0.49699\n",
      "Epoch: 24/99 Iteration: 1997 Training loss: 0.59331\n",
      "Epoch: 24/99 Iteration: 1998 Training loss: 0.49812\n",
      "Epoch: 24/99 Iteration: 1999 Training loss: 0.43446\n",
      "Epoch: 25/99 Iteration: 2000 Training loss: 0.40731\n",
      "***\n",
      "Epoch: 25/99 Iteration: 2000 Validation Acc: 0.8170\n",
      "***\n",
      "Epoch: 25/99 Iteration: 2001 Training loss: 0.49968\n",
      "Epoch: 25/99 Iteration: 2002 Training loss: 0.62995\n",
      "Epoch: 25/99 Iteration: 2003 Training loss: 0.50263\n",
      "Epoch: 25/99 Iteration: 2004 Training loss: 0.45287\n",
      "Epoch: 25/99 Iteration: 2005 Training loss: 0.40671\n",
      "Epoch: 25/99 Iteration: 2006 Training loss: 0.39063\n",
      "Epoch: 25/99 Iteration: 2007 Training loss: 0.49426\n",
      "Epoch: 25/99 Iteration: 2008 Training loss: 0.58290\n",
      "Epoch: 25/99 Iteration: 2009 Training loss: 0.47306\n",
      "Epoch: 25/99 Iteration: 2010 Training loss: 0.35125\n",
      "Epoch: 25/99 Iteration: 2011 Training loss: 0.42879\n",
      "Epoch: 25/99 Iteration: 2012 Training loss: 0.46078\n",
      "Epoch: 25/99 Iteration: 2013 Training loss: 0.60624\n",
      "Epoch: 25/99 Iteration: 2014 Training loss: 0.66913\n",
      "Epoch: 25/99 Iteration: 2015 Training loss: 0.39865\n",
      "Epoch: 25/99 Iteration: 2016 Training loss: 0.29300\n",
      "Epoch: 25/99 Iteration: 2017 Training loss: 0.47241\n",
      "Epoch: 25/99 Iteration: 2018 Training loss: 0.44870\n",
      "Epoch: 25/99 Iteration: 2019 Training loss: 0.37576\n",
      "Epoch: 25/99 Iteration: 2020 Training loss: 0.45102\n",
      "Epoch: 25/99 Iteration: 2021 Training loss: 0.41709\n",
      "Epoch: 25/99 Iteration: 2022 Training loss: 0.66300\n",
      "Epoch: 25/99 Iteration: 2023 Training loss: 0.55023\n",
      "Epoch: 25/99 Iteration: 2024 Training loss: 0.49934\n",
      "Epoch: 25/99 Iteration: 2025 Training loss: 0.56556\n",
      "Epoch: 25/99 Iteration: 2026 Training loss: 0.46361\n",
      "Epoch: 25/99 Iteration: 2027 Training loss: 0.51812\n",
      "Epoch: 25/99 Iteration: 2028 Training loss: 0.51904\n",
      "Epoch: 25/99 Iteration: 2029 Training loss: 0.66078\n",
      "Epoch: 25/99 Iteration: 2030 Training loss: 0.36033\n",
      "Epoch: 25/99 Iteration: 2031 Training loss: 0.39911\n",
      "Epoch: 25/99 Iteration: 2032 Training loss: 0.57786\n",
      "Epoch: 25/99 Iteration: 2033 Training loss: 0.51193\n",
      "Epoch: 25/99 Iteration: 2034 Training loss: 0.43631\n",
      "Epoch: 25/99 Iteration: 2035 Training loss: 0.47275\n",
      "Epoch: 25/99 Iteration: 2036 Training loss: 0.81514\n",
      "Epoch: 25/99 Iteration: 2037 Training loss: 0.30776\n",
      "Epoch: 25/99 Iteration: 2038 Training loss: 0.45666\n",
      "Epoch: 25/99 Iteration: 2039 Training loss: 0.42961\n",
      "Epoch: 25/99 Iteration: 2040 Training loss: 0.46236\n",
      "Epoch: 25/99 Iteration: 2041 Training loss: 0.44605\n",
      "Epoch: 25/99 Iteration: 2042 Training loss: 0.52416\n",
      "Epoch: 25/99 Iteration: 2043 Training loss: 0.36985\n",
      "Epoch: 25/99 Iteration: 2044 Training loss: 0.51731\n",
      "Epoch: 25/99 Iteration: 2045 Training loss: 0.39185\n",
      "Epoch: 25/99 Iteration: 2046 Training loss: 0.58900\n",
      "Epoch: 25/99 Iteration: 2047 Training loss: 0.43221\n",
      "Epoch: 25/99 Iteration: 2048 Training loss: 0.68854\n",
      "Epoch: 25/99 Iteration: 2049 Training loss: 0.37415\n",
      "Epoch: 25/99 Iteration: 2050 Training loss: 0.51636\n",
      "***\n",
      "Epoch: 25/99 Iteration: 2050 Validation Acc: 0.8230\n",
      "***\n",
      "Epoch: 25/99 Iteration: 2051 Training loss: 0.51916\n",
      "Epoch: 25/99 Iteration: 2052 Training loss: 0.54195\n",
      "Epoch: 25/99 Iteration: 2053 Training loss: 0.46731\n",
      "Epoch: 25/99 Iteration: 2054 Training loss: 0.59739\n",
      "Epoch: 25/99 Iteration: 2055 Training loss: 0.41234\n",
      "Epoch: 25/99 Iteration: 2056 Training loss: 0.51061\n",
      "Epoch: 25/99 Iteration: 2057 Training loss: 0.42013\n",
      "Epoch: 25/99 Iteration: 2058 Training loss: 0.51296\n",
      "Epoch: 25/99 Iteration: 2059 Training loss: 0.58099\n",
      "Epoch: 25/99 Iteration: 2060 Training loss: 0.41201\n",
      "Epoch: 25/99 Iteration: 2061 Training loss: 0.40567\n",
      "Epoch: 25/99 Iteration: 2062 Training loss: 0.39965\n",
      "Epoch: 25/99 Iteration: 2063 Training loss: 0.57809\n",
      "Epoch: 25/99 Iteration: 2064 Training loss: 0.43438\n",
      "Epoch: 25/99 Iteration: 2065 Training loss: 0.61148\n",
      "Epoch: 25/99 Iteration: 2066 Training loss: 0.71738\n",
      "Epoch: 25/99 Iteration: 2067 Training loss: 0.71090\n",
      "Epoch: 25/99 Iteration: 2068 Training loss: 0.49467\n",
      "Epoch: 25/99 Iteration: 2069 Training loss: 0.45906\n",
      "Epoch: 25/99 Iteration: 2070 Training loss: 0.59456\n",
      "Epoch: 25/99 Iteration: 2071 Training loss: 0.54607\n",
      "Epoch: 25/99 Iteration: 2072 Training loss: 0.57177\n",
      "Epoch: 25/99 Iteration: 2073 Training loss: 0.54284\n",
      "Epoch: 25/99 Iteration: 2074 Training loss: 0.52909\n",
      "Epoch: 25/99 Iteration: 2075 Training loss: 0.49871\n",
      "Epoch: 25/99 Iteration: 2076 Training loss: 0.48254\n",
      "Epoch: 25/99 Iteration: 2077 Training loss: 0.65669\n",
      "Epoch: 25/99 Iteration: 2078 Training loss: 0.46234\n",
      "Epoch: 25/99 Iteration: 2079 Training loss: 0.58320\n",
      "Epoch: 26/99 Iteration: 2080 Training loss: 0.46997\n",
      "Epoch: 26/99 Iteration: 2081 Training loss: 0.58581\n",
      "Epoch: 26/99 Iteration: 2082 Training loss: 0.44464\n",
      "Epoch: 26/99 Iteration: 2083 Training loss: 0.40739\n",
      "Epoch: 26/99 Iteration: 2084 Training loss: 0.61429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/99 Iteration: 2085 Training loss: 0.50082\n",
      "Epoch: 26/99 Iteration: 2086 Training loss: 0.47062\n",
      "Epoch: 26/99 Iteration: 2087 Training loss: 0.66382\n",
      "Epoch: 26/99 Iteration: 2088 Training loss: 0.63386\n",
      "Epoch: 26/99 Iteration: 2089 Training loss: 0.51912\n",
      "Epoch: 26/99 Iteration: 2090 Training loss: 0.50894\n",
      "Epoch: 26/99 Iteration: 2091 Training loss: 0.51314\n",
      "Epoch: 26/99 Iteration: 2092 Training loss: 0.51277\n",
      "Epoch: 26/99 Iteration: 2093 Training loss: 0.65604\n",
      "Epoch: 26/99 Iteration: 2094 Training loss: 0.69534\n",
      "Epoch: 26/99 Iteration: 2095 Training loss: 0.31514\n",
      "Epoch: 26/99 Iteration: 2096 Training loss: 0.48839\n",
      "Epoch: 26/99 Iteration: 2097 Training loss: 0.44707\n",
      "Epoch: 26/99 Iteration: 2098 Training loss: 0.43594\n",
      "Epoch: 26/99 Iteration: 2099 Training loss: 0.54125\n",
      "Epoch: 26/99 Iteration: 2100 Training loss: 0.49921\n",
      "***\n",
      "Epoch: 26/99 Iteration: 2100 Validation Acc: 0.8200\n",
      "***\n",
      "Epoch: 26/99 Iteration: 2101 Training loss: 0.45583\n",
      "Epoch: 26/99 Iteration: 2102 Training loss: 0.61560\n",
      "Epoch: 26/99 Iteration: 2103 Training loss: 0.56067\n",
      "Epoch: 26/99 Iteration: 2104 Training loss: 0.36706\n",
      "Epoch: 26/99 Iteration: 2105 Training loss: 0.54755\n",
      "Epoch: 26/99 Iteration: 2106 Training loss: 0.58874\n",
      "Epoch: 26/99 Iteration: 2107 Training loss: 0.46315\n",
      "Epoch: 26/99 Iteration: 2108 Training loss: 0.54914\n",
      "Epoch: 26/99 Iteration: 2109 Training loss: 0.68120\n",
      "Epoch: 26/99 Iteration: 2110 Training loss: 0.35683\n",
      "Epoch: 26/99 Iteration: 2111 Training loss: 0.50335\n",
      "Epoch: 26/99 Iteration: 2112 Training loss: 0.50698\n",
      "Epoch: 26/99 Iteration: 2113 Training loss: 0.50044\n",
      "Epoch: 26/99 Iteration: 2114 Training loss: 0.50725\n",
      "Epoch: 26/99 Iteration: 2115 Training loss: 0.49582\n",
      "Epoch: 26/99 Iteration: 2116 Training loss: 0.62922\n",
      "Epoch: 26/99 Iteration: 2117 Training loss: 0.45150\n",
      "Epoch: 26/99 Iteration: 2118 Training loss: 0.37117\n",
      "Epoch: 26/99 Iteration: 2119 Training loss: 0.40223\n",
      "Epoch: 26/99 Iteration: 2120 Training loss: 0.49187\n",
      "Epoch: 26/99 Iteration: 2121 Training loss: 0.58797\n",
      "Epoch: 26/99 Iteration: 2122 Training loss: 0.50805\n",
      "Epoch: 26/99 Iteration: 2123 Training loss: 0.38853\n",
      "Epoch: 26/99 Iteration: 2124 Training loss: 0.56946\n",
      "Epoch: 26/99 Iteration: 2125 Training loss: 0.37375\n",
      "Epoch: 26/99 Iteration: 2126 Training loss: 0.55406\n",
      "Epoch: 26/99 Iteration: 2127 Training loss: 0.44552\n",
      "Epoch: 26/99 Iteration: 2128 Training loss: 0.56088\n",
      "Epoch: 26/99 Iteration: 2129 Training loss: 0.45556\n",
      "Epoch: 26/99 Iteration: 2130 Training loss: 0.48673\n",
      "Epoch: 26/99 Iteration: 2131 Training loss: 0.49729\n",
      "Epoch: 26/99 Iteration: 2132 Training loss: 0.61724\n",
      "Epoch: 26/99 Iteration: 2133 Training loss: 0.44992\n",
      "Epoch: 26/99 Iteration: 2134 Training loss: 0.64033\n",
      "Epoch: 26/99 Iteration: 2135 Training loss: 0.43991\n",
      "Epoch: 26/99 Iteration: 2136 Training loss: 0.61217\n",
      "Epoch: 26/99 Iteration: 2137 Training loss: 0.57740\n",
      "Epoch: 26/99 Iteration: 2138 Training loss: 0.64680\n",
      "Epoch: 26/99 Iteration: 2139 Training loss: 0.58460\n",
      "Epoch: 26/99 Iteration: 2140 Training loss: 0.47802\n",
      "Epoch: 26/99 Iteration: 2141 Training loss: 0.56657\n",
      "Epoch: 26/99 Iteration: 2142 Training loss: 0.44731\n",
      "Epoch: 26/99 Iteration: 2143 Training loss: 0.55258\n",
      "Epoch: 26/99 Iteration: 2144 Training loss: 0.44191\n",
      "Epoch: 26/99 Iteration: 2145 Training loss: 0.65854\n",
      "Epoch: 26/99 Iteration: 2146 Training loss: 0.50094\n",
      "Epoch: 26/99 Iteration: 2147 Training loss: 0.51432\n",
      "Epoch: 26/99 Iteration: 2148 Training loss: 0.52880\n",
      "Epoch: 26/99 Iteration: 2149 Training loss: 0.47554\n",
      "Epoch: 26/99 Iteration: 2150 Training loss: 0.62717\n",
      "***\n",
      "Epoch: 26/99 Iteration: 2150 Validation Acc: 0.8240\n",
      "***\n",
      "Epoch: 26/99 Iteration: 2151 Training loss: 0.50468\n",
      "Epoch: 26/99 Iteration: 2152 Training loss: 0.54176\n",
      "Epoch: 26/99 Iteration: 2153 Training loss: 0.36996\n",
      "Epoch: 26/99 Iteration: 2154 Training loss: 0.44607\n",
      "Epoch: 26/99 Iteration: 2155 Training loss: 0.41578\n",
      "Epoch: 26/99 Iteration: 2156 Training loss: 0.53246\n",
      "Epoch: 26/99 Iteration: 2157 Training loss: 0.52039\n",
      "Epoch: 26/99 Iteration: 2158 Training loss: 0.61833\n",
      "Epoch: 26/99 Iteration: 2159 Training loss: 0.55851\n",
      "Epoch: 27/99 Iteration: 2160 Training loss: 0.40062\n",
      "Epoch: 27/99 Iteration: 2161 Training loss: 0.61455\n",
      "Epoch: 27/99 Iteration: 2162 Training loss: 0.44980\n",
      "Epoch: 27/99 Iteration: 2163 Training loss: 0.54719\n",
      "Epoch: 27/99 Iteration: 2164 Training loss: 0.50153\n",
      "Epoch: 27/99 Iteration: 2165 Training loss: 0.44152\n",
      "Epoch: 27/99 Iteration: 2166 Training loss: 0.50592\n",
      "Epoch: 27/99 Iteration: 2167 Training loss: 0.54557\n",
      "Epoch: 27/99 Iteration: 2168 Training loss: 0.61956\n",
      "Epoch: 27/99 Iteration: 2169 Training loss: 0.34289\n",
      "Epoch: 27/99 Iteration: 2170 Training loss: 0.44253\n",
      "Epoch: 27/99 Iteration: 2171 Training loss: 0.51568\n",
      "Epoch: 27/99 Iteration: 2172 Training loss: 0.48934\n",
      "Epoch: 27/99 Iteration: 2173 Training loss: 0.61620\n",
      "Epoch: 27/99 Iteration: 2174 Training loss: 0.64345\n",
      "Epoch: 27/99 Iteration: 2175 Training loss: 0.44376\n",
      "Epoch: 27/99 Iteration: 2176 Training loss: 0.41267\n",
      "Epoch: 27/99 Iteration: 2177 Training loss: 0.47215\n",
      "Epoch: 27/99 Iteration: 2178 Training loss: 0.47440\n",
      "Epoch: 27/99 Iteration: 2179 Training loss: 0.51336\n",
      "Epoch: 27/99 Iteration: 2180 Training loss: 0.63350\n",
      "Epoch: 27/99 Iteration: 2181 Training loss: 0.51689\n",
      "Epoch: 27/99 Iteration: 2182 Training loss: 0.45447\n",
      "Epoch: 27/99 Iteration: 2183 Training loss: 0.50746\n",
      "Epoch: 27/99 Iteration: 2184 Training loss: 0.45279\n",
      "Epoch: 27/99 Iteration: 2185 Training loss: 0.58818\n",
      "Epoch: 27/99 Iteration: 2186 Training loss: 0.60020\n",
      "Epoch: 27/99 Iteration: 2187 Training loss: 0.53677\n",
      "Epoch: 27/99 Iteration: 2188 Training loss: 0.72196\n",
      "Epoch: 27/99 Iteration: 2189 Training loss: 0.58956\n",
      "Epoch: 27/99 Iteration: 2190 Training loss: 0.41184\n",
      "Epoch: 27/99 Iteration: 2191 Training loss: 0.50579\n",
      "Epoch: 27/99 Iteration: 2192 Training loss: 0.45366\n",
      "Epoch: 27/99 Iteration: 2193 Training loss: 0.60462\n",
      "Epoch: 27/99 Iteration: 2194 Training loss: 0.52316\n",
      "Epoch: 27/99 Iteration: 2195 Training loss: 0.40637\n",
      "Epoch: 27/99 Iteration: 2196 Training loss: 0.63111\n",
      "Epoch: 27/99 Iteration: 2197 Training loss: 0.48096\n",
      "Epoch: 27/99 Iteration: 2198 Training loss: 0.51820\n",
      "Epoch: 27/99 Iteration: 2199 Training loss: 0.53528\n",
      "Epoch: 27/99 Iteration: 2200 Training loss: 0.60140\n",
      "***\n",
      "Epoch: 27/99 Iteration: 2200 Validation Acc: 0.8020\n",
      "***\n",
      "Epoch: 27/99 Iteration: 2201 Training loss: 0.72847\n",
      "Epoch: 27/99 Iteration: 2202 Training loss: 0.58155\n",
      "Epoch: 27/99 Iteration: 2203 Training loss: 0.57167\n",
      "Epoch: 27/99 Iteration: 2204 Training loss: 0.54627\n",
      "Epoch: 27/99 Iteration: 2205 Training loss: 0.51018\n",
      "Epoch: 27/99 Iteration: 2206 Training loss: 0.56762\n",
      "Epoch: 27/99 Iteration: 2207 Training loss: 0.47546\n",
      "Epoch: 27/99 Iteration: 2208 Training loss: 0.58485\n",
      "Epoch: 27/99 Iteration: 2209 Training loss: 0.38127\n",
      "Epoch: 27/99 Iteration: 2210 Training loss: 0.58867\n",
      "Epoch: 27/99 Iteration: 2211 Training loss: 0.45629\n",
      "Epoch: 27/99 Iteration: 2212 Training loss: 0.52198\n",
      "Epoch: 27/99 Iteration: 2213 Training loss: 0.54099\n",
      "Epoch: 27/99 Iteration: 2214 Training loss: 0.59836\n",
      "Epoch: 27/99 Iteration: 2215 Training loss: 0.47805\n",
      "Epoch: 27/99 Iteration: 2216 Training loss: 0.57370\n",
      "Epoch: 27/99 Iteration: 2217 Training loss: 0.50862\n",
      "Epoch: 27/99 Iteration: 2218 Training loss: 0.49323\n",
      "Epoch: 27/99 Iteration: 2219 Training loss: 0.64895\n",
      "Epoch: 27/99 Iteration: 2220 Training loss: 0.46263\n",
      "Epoch: 27/99 Iteration: 2221 Training loss: 0.66672\n",
      "Epoch: 27/99 Iteration: 2222 Training loss: 0.41539\n",
      "Epoch: 27/99 Iteration: 2223 Training loss: 0.59546\n",
      "Epoch: 27/99 Iteration: 2224 Training loss: 0.62713\n",
      "Epoch: 27/99 Iteration: 2225 Training loss: 0.53793\n",
      "Epoch: 27/99 Iteration: 2226 Training loss: 0.57372\n",
      "Epoch: 27/99 Iteration: 2227 Training loss: 0.62019\n",
      "Epoch: 27/99 Iteration: 2228 Training loss: 0.39791\n",
      "Epoch: 27/99 Iteration: 2229 Training loss: 0.43887\n",
      "Epoch: 27/99 Iteration: 2230 Training loss: 0.64191\n",
      "Epoch: 27/99 Iteration: 2231 Training loss: 0.50428\n",
      "Epoch: 27/99 Iteration: 2232 Training loss: 0.59179\n",
      "Epoch: 27/99 Iteration: 2233 Training loss: 0.55059\n",
      "Epoch: 27/99 Iteration: 2234 Training loss: 0.51712\n",
      "Epoch: 27/99 Iteration: 2235 Training loss: 0.44050\n",
      "Epoch: 27/99 Iteration: 2236 Training loss: 0.62078\n",
      "Epoch: 27/99 Iteration: 2237 Training loss: 0.48732\n",
      "Epoch: 27/99 Iteration: 2238 Training loss: 0.65591\n",
      "Epoch: 27/99 Iteration: 2239 Training loss: 0.53536\n",
      "Epoch: 28/99 Iteration: 2240 Training loss: 0.50361\n",
      "Epoch: 28/99 Iteration: 2241 Training loss: 0.65108\n",
      "Epoch: 28/99 Iteration: 2242 Training loss: 0.52265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/99 Iteration: 2243 Training loss: 0.51210\n",
      "Epoch: 28/99 Iteration: 2244 Training loss: 0.34704\n",
      "Epoch: 28/99 Iteration: 2245 Training loss: 0.39600\n",
      "Epoch: 28/99 Iteration: 2246 Training loss: 0.43689\n",
      "Epoch: 28/99 Iteration: 2247 Training loss: 0.52992\n",
      "Epoch: 28/99 Iteration: 2248 Training loss: 0.62054\n",
      "Epoch: 28/99 Iteration: 2249 Training loss: 0.64865\n",
      "Epoch: 28/99 Iteration: 2250 Training loss: 0.40322\n",
      "***\n",
      "Epoch: 28/99 Iteration: 2250 Validation Acc: 0.8100\n",
      "***\n",
      "Epoch: 28/99 Iteration: 2251 Training loss: 0.38840\n",
      "Epoch: 28/99 Iteration: 2252 Training loss: 0.38866\n",
      "Epoch: 28/99 Iteration: 2253 Training loss: 0.46218\n",
      "Epoch: 28/99 Iteration: 2254 Training loss: 0.61271\n",
      "Epoch: 28/99 Iteration: 2255 Training loss: 0.54544\n",
      "Epoch: 28/99 Iteration: 2256 Training loss: 0.42501\n",
      "Epoch: 28/99 Iteration: 2257 Training loss: 0.41123\n",
      "Epoch: 28/99 Iteration: 2258 Training loss: 0.48744\n",
      "Epoch: 28/99 Iteration: 2259 Training loss: 0.38712\n",
      "Epoch: 28/99 Iteration: 2260 Training loss: 0.37300\n",
      "Epoch: 28/99 Iteration: 2261 Training loss: 0.45251\n",
      "Epoch: 28/99 Iteration: 2262 Training loss: 0.40793\n",
      "Epoch: 28/99 Iteration: 2263 Training loss: 0.41332\n",
      "Epoch: 28/99 Iteration: 2264 Training loss: 0.34067\n",
      "Epoch: 28/99 Iteration: 2265 Training loss: 0.43918\n",
      "Epoch: 28/99 Iteration: 2266 Training loss: 0.49019\n",
      "Epoch: 28/99 Iteration: 2267 Training loss: 0.33820\n",
      "Epoch: 28/99 Iteration: 2268 Training loss: 0.52290\n",
      "Epoch: 28/99 Iteration: 2269 Training loss: 0.52024\n",
      "Epoch: 28/99 Iteration: 2270 Training loss: 0.30419\n",
      "Epoch: 28/99 Iteration: 2271 Training loss: 0.45977\n",
      "Epoch: 28/99 Iteration: 2272 Training loss: 0.47829\n",
      "Epoch: 28/99 Iteration: 2273 Training loss: 0.39488\n",
      "Epoch: 28/99 Iteration: 2274 Training loss: 0.35390\n",
      "Epoch: 28/99 Iteration: 2275 Training loss: 0.46393\n",
      "Epoch: 28/99 Iteration: 2276 Training loss: 0.47689\n",
      "Epoch: 28/99 Iteration: 2277 Training loss: 0.35157\n",
      "Epoch: 28/99 Iteration: 2278 Training loss: 0.32194\n",
      "Epoch: 28/99 Iteration: 2279 Training loss: 0.39119\n",
      "Epoch: 28/99 Iteration: 2280 Training loss: 0.37807\n",
      "Epoch: 28/99 Iteration: 2281 Training loss: 0.45534\n",
      "Epoch: 28/99 Iteration: 2282 Training loss: 0.47748\n",
      "Epoch: 28/99 Iteration: 2283 Training loss: 0.39939\n",
      "Epoch: 28/99 Iteration: 2284 Training loss: 0.45647\n",
      "Epoch: 28/99 Iteration: 2285 Training loss: 0.45960\n",
      "Epoch: 28/99 Iteration: 2286 Training loss: 0.41190\n",
      "Epoch: 28/99 Iteration: 2287 Training loss: 0.39337\n",
      "Epoch: 28/99 Iteration: 2288 Training loss: 0.72939\n",
      "Epoch: 28/99 Iteration: 2289 Training loss: 0.39576\n",
      "Epoch: 28/99 Iteration: 2290 Training loss: 0.51098\n",
      "Epoch: 28/99 Iteration: 2291 Training loss: 0.57656\n",
      "Epoch: 28/99 Iteration: 2292 Training loss: 0.52745\n",
      "Epoch: 28/99 Iteration: 2293 Training loss: 0.53571\n",
      "Epoch: 28/99 Iteration: 2294 Training loss: 0.48367\n",
      "Epoch: 28/99 Iteration: 2295 Training loss: 0.30545\n",
      "Epoch: 28/99 Iteration: 2296 Training loss: 0.52140\n",
      "Epoch: 28/99 Iteration: 2297 Training loss: 0.55037\n",
      "Epoch: 28/99 Iteration: 2298 Training loss: 0.56940\n",
      "Epoch: 28/99 Iteration: 2299 Training loss: 0.60863\n",
      "Epoch: 28/99 Iteration: 2300 Training loss: 0.39469\n",
      "***\n",
      "Epoch: 28/99 Iteration: 2300 Validation Acc: 0.8220\n",
      "***\n",
      "Epoch: 28/99 Iteration: 2301 Training loss: 0.49833\n",
      "Epoch: 28/99 Iteration: 2302 Training loss: 0.41184\n",
      "Epoch: 28/99 Iteration: 2303 Training loss: 0.55081\n",
      "Epoch: 28/99 Iteration: 2304 Training loss: 0.47958\n",
      "Epoch: 28/99 Iteration: 2305 Training loss: 0.56862\n",
      "Epoch: 28/99 Iteration: 2306 Training loss: 0.66175\n",
      "Epoch: 28/99 Iteration: 2307 Training loss: 0.67181\n",
      "Epoch: 28/99 Iteration: 2308 Training loss: 0.51488\n",
      "Epoch: 28/99 Iteration: 2309 Training loss: 0.46106\n",
      "Epoch: 28/99 Iteration: 2310 Training loss: 0.61577\n",
      "Epoch: 28/99 Iteration: 2311 Training loss: 0.48127\n",
      "Epoch: 28/99 Iteration: 2312 Training loss: 0.44632\n",
      "Epoch: 28/99 Iteration: 2313 Training loss: 0.39752\n",
      "Epoch: 28/99 Iteration: 2314 Training loss: 0.51520\n",
      "Epoch: 28/99 Iteration: 2315 Training loss: 0.45561\n",
      "Epoch: 28/99 Iteration: 2316 Training loss: 0.53848\n",
      "Epoch: 28/99 Iteration: 2317 Training loss: 0.47337\n",
      "Epoch: 28/99 Iteration: 2318 Training loss: 0.52283\n",
      "Epoch: 28/99 Iteration: 2319 Training loss: 0.39376\n",
      "Epoch: 29/99 Iteration: 2320 Training loss: 0.32593\n",
      "Epoch: 29/99 Iteration: 2321 Training loss: 0.59865\n",
      "Epoch: 29/99 Iteration: 2322 Training loss: 0.58116\n",
      "Epoch: 29/99 Iteration: 2323 Training loss: 0.57490\n",
      "Epoch: 29/99 Iteration: 2324 Training loss: 0.47593\n",
      "Epoch: 29/99 Iteration: 2325 Training loss: 0.29983\n",
      "Epoch: 29/99 Iteration: 2326 Training loss: 0.39901\n",
      "Epoch: 29/99 Iteration: 2327 Training loss: 0.43986\n",
      "Epoch: 29/99 Iteration: 2328 Training loss: 0.54267\n",
      "Epoch: 29/99 Iteration: 2329 Training loss: 0.42438\n",
      "Epoch: 29/99 Iteration: 2330 Training loss: 0.46168\n",
      "Epoch: 29/99 Iteration: 2331 Training loss: 0.46028\n",
      "Epoch: 29/99 Iteration: 2332 Training loss: 0.47069\n",
      "Epoch: 29/99 Iteration: 2333 Training loss: 0.60744\n",
      "Epoch: 29/99 Iteration: 2334 Training loss: 0.57573\n",
      "Epoch: 29/99 Iteration: 2335 Training loss: 0.41589\n",
      "Epoch: 29/99 Iteration: 2336 Training loss: 0.31724\n",
      "Epoch: 29/99 Iteration: 2337 Training loss: 0.28475\n",
      "Epoch: 29/99 Iteration: 2338 Training loss: 0.61772\n",
      "Epoch: 29/99 Iteration: 2339 Training loss: 0.47104\n",
      "Epoch: 29/99 Iteration: 2340 Training loss: 0.47740\n",
      "Epoch: 29/99 Iteration: 2341 Training loss: 0.38979\n",
      "Epoch: 29/99 Iteration: 2342 Training loss: 0.49184\n",
      "Epoch: 29/99 Iteration: 2343 Training loss: 0.53688\n",
      "Epoch: 29/99 Iteration: 2344 Training loss: 0.36308\n",
      "Epoch: 29/99 Iteration: 2345 Training loss: 0.44772\n",
      "Epoch: 29/99 Iteration: 2346 Training loss: 0.48152\n",
      "Epoch: 29/99 Iteration: 2347 Training loss: 0.36411\n",
      "Epoch: 29/99 Iteration: 2348 Training loss: 0.60635\n",
      "Epoch: 29/99 Iteration: 2349 Training loss: 0.62829\n",
      "Epoch: 29/99 Iteration: 2350 Training loss: 0.38429\n",
      "***\n",
      "Epoch: 29/99 Iteration: 2350 Validation Acc: 0.8300\n",
      "***\n",
      "Epoch: 29/99 Iteration: 2351 Training loss: 0.45462\n",
      "Epoch: 29/99 Iteration: 2352 Training loss: 0.51489\n",
      "Epoch: 29/99 Iteration: 2353 Training loss: 0.50975\n",
      "Epoch: 29/99 Iteration: 2354 Training loss: 0.61634\n",
      "Epoch: 29/99 Iteration: 2355 Training loss: 0.48164\n",
      "Epoch: 29/99 Iteration: 2356 Training loss: 0.53171\n",
      "Epoch: 29/99 Iteration: 2357 Training loss: 0.46612\n",
      "Epoch: 29/99 Iteration: 2358 Training loss: 0.41422\n",
      "Epoch: 29/99 Iteration: 2359 Training loss: 0.51311\n",
      "Epoch: 29/99 Iteration: 2360 Training loss: 0.49696\n",
      "Epoch: 29/99 Iteration: 2361 Training loss: 0.54130\n",
      "Epoch: 29/99 Iteration: 2362 Training loss: 0.50594\n",
      "Epoch: 29/99 Iteration: 2363 Training loss: 0.35630\n",
      "Epoch: 29/99 Iteration: 2364 Training loss: 0.47508\n",
      "Epoch: 29/99 Iteration: 2365 Training loss: 0.47288\n",
      "Epoch: 29/99 Iteration: 2366 Training loss: 0.64688\n",
      "Epoch: 29/99 Iteration: 2367 Training loss: 0.55073\n",
      "Epoch: 29/99 Iteration: 2368 Training loss: 0.63909\n",
      "Epoch: 29/99 Iteration: 2369 Training loss: 0.29916\n",
      "Epoch: 29/99 Iteration: 2370 Training loss: 0.43922\n",
      "Epoch: 29/99 Iteration: 2371 Training loss: 0.50151\n",
      "Epoch: 29/99 Iteration: 2372 Training loss: 0.39750\n",
      "Epoch: 29/99 Iteration: 2373 Training loss: 0.46070\n",
      "Epoch: 29/99 Iteration: 2374 Training loss: 0.42508\n",
      "Epoch: 29/99 Iteration: 2375 Training loss: 0.49660\n",
      "Epoch: 29/99 Iteration: 2376 Training loss: 0.52279\n",
      "Epoch: 29/99 Iteration: 2377 Training loss: 0.49151\n",
      "Epoch: 29/99 Iteration: 2378 Training loss: 0.42624\n",
      "Epoch: 29/99 Iteration: 2379 Training loss: 0.56033\n",
      "Epoch: 29/99 Iteration: 2380 Training loss: 0.40047\n",
      "Epoch: 29/99 Iteration: 2381 Training loss: 0.50085\n",
      "Epoch: 29/99 Iteration: 2382 Training loss: 0.40501\n",
      "Epoch: 29/99 Iteration: 2383 Training loss: 0.57501\n",
      "Epoch: 29/99 Iteration: 2384 Training loss: 0.42837\n",
      "Epoch: 29/99 Iteration: 2385 Training loss: 0.57840\n",
      "Epoch: 29/99 Iteration: 2386 Training loss: 0.66890\n",
      "Epoch: 29/99 Iteration: 2387 Training loss: 0.53241\n",
      "Epoch: 29/99 Iteration: 2388 Training loss: 0.42108\n",
      "Epoch: 29/99 Iteration: 2389 Training loss: 0.45813\n",
      "Epoch: 29/99 Iteration: 2390 Training loss: 0.63645\n",
      "Epoch: 29/99 Iteration: 2391 Training loss: 0.52567\n",
      "Epoch: 29/99 Iteration: 2392 Training loss: 0.62322\n",
      "Epoch: 29/99 Iteration: 2393 Training loss: 0.39630\n",
      "Epoch: 29/99 Iteration: 2394 Training loss: 0.43361\n",
      "Epoch: 29/99 Iteration: 2395 Training loss: 0.36396\n",
      "Epoch: 29/99 Iteration: 2396 Training loss: 0.44888\n",
      "Epoch: 29/99 Iteration: 2397 Training loss: 0.55822\n",
      "Epoch: 29/99 Iteration: 2398 Training loss: 0.54263\n",
      "Epoch: 29/99 Iteration: 2399 Training loss: 0.51189\n",
      "Epoch: 30/99 Iteration: 2400 Training loss: 0.37928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "Epoch: 30/99 Iteration: 2400 Validation Acc: 0.8320\n",
      "***\n",
      "Epoch: 30/99 Iteration: 2401 Training loss: 0.41615\n",
      "Epoch: 30/99 Iteration: 2402 Training loss: 0.54470\n",
      "Epoch: 30/99 Iteration: 2403 Training loss: 0.41892\n",
      "Epoch: 30/99 Iteration: 2404 Training loss: 0.39579\n",
      "Epoch: 30/99 Iteration: 2405 Training loss: 0.40876\n",
      "Epoch: 30/99 Iteration: 2406 Training loss: 0.46753\n",
      "Epoch: 30/99 Iteration: 2407 Training loss: 0.35126\n",
      "Epoch: 30/99 Iteration: 2408 Training loss: 0.57807\n",
      "Epoch: 30/99 Iteration: 2409 Training loss: 0.46953\n",
      "Epoch: 30/99 Iteration: 2410 Training loss: 0.36997\n",
      "Epoch: 30/99 Iteration: 2411 Training loss: 0.38581\n",
      "Epoch: 30/99 Iteration: 2412 Training loss: 0.36455\n",
      "Epoch: 30/99 Iteration: 2413 Training loss: 0.51292\n",
      "Epoch: 30/99 Iteration: 2414 Training loss: 0.61790\n",
      "Epoch: 30/99 Iteration: 2415 Training loss: 0.46538\n",
      "Epoch: 30/99 Iteration: 2416 Training loss: 0.36508\n",
      "Epoch: 30/99 Iteration: 2417 Training loss: 0.28755\n",
      "Epoch: 30/99 Iteration: 2418 Training loss: 0.55035\n",
      "Epoch: 30/99 Iteration: 2419 Training loss: 0.45924\n",
      "Epoch: 30/99 Iteration: 2420 Training loss: 0.64391\n",
      "Epoch: 30/99 Iteration: 2421 Training loss: 0.50322\n",
      "Epoch: 30/99 Iteration: 2422 Training loss: 0.39610\n",
      "Epoch: 30/99 Iteration: 2423 Training loss: 0.42178\n",
      "Epoch: 30/99 Iteration: 2424 Training loss: 0.33964\n",
      "Epoch: 30/99 Iteration: 2425 Training loss: 0.43406\n",
      "Epoch: 30/99 Iteration: 2426 Training loss: 0.54261\n",
      "Epoch: 30/99 Iteration: 2427 Training loss: 0.38938\n",
      "Epoch: 30/99 Iteration: 2428 Training loss: 0.50798\n",
      "Epoch: 30/99 Iteration: 2429 Training loss: 0.67227\n",
      "Epoch: 30/99 Iteration: 2430 Training loss: 0.48528\n",
      "Epoch: 30/99 Iteration: 2431 Training loss: 0.38831\n",
      "Epoch: 30/99 Iteration: 2432 Training loss: 0.54013\n",
      "Epoch: 30/99 Iteration: 2433 Training loss: 0.36148\n",
      "Epoch: 30/99 Iteration: 2434 Training loss: 0.40356\n",
      "Epoch: 30/99 Iteration: 2435 Training loss: 0.36563\n",
      "Epoch: 30/99 Iteration: 2436 Training loss: 0.47531\n",
      "Epoch: 30/99 Iteration: 2437 Training loss: 0.37814\n",
      "Epoch: 30/99 Iteration: 2438 Training loss: 0.36734\n",
      "Epoch: 30/99 Iteration: 2439 Training loss: 0.43907\n",
      "Epoch: 30/99 Iteration: 2440 Training loss: 0.46170\n",
      "Epoch: 30/99 Iteration: 2441 Training loss: 0.37316\n",
      "Epoch: 30/99 Iteration: 2442 Training loss: 0.49106\n",
      "Epoch: 30/99 Iteration: 2443 Training loss: 0.36249\n",
      "Epoch: 30/99 Iteration: 2444 Training loss: 0.64875\n",
      "Epoch: 30/99 Iteration: 2445 Training loss: 0.53560\n",
      "Epoch: 30/99 Iteration: 2446 Training loss: 0.46411\n",
      "Epoch: 30/99 Iteration: 2447 Training loss: 0.41977\n",
      "Epoch: 30/99 Iteration: 2448 Training loss: 0.54916\n",
      "Epoch: 30/99 Iteration: 2449 Training loss: 0.40440\n",
      "Epoch: 30/99 Iteration: 2450 Training loss: 0.42203\n",
      "***\n",
      "Epoch: 30/99 Iteration: 2450 Validation Acc: 0.8330\n",
      "***\n",
      "Epoch: 30/99 Iteration: 2451 Training loss: 0.45198\n",
      "Epoch: 30/99 Iteration: 2452 Training loss: 0.44128\n",
      "Epoch: 30/99 Iteration: 2453 Training loss: 0.59254\n",
      "Epoch: 30/99 Iteration: 2454 Training loss: 0.66617\n",
      "Epoch: 30/99 Iteration: 2455 Training loss: 0.38890\n",
      "Epoch: 30/99 Iteration: 2456 Training loss: 0.43184\n",
      "Epoch: 30/99 Iteration: 2457 Training loss: 0.40906\n",
      "Epoch: 30/99 Iteration: 2458 Training loss: 0.43071\n",
      "Epoch: 30/99 Iteration: 2459 Training loss: 0.44946\n",
      "Epoch: 30/99 Iteration: 2460 Training loss: 0.42606\n",
      "Epoch: 30/99 Iteration: 2461 Training loss: 0.51128\n",
      "Epoch: 30/99 Iteration: 2462 Training loss: 0.45989\n",
      "Epoch: 30/99 Iteration: 2463 Training loss: 0.47133\n",
      "Epoch: 30/99 Iteration: 2464 Training loss: 0.42617\n",
      "Epoch: 30/99 Iteration: 2465 Training loss: 0.53436\n",
      "Epoch: 30/99 Iteration: 2466 Training loss: 0.55711\n",
      "Epoch: 30/99 Iteration: 2467 Training loss: 0.56943\n",
      "Epoch: 30/99 Iteration: 2468 Training loss: 0.49412\n",
      "Epoch: 30/99 Iteration: 2469 Training loss: 0.49342\n",
      "Epoch: 30/99 Iteration: 2470 Training loss: 0.57207\n",
      "Epoch: 30/99 Iteration: 2471 Training loss: 0.37250\n",
      "Epoch: 30/99 Iteration: 2472 Training loss: 0.41597\n",
      "Epoch: 30/99 Iteration: 2473 Training loss: 0.34242\n",
      "Epoch: 30/99 Iteration: 2474 Training loss: 0.38736\n",
      "Epoch: 30/99 Iteration: 2475 Training loss: 0.31412\n",
      "Epoch: 30/99 Iteration: 2476 Training loss: 0.50005\n",
      "Epoch: 30/99 Iteration: 2477 Training loss: 0.51930\n",
      "Epoch: 30/99 Iteration: 2478 Training loss: 0.57773\n",
      "Epoch: 30/99 Iteration: 2479 Training loss: 0.41761\n",
      "Epoch: 31/99 Iteration: 2480 Training loss: 0.32410\n",
      "Epoch: 31/99 Iteration: 2481 Training loss: 0.46354\n",
      "Epoch: 31/99 Iteration: 2482 Training loss: 0.41171\n",
      "Epoch: 31/99 Iteration: 2483 Training loss: 0.37411\n",
      "Epoch: 31/99 Iteration: 2484 Training loss: 0.33963\n",
      "Epoch: 31/99 Iteration: 2485 Training loss: 0.47293\n",
      "Epoch: 31/99 Iteration: 2486 Training loss: 0.38974\n",
      "Epoch: 31/99 Iteration: 2487 Training loss: 0.39401\n",
      "Epoch: 31/99 Iteration: 2488 Training loss: 0.53303\n",
      "Epoch: 31/99 Iteration: 2489 Training loss: 0.44167\n",
      "Epoch: 31/99 Iteration: 2490 Training loss: 0.33303\n",
      "Epoch: 31/99 Iteration: 2491 Training loss: 0.36947\n",
      "Epoch: 31/99 Iteration: 2492 Training loss: 0.37186\n",
      "Epoch: 31/99 Iteration: 2493 Training loss: 0.54774\n",
      "Epoch: 31/99 Iteration: 2494 Training loss: 0.42605\n",
      "Epoch: 31/99 Iteration: 2495 Training loss: 0.34231\n",
      "Epoch: 31/99 Iteration: 2496 Training loss: 0.42565\n",
      "Epoch: 31/99 Iteration: 2497 Training loss: 0.40763\n",
      "Epoch: 31/99 Iteration: 2498 Training loss: 0.33059\n",
      "Epoch: 31/99 Iteration: 2499 Training loss: 0.44769\n",
      "Epoch: 31/99 Iteration: 2500 Training loss: 0.39529\n",
      "***\n",
      "Epoch: 31/99 Iteration: 2500 Validation Acc: 0.8390\n",
      "***\n",
      "Epoch: 31/99 Iteration: 2501 Training loss: 0.43314\n",
      "Epoch: 31/99 Iteration: 2502 Training loss: 0.54197\n",
      "Epoch: 31/99 Iteration: 2503 Training loss: 0.41961\n",
      "Epoch: 31/99 Iteration: 2504 Training loss: 0.35630\n",
      "Epoch: 31/99 Iteration: 2505 Training loss: 0.43084\n",
      "Epoch: 31/99 Iteration: 2506 Training loss: 0.43289\n",
      "Epoch: 31/99 Iteration: 2507 Training loss: 0.49291\n",
      "Epoch: 31/99 Iteration: 2508 Training loss: 0.54681\n",
      "Epoch: 31/99 Iteration: 2509 Training loss: 0.63046\n",
      "Epoch: 31/99 Iteration: 2510 Training loss: 0.37259\n",
      "Epoch: 31/99 Iteration: 2511 Training loss: 0.42689\n",
      "Epoch: 31/99 Iteration: 2512 Training loss: 0.38257\n",
      "Epoch: 31/99 Iteration: 2513 Training loss: 0.40963\n",
      "Epoch: 31/99 Iteration: 2514 Training loss: 0.42851\n",
      "Epoch: 31/99 Iteration: 2515 Training loss: 0.46529\n",
      "Epoch: 31/99 Iteration: 2516 Training loss: 0.49732\n",
      "Epoch: 31/99 Iteration: 2517 Training loss: 0.43778\n",
      "Epoch: 31/99 Iteration: 2518 Training loss: 0.33726\n",
      "Epoch: 31/99 Iteration: 2519 Training loss: 0.38573\n",
      "Epoch: 31/99 Iteration: 2520 Training loss: 0.45732\n",
      "Epoch: 31/99 Iteration: 2521 Training loss: 0.31240\n",
      "Epoch: 31/99 Iteration: 2522 Training loss: 0.41320\n",
      "Epoch: 31/99 Iteration: 2523 Training loss: 0.33097\n",
      "Epoch: 31/99 Iteration: 2524 Training loss: 0.43583\n",
      "Epoch: 31/99 Iteration: 2525 Training loss: 0.39642\n",
      "Epoch: 31/99 Iteration: 2526 Training loss: 0.44107\n",
      "Epoch: 31/99 Iteration: 2527 Training loss: 0.48388\n",
      "Epoch: 31/99 Iteration: 2528 Training loss: 0.69890\n",
      "Epoch: 31/99 Iteration: 2529 Training loss: 0.44411\n",
      "Epoch: 31/99 Iteration: 2530 Training loss: 0.39957\n",
      "Epoch: 31/99 Iteration: 2531 Training loss: 0.37618\n",
      "Epoch: 31/99 Iteration: 2532 Training loss: 0.43948\n",
      "Epoch: 31/99 Iteration: 2533 Training loss: 0.40579\n",
      "Epoch: 31/99 Iteration: 2534 Training loss: 0.45018\n",
      "Epoch: 31/99 Iteration: 2535 Training loss: 0.52069\n",
      "Epoch: 31/99 Iteration: 2536 Training loss: 0.46755\n",
      "Epoch: 31/99 Iteration: 2537 Training loss: 0.47208\n",
      "Epoch: 31/99 Iteration: 2538 Training loss: 0.48664\n",
      "Epoch: 31/99 Iteration: 2539 Training loss: 0.59925\n",
      "Epoch: 31/99 Iteration: 2540 Training loss: 0.45469\n",
      "Epoch: 31/99 Iteration: 2541 Training loss: 0.43638\n",
      "Epoch: 31/99 Iteration: 2542 Training loss: 0.29084\n",
      "Epoch: 31/99 Iteration: 2543 Training loss: 0.56682\n",
      "Epoch: 31/99 Iteration: 2544 Training loss: 0.40586\n",
      "Epoch: 31/99 Iteration: 2545 Training loss: 0.63591\n",
      "Epoch: 31/99 Iteration: 2546 Training loss: 0.60463\n",
      "Epoch: 31/99 Iteration: 2547 Training loss: 0.49589\n",
      "Epoch: 31/99 Iteration: 2548 Training loss: 0.39071\n",
      "Epoch: 31/99 Iteration: 2549 Training loss: 0.43661\n",
      "Epoch: 31/99 Iteration: 2550 Training loss: 0.53690\n",
      "***\n",
      "Epoch: 31/99 Iteration: 2550 Validation Acc: 0.8370\n",
      "***\n",
      "Epoch: 31/99 Iteration: 2551 Training loss: 0.47111\n",
      "Epoch: 31/99 Iteration: 2552 Training loss: 0.43723\n",
      "Epoch: 31/99 Iteration: 2553 Training loss: 0.39417\n",
      "Epoch: 31/99 Iteration: 2554 Training loss: 0.33681\n",
      "Epoch: 31/99 Iteration: 2555 Training loss: 0.38966\n",
      "Epoch: 31/99 Iteration: 2556 Training loss: 0.40380\n",
      "Epoch: 31/99 Iteration: 2557 Training loss: 0.55162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/99 Iteration: 2558 Training loss: 0.34399\n",
      "Epoch: 31/99 Iteration: 2559 Training loss: 0.45806\n",
      "Epoch: 32/99 Iteration: 2560 Training loss: 0.28452\n",
      "Epoch: 32/99 Iteration: 2561 Training loss: 0.60455\n",
      "Epoch: 32/99 Iteration: 2562 Training loss: 0.36643\n",
      "Epoch: 32/99 Iteration: 2563 Training loss: 0.58360\n",
      "Epoch: 32/99 Iteration: 2564 Training loss: 0.42949\n",
      "Epoch: 32/99 Iteration: 2565 Training loss: 0.35447\n",
      "Epoch: 32/99 Iteration: 2566 Training loss: 0.44659\n",
      "Epoch: 32/99 Iteration: 2567 Training loss: 0.33066\n",
      "Epoch: 32/99 Iteration: 2568 Training loss: 0.58117\n",
      "Epoch: 32/99 Iteration: 2569 Training loss: 0.31007\n",
      "Epoch: 32/99 Iteration: 2570 Training loss: 0.36773\n",
      "Epoch: 32/99 Iteration: 2571 Training loss: 0.42018\n",
      "Epoch: 32/99 Iteration: 2572 Training loss: 0.42804\n",
      "Epoch: 32/99 Iteration: 2573 Training loss: 0.46785\n",
      "Epoch: 32/99 Iteration: 2574 Training loss: 0.47353\n",
      "Epoch: 32/99 Iteration: 2575 Training loss: 0.35419\n",
      "Epoch: 32/99 Iteration: 2576 Training loss: 0.45191\n",
      "Epoch: 32/99 Iteration: 2577 Training loss: 0.36279\n",
      "Epoch: 32/99 Iteration: 2578 Training loss: 0.41924\n",
      "Epoch: 32/99 Iteration: 2579 Training loss: 0.42584\n",
      "Epoch: 32/99 Iteration: 2580 Training loss: 0.42493\n",
      "Epoch: 32/99 Iteration: 2581 Training loss: 0.50943\n",
      "Epoch: 32/99 Iteration: 2582 Training loss: 0.43181\n",
      "Epoch: 32/99 Iteration: 2583 Training loss: 0.52375\n",
      "Epoch: 32/99 Iteration: 2584 Training loss: 0.38113\n",
      "Epoch: 32/99 Iteration: 2585 Training loss: 0.40899\n",
      "Epoch: 32/99 Iteration: 2586 Training loss: 0.43946\n",
      "Epoch: 32/99 Iteration: 2587 Training loss: 0.30617\n",
      "Epoch: 32/99 Iteration: 2588 Training loss: 0.50036\n",
      "Epoch: 32/99 Iteration: 2589 Training loss: 0.62460\n",
      "Epoch: 32/99 Iteration: 2590 Training loss: 0.22159\n",
      "Epoch: 32/99 Iteration: 2591 Training loss: 0.31115\n",
      "Epoch: 32/99 Iteration: 2592 Training loss: 0.46025\n",
      "Epoch: 32/99 Iteration: 2593 Training loss: 0.43436\n",
      "Epoch: 32/99 Iteration: 2594 Training loss: 0.45335\n",
      "Epoch: 32/99 Iteration: 2595 Training loss: 0.41084\n",
      "Epoch: 32/99 Iteration: 2596 Training loss: 0.52901\n",
      "Epoch: 32/99 Iteration: 2597 Training loss: 0.30674\n",
      "Epoch: 32/99 Iteration: 2598 Training loss: 0.39417\n",
      "Epoch: 32/99 Iteration: 2599 Training loss: 0.41204\n",
      "Epoch: 32/99 Iteration: 2600 Training loss: 0.48204\n",
      "***\n",
      "Epoch: 32/99 Iteration: 2600 Validation Acc: 0.8390\n",
      "***\n",
      "Epoch: 32/99 Iteration: 2601 Training loss: 0.47205\n",
      "Epoch: 32/99 Iteration: 2602 Training loss: 0.51366\n",
      "Epoch: 32/99 Iteration: 2603 Training loss: 0.33811\n",
      "Epoch: 32/99 Iteration: 2604 Training loss: 0.52915\n",
      "Epoch: 32/99 Iteration: 2605 Training loss: 0.42922\n",
      "Epoch: 32/99 Iteration: 2606 Training loss: 0.58712\n",
      "Epoch: 32/99 Iteration: 2607 Training loss: 0.42558\n",
      "Epoch: 32/99 Iteration: 2608 Training loss: 0.64758\n",
      "Epoch: 32/99 Iteration: 2609 Training loss: 0.32788\n",
      "Epoch: 32/99 Iteration: 2610 Training loss: 0.33930\n",
      "Epoch: 32/99 Iteration: 2611 Training loss: 0.53608\n",
      "Epoch: 32/99 Iteration: 2612 Training loss: 0.36225\n",
      "Epoch: 32/99 Iteration: 2613 Training loss: 0.36171\n",
      "Epoch: 32/99 Iteration: 2614 Training loss: 0.48636\n",
      "Epoch: 32/99 Iteration: 2615 Training loss: 0.36940\n",
      "Epoch: 32/99 Iteration: 2616 Training loss: 0.44622\n",
      "Epoch: 32/99 Iteration: 2617 Training loss: 0.44502\n",
      "Epoch: 32/99 Iteration: 2618 Training loss: 0.52596\n",
      "Epoch: 32/99 Iteration: 2619 Training loss: 0.62868\n",
      "Epoch: 32/99 Iteration: 2620 Training loss: 0.56148\n",
      "Epoch: 32/99 Iteration: 2621 Training loss: 0.43225\n",
      "Epoch: 32/99 Iteration: 2622 Training loss: 0.44812\n",
      "Epoch: 32/99 Iteration: 2623 Training loss: 0.50465\n",
      "Epoch: 32/99 Iteration: 2624 Training loss: 0.55711\n",
      "Epoch: 32/99 Iteration: 2625 Training loss: 0.47827\n",
      "Epoch: 32/99 Iteration: 2626 Training loss: 0.54851\n",
      "Epoch: 32/99 Iteration: 2627 Training loss: 0.72725\n",
      "Epoch: 32/99 Iteration: 2628 Training loss: 0.53482\n",
      "Epoch: 32/99 Iteration: 2629 Training loss: 0.44442\n",
      "Epoch: 32/99 Iteration: 2630 Training loss: 0.40832\n",
      "Epoch: 32/99 Iteration: 2631 Training loss: 0.46644\n",
      "Epoch: 32/99 Iteration: 2632 Training loss: 0.43878\n",
      "Epoch: 32/99 Iteration: 2633 Training loss: 0.48055\n",
      "Epoch: 32/99 Iteration: 2634 Training loss: 0.53223\n",
      "Epoch: 32/99 Iteration: 2635 Training loss: 0.33897\n",
      "Epoch: 32/99 Iteration: 2636 Training loss: 0.44724\n",
      "Epoch: 32/99 Iteration: 2637 Training loss: 0.50765\n",
      "Epoch: 32/99 Iteration: 2638 Training loss: 0.52511\n",
      "Epoch: 32/99 Iteration: 2639 Training loss: 0.48822\n",
      "Epoch: 33/99 Iteration: 2640 Training loss: 0.35384\n",
      "Epoch: 33/99 Iteration: 2641 Training loss: 0.49281\n",
      "Epoch: 33/99 Iteration: 2642 Training loss: 0.53109\n",
      "Epoch: 33/99 Iteration: 2643 Training loss: 0.56527\n",
      "Epoch: 33/99 Iteration: 2644 Training loss: 0.53590\n",
      "Epoch: 33/99 Iteration: 2645 Training loss: 0.40642\n",
      "Epoch: 33/99 Iteration: 2646 Training loss: 0.63262\n",
      "Epoch: 33/99 Iteration: 2647 Training loss: 0.40093\n",
      "Epoch: 33/99 Iteration: 2648 Training loss: 0.52782\n",
      "Epoch: 33/99 Iteration: 2649 Training loss: 0.34786\n",
      "Epoch: 33/99 Iteration: 2650 Training loss: 0.36330\n",
      "***\n",
      "Epoch: 33/99 Iteration: 2650 Validation Acc: 0.8440\n",
      "***\n",
      "Epoch: 33/99 Iteration: 2651 Training loss: 0.34798\n",
      "Epoch: 33/99 Iteration: 2652 Training loss: 0.51902\n",
      "Epoch: 33/99 Iteration: 2653 Training loss: 0.55020\n",
      "Epoch: 33/99 Iteration: 2654 Training loss: 0.52046\n",
      "Epoch: 33/99 Iteration: 2655 Training loss: 0.41266\n",
      "Epoch: 33/99 Iteration: 2656 Training loss: 0.35542\n",
      "Epoch: 33/99 Iteration: 2657 Training loss: 0.34086\n",
      "Epoch: 33/99 Iteration: 2658 Training loss: 0.45942\n",
      "Epoch: 33/99 Iteration: 2659 Training loss: 0.34327\n",
      "Epoch: 33/99 Iteration: 2660 Training loss: 0.49476\n",
      "Epoch: 33/99 Iteration: 2661 Training loss: 0.43214\n",
      "Epoch: 33/99 Iteration: 2662 Training loss: 0.37386\n",
      "Epoch: 33/99 Iteration: 2663 Training loss: 0.48757\n",
      "Epoch: 33/99 Iteration: 2664 Training loss: 0.39405\n",
      "Epoch: 33/99 Iteration: 2665 Training loss: 0.47014\n",
      "Epoch: 33/99 Iteration: 2666 Training loss: 0.49624\n",
      "Epoch: 33/99 Iteration: 2667 Training loss: 0.35056\n",
      "Epoch: 33/99 Iteration: 2668 Training loss: 0.56594\n",
      "Epoch: 33/99 Iteration: 2669 Training loss: 0.57242\n",
      "Epoch: 33/99 Iteration: 2670 Training loss: 0.44710\n",
      "Epoch: 33/99 Iteration: 2671 Training loss: 0.43626\n",
      "Epoch: 33/99 Iteration: 2672 Training loss: 0.46717\n",
      "Epoch: 33/99 Iteration: 2673 Training loss: 0.38117\n",
      "Epoch: 33/99 Iteration: 2674 Training loss: 0.37941\n",
      "Epoch: 33/99 Iteration: 2675 Training loss: 0.55045\n",
      "Epoch: 33/99 Iteration: 2676 Training loss: 0.59561\n",
      "Epoch: 33/99 Iteration: 2677 Training loss: 0.34664\n",
      "Epoch: 33/99 Iteration: 2678 Training loss: 0.41193\n",
      "Epoch: 33/99 Iteration: 2679 Training loss: 0.54513\n",
      "Epoch: 33/99 Iteration: 2680 Training loss: 0.37822\n",
      "Epoch: 33/99 Iteration: 2681 Training loss: 0.44551\n",
      "Epoch: 33/99 Iteration: 2682 Training loss: 0.50403\n",
      "Epoch: 33/99 Iteration: 2683 Training loss: 0.42566\n",
      "Epoch: 33/99 Iteration: 2684 Training loss: 0.40838\n",
      "Epoch: 33/99 Iteration: 2685 Training loss: 0.39405\n",
      "Epoch: 33/99 Iteration: 2686 Training loss: 0.58054\n",
      "Epoch: 33/99 Iteration: 2687 Training loss: 0.46118\n",
      "Epoch: 33/99 Iteration: 2688 Training loss: 0.88234\n",
      "Epoch: 33/99 Iteration: 2689 Training loss: 0.42952\n",
      "Epoch: 33/99 Iteration: 2690 Training loss: 0.48595\n",
      "Epoch: 33/99 Iteration: 2691 Training loss: 0.38295\n",
      "Epoch: 33/99 Iteration: 2692 Training loss: 0.57541\n",
      "Epoch: 33/99 Iteration: 2693 Training loss: 0.53401\n",
      "Epoch: 33/99 Iteration: 2694 Training loss: 0.54610\n",
      "Epoch: 33/99 Iteration: 2695 Training loss: 0.42269\n",
      "Epoch: 33/99 Iteration: 2696 Training loss: 0.48328\n",
      "Epoch: 33/99 Iteration: 2697 Training loss: 0.45635\n",
      "Epoch: 33/99 Iteration: 2698 Training loss: 0.58483\n",
      "Epoch: 33/99 Iteration: 2699 Training loss: 0.56231\n",
      "Epoch: 33/99 Iteration: 2700 Training loss: 0.38168\n",
      "***\n",
      "Epoch: 33/99 Iteration: 2700 Validation Acc: 0.8300\n",
      "***\n",
      "Epoch: 33/99 Iteration: 2701 Training loss: 0.50031\n",
      "Epoch: 33/99 Iteration: 2702 Training loss: 0.45106\n",
      "Epoch: 33/99 Iteration: 2703 Training loss: 0.41672\n",
      "Epoch: 33/99 Iteration: 2704 Training loss: 0.42593\n",
      "Epoch: 33/99 Iteration: 2705 Training loss: 0.56069\n",
      "Epoch: 33/99 Iteration: 2706 Training loss: 0.57041\n",
      "Epoch: 33/99 Iteration: 2707 Training loss: 0.49984\n",
      "Epoch: 33/99 Iteration: 2708 Training loss: 0.44351\n",
      "Epoch: 33/99 Iteration: 2709 Training loss: 0.42449\n",
      "Epoch: 33/99 Iteration: 2710 Training loss: 0.46834\n",
      "Epoch: 33/99 Iteration: 2711 Training loss: 0.47449\n",
      "Epoch: 33/99 Iteration: 2712 Training loss: 0.53485\n",
      "Epoch: 33/99 Iteration: 2713 Training loss: 0.32313\n",
      "Epoch: 33/99 Iteration: 2714 Training loss: 0.49585\n",
      "Epoch: 33/99 Iteration: 2715 Training loss: 0.33401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/99 Iteration: 2716 Training loss: 0.41401\n",
      "Epoch: 33/99 Iteration: 2717 Training loss: 0.54734\n",
      "Epoch: 33/99 Iteration: 2718 Training loss: 0.48635\n",
      "Epoch: 33/99 Iteration: 2719 Training loss: 0.52584\n",
      "Epoch: 34/99 Iteration: 2720 Training loss: 0.34428\n",
      "Epoch: 34/99 Iteration: 2721 Training loss: 0.48808\n",
      "Epoch: 34/99 Iteration: 2722 Training loss: 0.59069\n",
      "Epoch: 34/99 Iteration: 2723 Training loss: 0.48004\n",
      "Epoch: 34/99 Iteration: 2724 Training loss: 0.42680\n",
      "Epoch: 34/99 Iteration: 2725 Training loss: 0.41263\n",
      "Epoch: 34/99 Iteration: 2726 Training loss: 0.43023\n",
      "Epoch: 34/99 Iteration: 2727 Training loss: 0.41234\n",
      "Epoch: 34/99 Iteration: 2728 Training loss: 0.61973\n",
      "Epoch: 34/99 Iteration: 2729 Training loss: 0.41141\n",
      "Epoch: 34/99 Iteration: 2730 Training loss: 0.31734\n",
      "Epoch: 34/99 Iteration: 2731 Training loss: 0.39474\n",
      "Epoch: 34/99 Iteration: 2732 Training loss: 0.34180\n",
      "Epoch: 34/99 Iteration: 2733 Training loss: 0.47184\n",
      "Epoch: 34/99 Iteration: 2734 Training loss: 0.52185\n",
      "Epoch: 34/99 Iteration: 2735 Training loss: 0.41926\n",
      "Epoch: 34/99 Iteration: 2736 Training loss: 0.34501\n",
      "Epoch: 34/99 Iteration: 2737 Training loss: 0.40110\n",
      "Epoch: 34/99 Iteration: 2738 Training loss: 0.40592\n",
      "Epoch: 34/99 Iteration: 2739 Training loss: 0.38263\n",
      "Epoch: 34/99 Iteration: 2740 Training loss: 0.30984\n",
      "Epoch: 34/99 Iteration: 2741 Training loss: 0.40704\n",
      "Epoch: 34/99 Iteration: 2742 Training loss: 0.53198\n",
      "Epoch: 34/99 Iteration: 2743 Training loss: 0.46017\n",
      "Epoch: 34/99 Iteration: 2744 Training loss: 0.39729\n",
      "Epoch: 34/99 Iteration: 2745 Training loss: 0.52376\n",
      "Epoch: 34/99 Iteration: 2746 Training loss: 0.54328\n",
      "Epoch: 34/99 Iteration: 2747 Training loss: 0.33218\n",
      "Epoch: 34/99 Iteration: 2748 Training loss: 0.38939\n",
      "Epoch: 34/99 Iteration: 2749 Training loss: 0.47825\n",
      "Epoch: 34/99 Iteration: 2750 Training loss: 0.34409\n",
      "***\n",
      "Epoch: 34/99 Iteration: 2750 Validation Acc: 0.8290\n",
      "***\n",
      "Epoch: 34/99 Iteration: 2751 Training loss: 0.43324\n",
      "Epoch: 34/99 Iteration: 2752 Training loss: 0.46623\n",
      "Epoch: 34/99 Iteration: 2753 Training loss: 0.39273\n",
      "Epoch: 34/99 Iteration: 2754 Training loss: 0.48237\n",
      "Epoch: 34/99 Iteration: 2755 Training loss: 0.38268\n",
      "Epoch: 34/99 Iteration: 2756 Training loss: 0.49980\n",
      "Epoch: 34/99 Iteration: 2757 Training loss: 0.39679\n",
      "Epoch: 34/99 Iteration: 2758 Training loss: 0.37967\n",
      "Epoch: 34/99 Iteration: 2759 Training loss: 0.40885\n",
      "Epoch: 34/99 Iteration: 2760 Training loss: 0.61532\n",
      "Epoch: 34/99 Iteration: 2761 Training loss: 0.31689\n",
      "Epoch: 34/99 Iteration: 2762 Training loss: 0.50708\n",
      "Epoch: 34/99 Iteration: 2763 Training loss: 0.43349\n",
      "Epoch: 34/99 Iteration: 2764 Training loss: 0.52991\n",
      "Epoch: 34/99 Iteration: 2765 Training loss: 0.41043\n",
      "Epoch: 34/99 Iteration: 2766 Training loss: 0.53815\n",
      "Epoch: 34/99 Iteration: 2767 Training loss: 0.53764\n",
      "Epoch: 34/99 Iteration: 2768 Training loss: 0.49825\n",
      "Epoch: 34/99 Iteration: 2769 Training loss: 0.34945\n",
      "Epoch: 34/99 Iteration: 2770 Training loss: 0.45610\n",
      "Epoch: 34/99 Iteration: 2771 Training loss: 0.45710\n",
      "Epoch: 34/99 Iteration: 2772 Training loss: 0.54056\n",
      "Epoch: 34/99 Iteration: 2773 Training loss: 0.56347\n",
      "Epoch: 34/99 Iteration: 2774 Training loss: 0.54816\n",
      "Epoch: 34/99 Iteration: 2775 Training loss: 0.51782\n",
      "Epoch: 34/99 Iteration: 2776 Training loss: 0.44734\n",
      "Epoch: 34/99 Iteration: 2777 Training loss: 0.53153\n",
      "Epoch: 34/99 Iteration: 2778 Training loss: 0.53024\n",
      "Epoch: 34/99 Iteration: 2779 Training loss: 0.53704\n",
      "Epoch: 34/99 Iteration: 2780 Training loss: 0.53407\n",
      "Epoch: 34/99 Iteration: 2781 Training loss: 0.51670\n",
      "Epoch: 34/99 Iteration: 2782 Training loss: 0.41339\n",
      "Epoch: 34/99 Iteration: 2783 Training loss: 0.57895\n",
      "Epoch: 34/99 Iteration: 2784 Training loss: 0.45086\n",
      "Epoch: 34/99 Iteration: 2785 Training loss: 0.59446\n",
      "Epoch: 34/99 Iteration: 2786 Training loss: 0.51366\n",
      "Epoch: 34/99 Iteration: 2787 Training loss: 0.60456\n",
      "Epoch: 34/99 Iteration: 2788 Training loss: 0.44182\n",
      "Epoch: 34/99 Iteration: 2789 Training loss: 0.45992\n",
      "Epoch: 34/99 Iteration: 2790 Training loss: 0.59790\n",
      "Epoch: 34/99 Iteration: 2791 Training loss: 0.43360\n",
      "Epoch: 34/99 Iteration: 2792 Training loss: 0.46771\n",
      "Epoch: 34/99 Iteration: 2793 Training loss: 0.43932\n",
      "Epoch: 34/99 Iteration: 2794 Training loss: 0.45465\n",
      "Epoch: 34/99 Iteration: 2795 Training loss: 0.36711\n",
      "Epoch: 34/99 Iteration: 2796 Training loss: 0.39687\n",
      "Epoch: 34/99 Iteration: 2797 Training loss: 0.56035\n",
      "Epoch: 34/99 Iteration: 2798 Training loss: 0.52196\n",
      "Epoch: 34/99 Iteration: 2799 Training loss: 0.46742\n",
      "Epoch: 35/99 Iteration: 2800 Training loss: 0.44317\n",
      "***\n",
      "Epoch: 35/99 Iteration: 2800 Validation Acc: 0.8380\n",
      "***\n",
      "Epoch: 35/99 Iteration: 2801 Training loss: 0.42125\n",
      "Epoch: 35/99 Iteration: 2802 Training loss: 0.50375\n",
      "Epoch: 35/99 Iteration: 2803 Training loss: 0.50210\n",
      "Epoch: 35/99 Iteration: 2804 Training loss: 0.37565\n",
      "Epoch: 35/99 Iteration: 2805 Training loss: 0.48029\n",
      "Epoch: 35/99 Iteration: 2806 Training loss: 0.40953\n",
      "Epoch: 35/99 Iteration: 2807 Training loss: 0.40072\n",
      "Epoch: 35/99 Iteration: 2808 Training loss: 0.53446\n",
      "Epoch: 35/99 Iteration: 2809 Training loss: 0.43298\n",
      "Epoch: 35/99 Iteration: 2810 Training loss: 0.46332\n",
      "Epoch: 35/99 Iteration: 2811 Training loss: 0.39256\n",
      "Epoch: 35/99 Iteration: 2812 Training loss: 0.35591\n",
      "Epoch: 35/99 Iteration: 2813 Training loss: 0.44601\n",
      "Epoch: 35/99 Iteration: 2814 Training loss: 0.48163\n",
      "Epoch: 35/99 Iteration: 2815 Training loss: 0.42006\n",
      "Epoch: 35/99 Iteration: 2816 Training loss: 0.29872\n",
      "Epoch: 35/99 Iteration: 2817 Training loss: 0.33112\n",
      "Epoch: 35/99 Iteration: 2818 Training loss: 0.47665\n",
      "Epoch: 35/99 Iteration: 2819 Training loss: 0.48900\n",
      "Epoch: 35/99 Iteration: 2820 Training loss: 0.53174\n",
      "Epoch: 35/99 Iteration: 2821 Training loss: 0.32526\n",
      "Epoch: 35/99 Iteration: 2822 Training loss: 0.31330\n",
      "Epoch: 35/99 Iteration: 2823 Training loss: 0.36429\n",
      "Epoch: 35/99 Iteration: 2824 Training loss: 0.37235\n",
      "Epoch: 35/99 Iteration: 2825 Training loss: 0.55233\n",
      "Epoch: 35/99 Iteration: 2826 Training loss: 0.43203\n",
      "Epoch: 35/99 Iteration: 2827 Training loss: 0.24275\n",
      "Epoch: 35/99 Iteration: 2828 Training loss: 0.54827\n",
      "Epoch: 35/99 Iteration: 2829 Training loss: 0.45921\n",
      "Epoch: 35/99 Iteration: 2830 Training loss: 0.39057\n",
      "Epoch: 35/99 Iteration: 2831 Training loss: 0.29294\n",
      "Epoch: 35/99 Iteration: 2832 Training loss: 0.42246\n",
      "Epoch: 35/99 Iteration: 2833 Training loss: 0.43657\n",
      "Epoch: 35/99 Iteration: 2834 Training loss: 0.41825\n",
      "Epoch: 35/99 Iteration: 2835 Training loss: 0.32245\n",
      "Epoch: 35/99 Iteration: 2836 Training loss: 0.56623\n",
      "Epoch: 35/99 Iteration: 2837 Training loss: 0.38842\n",
      "Epoch: 35/99 Iteration: 2838 Training loss: 0.36860\n",
      "Epoch: 35/99 Iteration: 2839 Training loss: 0.44874\n",
      "Epoch: 35/99 Iteration: 2840 Training loss: 0.55371\n",
      "Epoch: 35/99 Iteration: 2841 Training loss: 0.43971\n",
      "Epoch: 35/99 Iteration: 2842 Training loss: 0.38945\n",
      "Epoch: 35/99 Iteration: 2843 Training loss: 0.30544\n",
      "Epoch: 35/99 Iteration: 2844 Training loss: 0.43720\n",
      "Epoch: 35/99 Iteration: 2845 Training loss: 0.45415\n",
      "Epoch: 35/99 Iteration: 2846 Training loss: 0.55769\n",
      "Epoch: 35/99 Iteration: 2847 Training loss: 0.35857\n",
      "Epoch: 35/99 Iteration: 2848 Training loss: 0.47398\n",
      "Epoch: 35/99 Iteration: 2849 Training loss: 0.44488\n",
      "Epoch: 35/99 Iteration: 2850 Training loss: 0.32275\n",
      "***\n",
      "Epoch: 35/99 Iteration: 2850 Validation Acc: 0.8280\n",
      "***\n",
      "Epoch: 35/99 Iteration: 2851 Training loss: 0.40573\n",
      "Epoch: 35/99 Iteration: 2852 Training loss: 0.49230\n",
      "Epoch: 35/99 Iteration: 2853 Training loss: 0.44582\n",
      "Epoch: 35/99 Iteration: 2854 Training loss: 0.51375\n",
      "Epoch: 35/99 Iteration: 2855 Training loss: 0.44556\n",
      "Epoch: 35/99 Iteration: 2856 Training loss: 0.37348\n",
      "Epoch: 35/99 Iteration: 2857 Training loss: 0.43438\n",
      "Epoch: 35/99 Iteration: 2858 Training loss: 0.47052\n",
      "Epoch: 35/99 Iteration: 2859 Training loss: 0.46279\n",
      "Epoch: 35/99 Iteration: 2860 Training loss: 0.37168\n",
      "Epoch: 35/99 Iteration: 2861 Training loss: 0.55236\n",
      "Epoch: 35/99 Iteration: 2862 Training loss: 0.50485\n",
      "Epoch: 35/99 Iteration: 2863 Training loss: 0.49261\n",
      "Epoch: 35/99 Iteration: 2864 Training loss: 0.48368\n",
      "Epoch: 35/99 Iteration: 2865 Training loss: 0.58751\n",
      "Epoch: 35/99 Iteration: 2866 Training loss: 0.47668\n",
      "Epoch: 35/99 Iteration: 2867 Training loss: 0.49206\n",
      "Epoch: 35/99 Iteration: 2868 Training loss: 0.44537\n",
      "Epoch: 35/99 Iteration: 2869 Training loss: 0.59430\n",
      "Epoch: 35/99 Iteration: 2870 Training loss: 0.59735\n",
      "Epoch: 35/99 Iteration: 2871 Training loss: 0.39345\n",
      "Epoch: 35/99 Iteration: 2872 Training loss: 0.41978\n",
      "Epoch: 35/99 Iteration: 2873 Training loss: 0.39971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/99 Iteration: 2874 Training loss: 0.40975\n",
      "Epoch: 35/99 Iteration: 2875 Training loss: 0.48766\n",
      "Epoch: 35/99 Iteration: 2876 Training loss: 0.41270\n",
      "Epoch: 35/99 Iteration: 2877 Training loss: 0.33027\n",
      "Epoch: 35/99 Iteration: 2878 Training loss: 0.34413\n",
      "Epoch: 35/99 Iteration: 2879 Training loss: 0.49267\n",
      "Epoch: 36/99 Iteration: 2880 Training loss: 0.45697\n",
      "Epoch: 36/99 Iteration: 2881 Training loss: 0.45347\n",
      "Epoch: 36/99 Iteration: 2882 Training loss: 0.48523\n",
      "Epoch: 36/99 Iteration: 2883 Training loss: 0.56578\n",
      "Epoch: 36/99 Iteration: 2884 Training loss: 0.48530\n",
      "Epoch: 36/99 Iteration: 2885 Training loss: 0.46178\n",
      "Epoch: 36/99 Iteration: 2886 Training loss: 0.46946\n",
      "Epoch: 36/99 Iteration: 2887 Training loss: 0.43938\n",
      "Epoch: 36/99 Iteration: 2888 Training loss: 0.55337\n",
      "Epoch: 36/99 Iteration: 2889 Training loss: 0.43906\n",
      "Epoch: 36/99 Iteration: 2890 Training loss: 0.45503\n",
      "Epoch: 36/99 Iteration: 2891 Training loss: 0.41852\n",
      "Epoch: 36/99 Iteration: 2892 Training loss: 0.43348\n",
      "Epoch: 36/99 Iteration: 2893 Training loss: 0.46904\n",
      "Epoch: 36/99 Iteration: 2894 Training loss: 0.67824\n",
      "Epoch: 36/99 Iteration: 2895 Training loss: 0.32482\n",
      "Epoch: 36/99 Iteration: 2896 Training loss: 0.22770\n",
      "Epoch: 36/99 Iteration: 2897 Training loss: 0.34576\n",
      "Epoch: 36/99 Iteration: 2898 Training loss: 0.49587\n",
      "Epoch: 36/99 Iteration: 2899 Training loss: 0.44900\n",
      "Epoch: 36/99 Iteration: 2900 Training loss: 0.41509\n",
      "***\n",
      "Epoch: 36/99 Iteration: 2900 Validation Acc: 0.8400\n",
      "***\n",
      "Epoch: 36/99 Iteration: 2901 Training loss: 0.39889\n",
      "Epoch: 36/99 Iteration: 2902 Training loss: 0.44796\n",
      "Epoch: 36/99 Iteration: 2903 Training loss: 0.37297\n",
      "Epoch: 36/99 Iteration: 2904 Training loss: 0.30893\n",
      "Epoch: 36/99 Iteration: 2905 Training loss: 0.48940\n",
      "Epoch: 36/99 Iteration: 2906 Training loss: 0.47602\n",
      "Epoch: 36/99 Iteration: 2907 Training loss: 0.32058\n",
      "Epoch: 36/99 Iteration: 2908 Training loss: 0.49863\n",
      "Epoch: 36/99 Iteration: 2909 Training loss: 0.46199\n",
      "Epoch: 36/99 Iteration: 2910 Training loss: 0.33079\n",
      "Epoch: 36/99 Iteration: 2911 Training loss: 0.44323\n",
      "Epoch: 36/99 Iteration: 2912 Training loss: 0.39480\n",
      "Epoch: 36/99 Iteration: 2913 Training loss: 0.43588\n",
      "Epoch: 36/99 Iteration: 2914 Training loss: 0.41993\n",
      "Epoch: 36/99 Iteration: 2915 Training loss: 0.33597\n",
      "Epoch: 36/99 Iteration: 2916 Training loss: 0.46820\n",
      "Epoch: 36/99 Iteration: 2917 Training loss: 0.38789\n",
      "Epoch: 36/99 Iteration: 2918 Training loss: 0.34171\n",
      "Epoch: 36/99 Iteration: 2919 Training loss: 0.26096\n",
      "Epoch: 36/99 Iteration: 2920 Training loss: 0.29639\n",
      "Epoch: 36/99 Iteration: 2921 Training loss: 0.43081\n",
      "Epoch: 36/99 Iteration: 2922 Training loss: 0.47828\n",
      "Epoch: 36/99 Iteration: 2923 Training loss: 0.26075\n",
      "Epoch: 36/99 Iteration: 2924 Training loss: 0.47979\n",
      "Epoch: 36/99 Iteration: 2925 Training loss: 0.37503\n",
      "Epoch: 36/99 Iteration: 2926 Training loss: 0.43741\n",
      "Epoch: 36/99 Iteration: 2927 Training loss: 0.41936\n",
      "Epoch: 36/99 Iteration: 2928 Training loss: 0.50593\n",
      "Epoch: 36/99 Iteration: 2929 Training loss: 0.36887\n",
      "Epoch: 36/99 Iteration: 2930 Training loss: 0.39241\n",
      "Epoch: 36/99 Iteration: 2931 Training loss: 0.43701\n",
      "Epoch: 36/99 Iteration: 2932 Training loss: 0.47747\n",
      "Epoch: 36/99 Iteration: 2933 Training loss: 0.48912\n",
      "Epoch: 36/99 Iteration: 2934 Training loss: 0.57261\n",
      "Epoch: 36/99 Iteration: 2935 Training loss: 0.35185\n",
      "Epoch: 36/99 Iteration: 2936 Training loss: 0.38993\n",
      "Epoch: 36/99 Iteration: 2937 Training loss: 0.46968\n",
      "Epoch: 36/99 Iteration: 2938 Training loss: 0.52027\n",
      "Epoch: 36/99 Iteration: 2939 Training loss: 0.59763\n",
      "Epoch: 36/99 Iteration: 2940 Training loss: 0.50818\n",
      "Epoch: 36/99 Iteration: 2941 Training loss: 0.40982\n",
      "Epoch: 36/99 Iteration: 2942 Training loss: 0.39190\n",
      "Epoch: 36/99 Iteration: 2943 Training loss: 0.42044\n",
      "Epoch: 36/99 Iteration: 2944 Training loss: 0.46345\n",
      "Epoch: 36/99 Iteration: 2945 Training loss: 0.48718\n",
      "Epoch: 36/99 Iteration: 2946 Training loss: 0.57733\n",
      "Epoch: 36/99 Iteration: 2947 Training loss: 0.48114\n",
      "Epoch: 36/99 Iteration: 2948 Training loss: 0.45421\n",
      "Epoch: 36/99 Iteration: 2949 Training loss: 0.37219\n",
      "Epoch: 36/99 Iteration: 2950 Training loss: 0.53603\n",
      "***\n",
      "Epoch: 36/99 Iteration: 2950 Validation Acc: 0.8350\n",
      "***\n",
      "Epoch: 36/99 Iteration: 2951 Training loss: 0.35012\n",
      "Epoch: 36/99 Iteration: 2952 Training loss: 0.40893\n",
      "Epoch: 36/99 Iteration: 2953 Training loss: 0.40919\n",
      "Epoch: 36/99 Iteration: 2954 Training loss: 0.50333\n",
      "Epoch: 36/99 Iteration: 2955 Training loss: 0.37845\n",
      "Epoch: 36/99 Iteration: 2956 Training loss: 0.41894\n",
      "Epoch: 36/99 Iteration: 2957 Training loss: 0.44390\n",
      "Epoch: 36/99 Iteration: 2958 Training loss: 0.39993\n",
      "Epoch: 36/99 Iteration: 2959 Training loss: 0.53335\n",
      "Epoch: 37/99 Iteration: 2960 Training loss: 0.32398\n",
      "Epoch: 37/99 Iteration: 2961 Training loss: 0.42839\n",
      "Epoch: 37/99 Iteration: 2962 Training loss: 0.38997\n",
      "Epoch: 37/99 Iteration: 2963 Training loss: 0.43163\n",
      "Epoch: 37/99 Iteration: 2964 Training loss: 0.45210\n",
      "Epoch: 37/99 Iteration: 2965 Training loss: 0.47264\n",
      "Epoch: 37/99 Iteration: 2966 Training loss: 0.41546\n",
      "Epoch: 37/99 Iteration: 2967 Training loss: 0.43083\n",
      "Epoch: 37/99 Iteration: 2968 Training loss: 0.65930\n",
      "Epoch: 37/99 Iteration: 2969 Training loss: 0.47983\n",
      "Epoch: 37/99 Iteration: 2970 Training loss: 0.36355\n",
      "Epoch: 37/99 Iteration: 2971 Training loss: 0.38429\n",
      "Epoch: 37/99 Iteration: 2972 Training loss: 0.51431\n",
      "Epoch: 37/99 Iteration: 2973 Training loss: 0.57591\n",
      "Epoch: 37/99 Iteration: 2974 Training loss: 0.54646\n",
      "Epoch: 37/99 Iteration: 2975 Training loss: 0.39205\n",
      "Epoch: 37/99 Iteration: 2976 Training loss: 0.37340\n",
      "Epoch: 37/99 Iteration: 2977 Training loss: 0.31793\n",
      "Epoch: 37/99 Iteration: 2978 Training loss: 0.48507\n",
      "Epoch: 37/99 Iteration: 2979 Training loss: 0.50539\n",
      "Epoch: 37/99 Iteration: 2980 Training loss: 0.42734\n",
      "Epoch: 37/99 Iteration: 2981 Training loss: 0.38958\n",
      "Epoch: 37/99 Iteration: 2982 Training loss: 0.50496\n",
      "Epoch: 37/99 Iteration: 2983 Training loss: 0.52215\n",
      "Epoch: 37/99 Iteration: 2984 Training loss: 0.48525\n",
      "Epoch: 37/99 Iteration: 2985 Training loss: 0.47408\n",
      "Epoch: 37/99 Iteration: 2986 Training loss: 0.43982\n",
      "Epoch: 37/99 Iteration: 2987 Training loss: 0.35594\n",
      "Epoch: 37/99 Iteration: 2988 Training loss: 0.44057\n",
      "Epoch: 37/99 Iteration: 2989 Training loss: 0.53976\n",
      "Epoch: 37/99 Iteration: 2990 Training loss: 0.36439\n",
      "Epoch: 37/99 Iteration: 2991 Training loss: 0.35940\n",
      "Epoch: 37/99 Iteration: 2992 Training loss: 0.36200\n",
      "Epoch: 37/99 Iteration: 2993 Training loss: 0.55635\n",
      "Epoch: 37/99 Iteration: 2994 Training loss: 0.47445\n",
      "Epoch: 37/99 Iteration: 2995 Training loss: 0.40609\n",
      "Epoch: 37/99 Iteration: 2996 Training loss: 0.66039\n",
      "Epoch: 37/99 Iteration: 2997 Training loss: 0.40484\n",
      "Epoch: 37/99 Iteration: 2998 Training loss: 0.38567\n",
      "Epoch: 37/99 Iteration: 2999 Training loss: 0.40741\n",
      "Epoch: 37/99 Iteration: 3000 Training loss: 0.60270\n",
      "***\n",
      "Epoch: 37/99 Iteration: 3000 Validation Acc: 0.8260\n",
      "***\n",
      "Epoch: 37/99 Iteration: 3001 Training loss: 0.55294\n",
      "Epoch: 37/99 Iteration: 3002 Training loss: 0.40753\n",
      "Epoch: 37/99 Iteration: 3003 Training loss: 0.39961\n",
      "Epoch: 37/99 Iteration: 3004 Training loss: 0.47497\n",
      "Epoch: 37/99 Iteration: 3005 Training loss: 0.39754\n",
      "Epoch: 37/99 Iteration: 3006 Training loss: 0.53578\n",
      "Epoch: 37/99 Iteration: 3007 Training loss: 0.52626\n",
      "Epoch: 37/99 Iteration: 3008 Training loss: 0.53703\n",
      "Epoch: 37/99 Iteration: 3009 Training loss: 0.32349\n",
      "Epoch: 37/99 Iteration: 3010 Training loss: 0.42772\n",
      "Epoch: 37/99 Iteration: 3011 Training loss: 0.50972\n",
      "Epoch: 37/99 Iteration: 3012 Training loss: 0.44205\n",
      "Epoch: 37/99 Iteration: 3013 Training loss: 0.49268\n",
      "Epoch: 37/99 Iteration: 3014 Training loss: 0.69413\n",
      "Epoch: 37/99 Iteration: 3015 Training loss: 0.31547\n",
      "Epoch: 37/99 Iteration: 3016 Training loss: 0.44180\n",
      "Epoch: 37/99 Iteration: 3017 Training loss: 0.44631\n",
      "Epoch: 37/99 Iteration: 3018 Training loss: 0.54555\n",
      "Epoch: 37/99 Iteration: 3019 Training loss: 0.48235\n",
      "Epoch: 37/99 Iteration: 3020 Training loss: 0.53103\n",
      "Epoch: 37/99 Iteration: 3021 Training loss: 0.56784\n",
      "Epoch: 37/99 Iteration: 3022 Training loss: 0.51902\n",
      "Epoch: 37/99 Iteration: 3023 Training loss: 0.54317\n",
      "Epoch: 37/99 Iteration: 3024 Training loss: 0.52676\n",
      "Epoch: 37/99 Iteration: 3025 Training loss: 0.66642\n",
      "Epoch: 37/99 Iteration: 3026 Training loss: 0.60461\n",
      "Epoch: 37/99 Iteration: 3027 Training loss: 0.46917\n",
      "Epoch: 37/99 Iteration: 3028 Training loss: 0.43549\n",
      "Epoch: 37/99 Iteration: 3029 Training loss: 0.56192\n",
      "Epoch: 37/99 Iteration: 3030 Training loss: 0.67134\n",
      "Epoch: 37/99 Iteration: 3031 Training loss: 0.41967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/99 Iteration: 3032 Training loss: 0.42080\n",
      "Epoch: 37/99 Iteration: 3033 Training loss: 0.43277\n",
      "Epoch: 37/99 Iteration: 3034 Training loss: 0.58551\n",
      "Epoch: 37/99 Iteration: 3035 Training loss: 0.45154\n",
      "Epoch: 37/99 Iteration: 3036 Training loss: 0.34215\n",
      "Epoch: 37/99 Iteration: 3037 Training loss: 0.52507\n",
      "Epoch: 37/99 Iteration: 3038 Training loss: 0.52308\n",
      "Epoch: 37/99 Iteration: 3039 Training loss: 0.41164\n",
      "Epoch: 38/99 Iteration: 3040 Training loss: 0.36248\n",
      "Epoch: 38/99 Iteration: 3041 Training loss: 0.61856\n",
      "Epoch: 38/99 Iteration: 3042 Training loss: 0.60286\n",
      "Epoch: 38/99 Iteration: 3043 Training loss: 0.39827\n",
      "Epoch: 38/99 Iteration: 3044 Training loss: 0.36905\n",
      "Epoch: 38/99 Iteration: 3045 Training loss: 0.37942\n",
      "Epoch: 38/99 Iteration: 3046 Training loss: 0.39882\n",
      "Epoch: 38/99 Iteration: 3047 Training loss: 0.47083\n",
      "Epoch: 38/99 Iteration: 3048 Training loss: 0.65031\n",
      "Epoch: 38/99 Iteration: 3049 Training loss: 0.35463\n",
      "Epoch: 38/99 Iteration: 3050 Training loss: 0.51803\n",
      "***\n",
      "Epoch: 38/99 Iteration: 3050 Validation Acc: 0.8400\n",
      "***\n",
      "Epoch: 38/99 Iteration: 3051 Training loss: 0.38746\n",
      "Epoch: 38/99 Iteration: 3052 Training loss: 0.52553\n",
      "Epoch: 38/99 Iteration: 3053 Training loss: 0.50874\n",
      "Epoch: 38/99 Iteration: 3054 Training loss: 0.56150\n",
      "Epoch: 38/99 Iteration: 3055 Training loss: 0.40848\n",
      "Epoch: 38/99 Iteration: 3056 Training loss: 0.36994\n",
      "Epoch: 38/99 Iteration: 3057 Training loss: 0.28346\n",
      "Epoch: 38/99 Iteration: 3058 Training loss: 0.50531\n",
      "Epoch: 38/99 Iteration: 3059 Training loss: 0.45633\n",
      "Epoch: 38/99 Iteration: 3060 Training loss: 0.29523\n",
      "Epoch: 38/99 Iteration: 3061 Training loss: 0.37473\n",
      "Epoch: 38/99 Iteration: 3062 Training loss: 0.47317\n",
      "Epoch: 38/99 Iteration: 3063 Training loss: 0.41051\n",
      "Epoch: 38/99 Iteration: 3064 Training loss: 0.35250\n",
      "Epoch: 38/99 Iteration: 3065 Training loss: 0.57168\n",
      "Epoch: 38/99 Iteration: 3066 Training loss: 0.53902\n",
      "Epoch: 38/99 Iteration: 3067 Training loss: 0.43095\n",
      "Epoch: 38/99 Iteration: 3068 Training loss: 0.53910\n",
      "Epoch: 38/99 Iteration: 3069 Training loss: 0.55871\n",
      "Epoch: 38/99 Iteration: 3070 Training loss: 0.39072\n",
      "Epoch: 38/99 Iteration: 3071 Training loss: 0.39650\n",
      "Epoch: 38/99 Iteration: 3072 Training loss: 0.44848\n",
      "Epoch: 38/99 Iteration: 3073 Training loss: 0.42728\n",
      "Epoch: 38/99 Iteration: 3074 Training loss: 0.36139\n",
      "Epoch: 38/99 Iteration: 3075 Training loss: 0.36398\n",
      "Epoch: 38/99 Iteration: 3076 Training loss: 0.73971\n",
      "Epoch: 38/99 Iteration: 3077 Training loss: 0.40807\n",
      "Epoch: 38/99 Iteration: 3078 Training loss: 0.42168\n",
      "Epoch: 38/99 Iteration: 3079 Training loss: 0.43792\n",
      "Epoch: 38/99 Iteration: 3080 Training loss: 0.47743\n",
      "Epoch: 38/99 Iteration: 3081 Training loss: 0.41524\n",
      "Epoch: 38/99 Iteration: 3082 Training loss: 0.54767\n",
      "Epoch: 38/99 Iteration: 3083 Training loss: 0.45748\n",
      "Epoch: 38/99 Iteration: 3084 Training loss: 0.44381\n",
      "Epoch: 38/99 Iteration: 3085 Training loss: 0.39573\n",
      "Epoch: 38/99 Iteration: 3086 Training loss: 0.49507\n",
      "Epoch: 38/99 Iteration: 3087 Training loss: 0.49997\n",
      "Epoch: 38/99 Iteration: 3088 Training loss: 0.70227\n",
      "Epoch: 38/99 Iteration: 3089 Training loss: 0.37724\n",
      "Epoch: 38/99 Iteration: 3090 Training loss: 0.40466\n",
      "Epoch: 38/99 Iteration: 3091 Training loss: 0.46307\n",
      "Epoch: 38/99 Iteration: 3092 Training loss: 0.38684\n",
      "Epoch: 38/99 Iteration: 3093 Training loss: 0.39877\n",
      "Epoch: 38/99 Iteration: 3094 Training loss: 0.49417\n",
      "Epoch: 38/99 Iteration: 3095 Training loss: 0.42738\n",
      "Epoch: 38/99 Iteration: 3096 Training loss: 0.29461\n",
      "Epoch: 38/99 Iteration: 3097 Training loss: 0.35329\n",
      "Epoch: 38/99 Iteration: 3098 Training loss: 0.51654\n",
      "Epoch: 38/99 Iteration: 3099 Training loss: 0.59909\n",
      "Epoch: 38/99 Iteration: 3100 Training loss: 0.42618\n",
      "***\n",
      "Epoch: 38/99 Iteration: 3100 Validation Acc: 0.8520\n",
      "***\n",
      "Epoch: 38/99 Iteration: 3101 Training loss: 0.46422\n",
      "Epoch: 38/99 Iteration: 3102 Training loss: 0.40888\n",
      "Epoch: 38/99 Iteration: 3103 Training loss: 0.48961\n",
      "Epoch: 38/99 Iteration: 3104 Training loss: 0.50293\n",
      "Epoch: 38/99 Iteration: 3105 Training loss: 0.66399\n",
      "Epoch: 38/99 Iteration: 3106 Training loss: 0.43943\n",
      "Epoch: 38/99 Iteration: 3107 Training loss: 0.50540\n",
      "Epoch: 38/99 Iteration: 3108 Training loss: 0.38389\n",
      "Epoch: 38/99 Iteration: 3109 Training loss: 0.43115\n",
      "Epoch: 38/99 Iteration: 3110 Training loss: 0.60510\n",
      "Epoch: 38/99 Iteration: 3111 Training loss: 0.39509\n",
      "Epoch: 38/99 Iteration: 3112 Training loss: 0.51561\n",
      "Epoch: 38/99 Iteration: 3113 Training loss: 0.33610\n",
      "Epoch: 38/99 Iteration: 3114 Training loss: 0.44169\n",
      "Epoch: 38/99 Iteration: 3115 Training loss: 0.34438\n",
      "Epoch: 38/99 Iteration: 3116 Training loss: 0.37943\n",
      "Epoch: 38/99 Iteration: 3117 Training loss: 0.51726\n",
      "Epoch: 38/99 Iteration: 3118 Training loss: 0.49102\n",
      "Epoch: 38/99 Iteration: 3119 Training loss: 0.50440\n",
      "Epoch: 39/99 Iteration: 3120 Training loss: 0.24875\n",
      "Epoch: 39/99 Iteration: 3121 Training loss: 0.54963\n",
      "Epoch: 39/99 Iteration: 3122 Training loss: 0.40879\n",
      "Epoch: 39/99 Iteration: 3123 Training loss: 0.47732\n",
      "Epoch: 39/99 Iteration: 3124 Training loss: 0.41803\n",
      "Epoch: 39/99 Iteration: 3125 Training loss: 0.32487\n",
      "Epoch: 39/99 Iteration: 3126 Training loss: 0.41055\n",
      "Epoch: 39/99 Iteration: 3127 Training loss: 0.38795\n",
      "Epoch: 39/99 Iteration: 3128 Training loss: 0.56404\n",
      "Epoch: 39/99 Iteration: 3129 Training loss: 0.32857\n",
      "Epoch: 39/99 Iteration: 3130 Training loss: 0.33258\n",
      "Epoch: 39/99 Iteration: 3131 Training loss: 0.35225\n",
      "Epoch: 39/99 Iteration: 3132 Training loss: 0.38887\n",
      "Epoch: 39/99 Iteration: 3133 Training loss: 0.45287\n",
      "Epoch: 39/99 Iteration: 3134 Training loss: 0.47547\n",
      "Epoch: 39/99 Iteration: 3135 Training loss: 0.37621\n",
      "Epoch: 39/99 Iteration: 3136 Training loss: 0.37503\n",
      "Epoch: 39/99 Iteration: 3137 Training loss: 0.32179\n",
      "Epoch: 39/99 Iteration: 3138 Training loss: 0.50059\n",
      "Epoch: 39/99 Iteration: 3139 Training loss: 0.30880\n",
      "Epoch: 39/99 Iteration: 3140 Training loss: 0.42886\n",
      "Epoch: 39/99 Iteration: 3141 Training loss: 0.41051\n",
      "Epoch: 39/99 Iteration: 3142 Training loss: 0.35919\n",
      "Epoch: 39/99 Iteration: 3143 Training loss: 0.36610\n",
      "Epoch: 39/99 Iteration: 3144 Training loss: 0.27683\n",
      "Epoch: 39/99 Iteration: 3145 Training loss: 0.48889\n",
      "Epoch: 39/99 Iteration: 3146 Training loss: 0.34022\n",
      "Epoch: 39/99 Iteration: 3147 Training loss: 0.28592\n",
      "Epoch: 39/99 Iteration: 3148 Training loss: 0.36220\n",
      "Epoch: 39/99 Iteration: 3149 Training loss: 0.40470\n",
      "Epoch: 39/99 Iteration: 3150 Training loss: 0.34584\n",
      "***\n",
      "Epoch: 39/99 Iteration: 3150 Validation Acc: 0.8450\n",
      "***\n",
      "Epoch: 39/99 Iteration: 3151 Training loss: 0.25213\n",
      "Epoch: 39/99 Iteration: 3152 Training loss: 0.38086\n",
      "Epoch: 39/99 Iteration: 3153 Training loss: 0.34208\n",
      "Epoch: 39/99 Iteration: 3154 Training loss: 0.23036\n",
      "Epoch: 39/99 Iteration: 3155 Training loss: 0.46132\n",
      "Epoch: 39/99 Iteration: 3156 Training loss: 0.55064\n",
      "Epoch: 39/99 Iteration: 3157 Training loss: 0.44815\n",
      "Epoch: 39/99 Iteration: 3158 Training loss: 0.55225\n",
      "Epoch: 39/99 Iteration: 3159 Training loss: 0.38728\n",
      "Epoch: 39/99 Iteration: 3160 Training loss: 0.41610\n",
      "Epoch: 39/99 Iteration: 3161 Training loss: 0.43032\n",
      "Epoch: 39/99 Iteration: 3162 Training loss: 0.43297\n",
      "Epoch: 39/99 Iteration: 3163 Training loss: 0.41903\n",
      "Epoch: 39/99 Iteration: 3164 Training loss: 0.53345\n",
      "Epoch: 39/99 Iteration: 3165 Training loss: 0.32707\n",
      "Epoch: 39/99 Iteration: 3166 Training loss: 0.43688\n",
      "Epoch: 39/99 Iteration: 3167 Training loss: 0.46159\n",
      "Epoch: 39/99 Iteration: 3168 Training loss: 0.65179\n",
      "Epoch: 39/99 Iteration: 3169 Training loss: 0.28589\n",
      "Epoch: 39/99 Iteration: 3170 Training loss: 0.33930\n",
      "Epoch: 39/99 Iteration: 3171 Training loss: 0.35481\n",
      "Epoch: 39/99 Iteration: 3172 Training loss: 0.59312\n",
      "Epoch: 39/99 Iteration: 3173 Training loss: 0.52795\n",
      "Epoch: 39/99 Iteration: 3174 Training loss: 0.45014\n",
      "Epoch: 39/99 Iteration: 3175 Training loss: 0.38445\n",
      "Epoch: 39/99 Iteration: 3176 Training loss: 0.38619\n",
      "Epoch: 39/99 Iteration: 3177 Training loss: 0.55271\n",
      "Epoch: 39/99 Iteration: 3178 Training loss: 0.39504\n",
      "Epoch: 39/99 Iteration: 3179 Training loss: 0.49067\n",
      "Epoch: 39/99 Iteration: 3180 Training loss: 0.43335\n",
      "Epoch: 39/99 Iteration: 3181 Training loss: 0.57682\n",
      "Epoch: 39/99 Iteration: 3182 Training loss: 0.42208\n",
      "Epoch: 39/99 Iteration: 3183 Training loss: 0.36811\n",
      "Epoch: 39/99 Iteration: 3184 Training loss: 0.45794\n",
      "Epoch: 39/99 Iteration: 3185 Training loss: 0.59969\n",
      "Epoch: 39/99 Iteration: 3186 Training loss: 0.41021\n",
      "Epoch: 39/99 Iteration: 3187 Training loss: 0.51839\n",
      "Epoch: 39/99 Iteration: 3188 Training loss: 0.47611\n",
      "Epoch: 39/99 Iteration: 3189 Training loss: 0.36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/99 Iteration: 3190 Training loss: 0.48586\n",
      "Epoch: 39/99 Iteration: 3191 Training loss: 0.33228\n",
      "Epoch: 39/99 Iteration: 3192 Training loss: 0.51713\n",
      "Epoch: 39/99 Iteration: 3193 Training loss: 0.45389\n",
      "Epoch: 39/99 Iteration: 3194 Training loss: 0.48976\n",
      "Epoch: 39/99 Iteration: 3195 Training loss: 0.48981\n",
      "Epoch: 39/99 Iteration: 3196 Training loss: 0.39076\n",
      "Epoch: 39/99 Iteration: 3197 Training loss: 0.49049\n",
      "Epoch: 39/99 Iteration: 3198 Training loss: 0.50761\n",
      "Epoch: 39/99 Iteration: 3199 Training loss: 0.41206\n",
      "Epoch: 40/99 Iteration: 3200 Training loss: 0.23749\n",
      "***\n",
      "Epoch: 40/99 Iteration: 3200 Validation Acc: 0.8390\n",
      "***\n",
      "Epoch: 40/99 Iteration: 3201 Training loss: 0.46763\n",
      "Epoch: 40/99 Iteration: 3202 Training loss: 0.47268\n",
      "Epoch: 40/99 Iteration: 3203 Training loss: 0.54140\n",
      "Epoch: 40/99 Iteration: 3204 Training loss: 0.33030\n",
      "Epoch: 40/99 Iteration: 3205 Training loss: 0.38933\n",
      "Epoch: 40/99 Iteration: 3206 Training loss: 0.33356\n",
      "Epoch: 40/99 Iteration: 3207 Training loss: 0.39883\n",
      "Epoch: 40/99 Iteration: 3208 Training loss: 0.52790\n",
      "Epoch: 40/99 Iteration: 3209 Training loss: 0.32212\n",
      "Epoch: 40/99 Iteration: 3210 Training loss: 0.35507\n",
      "Epoch: 40/99 Iteration: 3211 Training loss: 0.39131\n",
      "Epoch: 40/99 Iteration: 3212 Training loss: 0.52354\n",
      "Epoch: 40/99 Iteration: 3213 Training loss: 0.54749\n",
      "Epoch: 40/99 Iteration: 3214 Training loss: 0.58992\n",
      "Epoch: 40/99 Iteration: 3215 Training loss: 0.27492\n",
      "Epoch: 40/99 Iteration: 3216 Training loss: 0.39662\n",
      "Epoch: 40/99 Iteration: 3217 Training loss: 0.42729\n",
      "Epoch: 40/99 Iteration: 3218 Training loss: 0.41054\n",
      "Epoch: 40/99 Iteration: 3219 Training loss: 0.33289\n",
      "Epoch: 40/99 Iteration: 3220 Training loss: 0.35321\n",
      "Epoch: 40/99 Iteration: 3221 Training loss: 0.36291\n",
      "Epoch: 40/99 Iteration: 3222 Training loss: 0.41366\n",
      "Epoch: 40/99 Iteration: 3223 Training loss: 0.48227\n",
      "Epoch: 40/99 Iteration: 3224 Training loss: 0.42621\n",
      "Epoch: 40/99 Iteration: 3225 Training loss: 0.61281\n",
      "Epoch: 40/99 Iteration: 3226 Training loss: 0.43205\n",
      "Epoch: 40/99 Iteration: 3227 Training loss: 0.39797\n",
      "Epoch: 40/99 Iteration: 3228 Training loss: 0.45571\n",
      "Epoch: 40/99 Iteration: 3229 Training loss: 0.67088\n",
      "Epoch: 40/99 Iteration: 3230 Training loss: 0.30239\n",
      "Epoch: 40/99 Iteration: 3231 Training loss: 0.49507\n",
      "Epoch: 40/99 Iteration: 3232 Training loss: 0.51549\n",
      "Epoch: 40/99 Iteration: 3233 Training loss: 0.40932\n",
      "Epoch: 40/99 Iteration: 3234 Training loss: 0.45472\n",
      "Epoch: 40/99 Iteration: 3235 Training loss: 0.43933\n",
      "Epoch: 40/99 Iteration: 3236 Training loss: 0.66122\n",
      "Epoch: 40/99 Iteration: 3237 Training loss: 0.44102\n",
      "Epoch: 40/99 Iteration: 3238 Training loss: 0.39169\n",
      "Epoch: 40/99 Iteration: 3239 Training loss: 0.36635\n",
      "Epoch: 40/99 Iteration: 3240 Training loss: 0.43435\n",
      "Epoch: 40/99 Iteration: 3241 Training loss: 0.43826\n",
      "Epoch: 40/99 Iteration: 3242 Training loss: 0.44146\n",
      "Epoch: 40/99 Iteration: 3243 Training loss: 0.39635\n",
      "Epoch: 40/99 Iteration: 3244 Training loss: 0.48333\n",
      "Epoch: 40/99 Iteration: 3245 Training loss: 0.30495\n",
      "Epoch: 40/99 Iteration: 3246 Training loss: 0.45863\n",
      "Epoch: 40/99 Iteration: 3247 Training loss: 0.38164\n",
      "Epoch: 40/99 Iteration: 3248 Training loss: 0.62467\n",
      "Epoch: 40/99 Iteration: 3249 Training loss: 0.34613\n",
      "Epoch: 40/99 Iteration: 3250 Training loss: 0.39346\n",
      "***\n",
      "Epoch: 40/99 Iteration: 3250 Validation Acc: 0.8400\n",
      "***\n",
      "Epoch: 40/99 Iteration: 3251 Training loss: 0.38514\n",
      "Epoch: 40/99 Iteration: 3252 Training loss: 0.46105\n",
      "Epoch: 40/99 Iteration: 3253 Training loss: 0.41813\n",
      "Epoch: 40/99 Iteration: 3254 Training loss: 0.61915\n",
      "Epoch: 40/99 Iteration: 3255 Training loss: 0.51135\n",
      "Epoch: 40/99 Iteration: 3256 Training loss: 0.52610\n",
      "Epoch: 40/99 Iteration: 3257 Training loss: 0.47069\n",
      "Epoch: 40/99 Iteration: 3258 Training loss: 0.43772\n",
      "Epoch: 40/99 Iteration: 3259 Training loss: 0.70074\n",
      "Epoch: 40/99 Iteration: 3260 Training loss: 0.41798\n",
      "Epoch: 40/99 Iteration: 3261 Training loss: 0.44956\n",
      "Epoch: 40/99 Iteration: 3262 Training loss: 0.30883\n",
      "Epoch: 40/99 Iteration: 3263 Training loss: 0.40972\n",
      "Epoch: 40/99 Iteration: 3264 Training loss: 0.46790\n",
      "Epoch: 40/99 Iteration: 3265 Training loss: 0.49849\n",
      "Epoch: 40/99 Iteration: 3266 Training loss: 0.39475\n",
      "Epoch: 40/99 Iteration: 3267 Training loss: 0.49695\n",
      "Epoch: 40/99 Iteration: 3268 Training loss: 0.42390\n",
      "Epoch: 40/99 Iteration: 3269 Training loss: 0.41320\n",
      "Epoch: 40/99 Iteration: 3270 Training loss: 0.53540\n",
      "Epoch: 40/99 Iteration: 3271 Training loss: 0.40109\n",
      "Epoch: 40/99 Iteration: 3272 Training loss: 0.37905\n",
      "Epoch: 40/99 Iteration: 3273 Training loss: 0.38517\n",
      "Epoch: 40/99 Iteration: 3274 Training loss: 0.50328\n",
      "Epoch: 40/99 Iteration: 3275 Training loss: 0.33519\n",
      "Epoch: 40/99 Iteration: 3276 Training loss: 0.36538\n",
      "Epoch: 40/99 Iteration: 3277 Training loss: 0.46350\n",
      "Epoch: 40/99 Iteration: 3278 Training loss: 0.39641\n",
      "Epoch: 40/99 Iteration: 3279 Training loss: 0.44046\n",
      "Epoch: 41/99 Iteration: 3280 Training loss: 0.26601\n",
      "Epoch: 41/99 Iteration: 3281 Training loss: 0.52658\n",
      "Epoch: 41/99 Iteration: 3282 Training loss: 0.43421\n",
      "Epoch: 41/99 Iteration: 3283 Training loss: 0.48857\n",
      "Epoch: 41/99 Iteration: 3284 Training loss: 0.41227\n",
      "Epoch: 41/99 Iteration: 3285 Training loss: 0.38617\n",
      "Epoch: 41/99 Iteration: 3286 Training loss: 0.48177\n",
      "Epoch: 41/99 Iteration: 3287 Training loss: 0.39686\n",
      "Epoch: 41/99 Iteration: 3288 Training loss: 0.51206\n",
      "Epoch: 41/99 Iteration: 3289 Training loss: 0.35683\n",
      "Epoch: 41/99 Iteration: 3290 Training loss: 0.45754\n",
      "Epoch: 41/99 Iteration: 3291 Training loss: 0.39929\n",
      "Epoch: 41/99 Iteration: 3292 Training loss: 0.36494\n",
      "Epoch: 41/99 Iteration: 3293 Training loss: 0.51113\n",
      "Epoch: 41/99 Iteration: 3294 Training loss: 0.51396\n",
      "Epoch: 41/99 Iteration: 3295 Training loss: 0.37575\n",
      "Epoch: 41/99 Iteration: 3296 Training loss: 0.42065\n",
      "Epoch: 41/99 Iteration: 3297 Training loss: 0.30940\n",
      "Epoch: 41/99 Iteration: 3298 Training loss: 0.52853\n",
      "Epoch: 41/99 Iteration: 3299 Training loss: 0.39534\n",
      "Epoch: 41/99 Iteration: 3300 Training loss: 0.43162\n",
      "***\n",
      "Epoch: 41/99 Iteration: 3300 Validation Acc: 0.8670\n",
      "***\n",
      "Epoch: 41/99 Iteration: 3301 Training loss: 0.36971\n",
      "Epoch: 41/99 Iteration: 3302 Training loss: 0.51376\n",
      "Epoch: 41/99 Iteration: 3303 Training loss: 0.36646\n",
      "Epoch: 41/99 Iteration: 3304 Training loss: 0.31006\n",
      "Epoch: 41/99 Iteration: 3305 Training loss: 0.39685\n",
      "Epoch: 41/99 Iteration: 3306 Training loss: 0.38715\n",
      "Epoch: 41/99 Iteration: 3307 Training loss: 0.35700\n",
      "Epoch: 41/99 Iteration: 3308 Training loss: 0.42301\n",
      "Epoch: 41/99 Iteration: 3309 Training loss: 0.39136\n",
      "Epoch: 41/99 Iteration: 3310 Training loss: 0.32954\n",
      "Epoch: 41/99 Iteration: 3311 Training loss: 0.41172\n",
      "Epoch: 41/99 Iteration: 3312 Training loss: 0.49387\n",
      "Epoch: 41/99 Iteration: 3313 Training loss: 0.36549\n",
      "Epoch: 41/99 Iteration: 3314 Training loss: 0.39308\n",
      "Epoch: 41/99 Iteration: 3315 Training loss: 0.36279\n",
      "Epoch: 41/99 Iteration: 3316 Training loss: 0.62973\n",
      "Epoch: 41/99 Iteration: 3317 Training loss: 0.38920\n",
      "Epoch: 41/99 Iteration: 3318 Training loss: 0.34228\n",
      "Epoch: 41/99 Iteration: 3319 Training loss: 0.35103\n",
      "Epoch: 41/99 Iteration: 3320 Training loss: 0.41957\n",
      "Epoch: 41/99 Iteration: 3321 Training loss: 0.45092\n",
      "Epoch: 41/99 Iteration: 3322 Training loss: 0.36267\n",
      "Epoch: 41/99 Iteration: 3323 Training loss: 0.47270\n",
      "Epoch: 41/99 Iteration: 3324 Training loss: 0.50866\n",
      "Epoch: 41/99 Iteration: 3325 Training loss: 0.35471\n",
      "Epoch: 41/99 Iteration: 3326 Training loss: 0.49562\n",
      "Epoch: 41/99 Iteration: 3327 Training loss: 0.33004\n",
      "Epoch: 41/99 Iteration: 3328 Training loss: 0.54928\n",
      "Epoch: 41/99 Iteration: 3329 Training loss: 0.28760\n",
      "Epoch: 41/99 Iteration: 3330 Training loss: 0.46029\n",
      "Epoch: 41/99 Iteration: 3331 Training loss: 0.38121\n",
      "Epoch: 41/99 Iteration: 3332 Training loss: 0.30740\n",
      "Epoch: 41/99 Iteration: 3333 Training loss: 0.50566\n",
      "Epoch: 41/99 Iteration: 3334 Training loss: 0.43923\n",
      "Epoch: 41/99 Iteration: 3335 Training loss: 0.34999\n",
      "Epoch: 41/99 Iteration: 3336 Training loss: 0.46349\n",
      "Epoch: 41/99 Iteration: 3337 Training loss: 0.41219\n",
      "Epoch: 41/99 Iteration: 3338 Training loss: 0.37740\n",
      "Epoch: 41/99 Iteration: 3339 Training loss: 0.55887\n",
      "Epoch: 41/99 Iteration: 3340 Training loss: 0.39376\n",
      "Epoch: 41/99 Iteration: 3341 Training loss: 0.40521\n",
      "Epoch: 41/99 Iteration: 3342 Training loss: 0.41547\n",
      "Epoch: 41/99 Iteration: 3343 Training loss: 0.38222\n",
      "Epoch: 41/99 Iteration: 3344 Training loss: 0.45692\n",
      "Epoch: 41/99 Iteration: 3345 Training loss: 0.51996\n",
      "Epoch: 41/99 Iteration: 3346 Training loss: 0.58958\n",
      "Epoch: 41/99 Iteration: 3347 Training loss: 0.57586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/99 Iteration: 3348 Training loss: 0.41650\n",
      "Epoch: 41/99 Iteration: 3349 Training loss: 0.39449\n",
      "Epoch: 41/99 Iteration: 3350 Training loss: 0.59476\n",
      "***\n",
      "Epoch: 41/99 Iteration: 3350 Validation Acc: 0.8320\n",
      "***\n",
      "Epoch: 41/99 Iteration: 3351 Training loss: 0.50478\n",
      "Epoch: 41/99 Iteration: 3352 Training loss: 0.42971\n",
      "Epoch: 41/99 Iteration: 3353 Training loss: 0.37956\n",
      "Epoch: 41/99 Iteration: 3354 Training loss: 0.48399\n",
      "Epoch: 41/99 Iteration: 3355 Training loss: 0.44242\n",
      "Epoch: 41/99 Iteration: 3356 Training loss: 0.45281\n",
      "Epoch: 41/99 Iteration: 3357 Training loss: 0.39749\n",
      "Epoch: 41/99 Iteration: 3358 Training loss: 0.45454\n",
      "Epoch: 41/99 Iteration: 3359 Training loss: 0.45275\n",
      "Epoch: 42/99 Iteration: 3360 Training loss: 0.39474\n",
      "Epoch: 42/99 Iteration: 3361 Training loss: 0.49308\n",
      "Epoch: 42/99 Iteration: 3362 Training loss: 0.47329\n",
      "Epoch: 42/99 Iteration: 3363 Training loss: 0.46561\n",
      "Epoch: 42/99 Iteration: 3364 Training loss: 0.35154\n",
      "Epoch: 42/99 Iteration: 3365 Training loss: 0.48971\n",
      "Epoch: 42/99 Iteration: 3366 Training loss: 0.51784\n",
      "Epoch: 42/99 Iteration: 3367 Training loss: 0.40261\n",
      "Epoch: 42/99 Iteration: 3368 Training loss: 0.69025\n",
      "Epoch: 42/99 Iteration: 3369 Training loss: 0.39216\n",
      "Epoch: 42/99 Iteration: 3370 Training loss: 0.30629\n",
      "Epoch: 42/99 Iteration: 3371 Training loss: 0.36971\n",
      "Epoch: 42/99 Iteration: 3372 Training loss: 0.32475\n",
      "Epoch: 42/99 Iteration: 3373 Training loss: 0.45509\n",
      "Epoch: 42/99 Iteration: 3374 Training loss: 0.44433\n",
      "Epoch: 42/99 Iteration: 3375 Training loss: 0.28468\n",
      "Epoch: 42/99 Iteration: 3376 Training loss: 0.32407\n",
      "Epoch: 42/99 Iteration: 3377 Training loss: 0.34011\n",
      "Epoch: 42/99 Iteration: 3378 Training loss: 0.39020\n",
      "Epoch: 42/99 Iteration: 3379 Training loss: 0.48761\n",
      "Epoch: 42/99 Iteration: 3380 Training loss: 0.38576\n",
      "Epoch: 42/99 Iteration: 3381 Training loss: 0.34589\n",
      "Epoch: 42/99 Iteration: 3382 Training loss: 0.48869\n",
      "Epoch: 42/99 Iteration: 3383 Training loss: 0.28331\n",
      "Epoch: 42/99 Iteration: 3384 Training loss: 0.50159\n",
      "Epoch: 42/99 Iteration: 3385 Training loss: 0.41967\n",
      "Epoch: 42/99 Iteration: 3386 Training loss: 0.36012\n",
      "Epoch: 42/99 Iteration: 3387 Training loss: 0.32269\n",
      "Epoch: 42/99 Iteration: 3388 Training loss: 0.42599\n",
      "Epoch: 42/99 Iteration: 3389 Training loss: 0.60676\n",
      "Epoch: 42/99 Iteration: 3390 Training loss: 0.35480\n",
      "Epoch: 42/99 Iteration: 3391 Training loss: 0.27306\n",
      "Epoch: 42/99 Iteration: 3392 Training loss: 0.43027\n",
      "Epoch: 42/99 Iteration: 3393 Training loss: 0.35441\n",
      "Epoch: 42/99 Iteration: 3394 Training loss: 0.40342\n",
      "Epoch: 42/99 Iteration: 3395 Training loss: 0.46350\n",
      "Epoch: 42/99 Iteration: 3396 Training loss: 0.47721\n",
      "Epoch: 42/99 Iteration: 3397 Training loss: 0.39015\n",
      "Epoch: 42/99 Iteration: 3398 Training loss: 0.31089\n",
      "Epoch: 42/99 Iteration: 3399 Training loss: 0.31979\n",
      "Epoch: 42/99 Iteration: 3400 Training loss: 0.47691\n",
      "***\n",
      "Epoch: 42/99 Iteration: 3400 Validation Acc: 0.8480\n",
      "***\n",
      "Epoch: 42/99 Iteration: 3401 Training loss: 0.39160\n",
      "Epoch: 42/99 Iteration: 3402 Training loss: 0.47480\n",
      "Epoch: 42/99 Iteration: 3403 Training loss: 0.34285\n",
      "Epoch: 42/99 Iteration: 3404 Training loss: 0.55739\n",
      "Epoch: 42/99 Iteration: 3405 Training loss: 0.40304\n",
      "Epoch: 42/99 Iteration: 3406 Training loss: 0.59054\n",
      "Epoch: 42/99 Iteration: 3407 Training loss: 0.36226\n",
      "Epoch: 42/99 Iteration: 3408 Training loss: 0.60488\n",
      "Epoch: 42/99 Iteration: 3409 Training loss: 0.34593\n",
      "Epoch: 42/99 Iteration: 3410 Training loss: 0.46814\n",
      "Epoch: 42/99 Iteration: 3411 Training loss: 0.49239\n",
      "Epoch: 42/99 Iteration: 3412 Training loss: 0.44789\n",
      "Epoch: 42/99 Iteration: 3413 Training loss: 0.46668\n",
      "Epoch: 42/99 Iteration: 3414 Training loss: 0.50118\n",
      "Epoch: 42/99 Iteration: 3415 Training loss: 0.35136\n",
      "Epoch: 42/99 Iteration: 3416 Training loss: 0.39943\n",
      "Epoch: 42/99 Iteration: 3417 Training loss: 0.48560\n",
      "Epoch: 42/99 Iteration: 3418 Training loss: 0.53905\n",
      "Epoch: 42/99 Iteration: 3419 Training loss: 0.63908\n",
      "Epoch: 42/99 Iteration: 3420 Training loss: 0.47654\n",
      "Epoch: 42/99 Iteration: 3421 Training loss: 0.38069\n",
      "Epoch: 42/99 Iteration: 3422 Training loss: 0.35451\n",
      "Epoch: 42/99 Iteration: 3423 Training loss: 0.55547\n",
      "Epoch: 42/99 Iteration: 3424 Training loss: 0.52357\n",
      "Epoch: 42/99 Iteration: 3425 Training loss: 0.48777\n",
      "Epoch: 42/99 Iteration: 3426 Training loss: 0.43096\n",
      "Epoch: 42/99 Iteration: 3427 Training loss: 0.48266\n",
      "Epoch: 42/99 Iteration: 3428 Training loss: 0.44755\n",
      "Epoch: 42/99 Iteration: 3429 Training loss: 0.43643\n",
      "Epoch: 42/99 Iteration: 3430 Training loss: 0.58136\n",
      "Epoch: 42/99 Iteration: 3431 Training loss: 0.35452\n",
      "Epoch: 42/99 Iteration: 3432 Training loss: 0.40398\n",
      "Epoch: 42/99 Iteration: 3433 Training loss: 0.31318\n",
      "Epoch: 42/99 Iteration: 3434 Training loss: 0.46626\n",
      "Epoch: 42/99 Iteration: 3435 Training loss: 0.35155\n",
      "Epoch: 42/99 Iteration: 3436 Training loss: 0.52427\n",
      "Epoch: 42/99 Iteration: 3437 Training loss: 0.46295\n",
      "Epoch: 42/99 Iteration: 3438 Training loss: 0.47814\n",
      "Epoch: 42/99 Iteration: 3439 Training loss: 0.36304\n",
      "Epoch: 43/99 Iteration: 3440 Training loss: 0.30209\n",
      "Epoch: 43/99 Iteration: 3441 Training loss: 0.45798\n",
      "Epoch: 43/99 Iteration: 3442 Training loss: 0.56907\n",
      "Epoch: 43/99 Iteration: 3443 Training loss: 0.58612\n",
      "Epoch: 43/99 Iteration: 3444 Training loss: 0.39500\n",
      "Epoch: 43/99 Iteration: 3445 Training loss: 0.42414\n",
      "Epoch: 43/99 Iteration: 3446 Training loss: 0.39423\n",
      "Epoch: 43/99 Iteration: 3447 Training loss: 0.38003\n",
      "Epoch: 43/99 Iteration: 3448 Training loss: 0.61115\n",
      "Epoch: 43/99 Iteration: 3449 Training loss: 0.36702\n",
      "Epoch: 43/99 Iteration: 3450 Training loss: 0.39800\n",
      "***\n",
      "Epoch: 43/99 Iteration: 3450 Validation Acc: 0.8340\n",
      "***\n",
      "Epoch: 43/99 Iteration: 3451 Training loss: 0.42965\n",
      "Epoch: 43/99 Iteration: 3452 Training loss: 0.33944\n",
      "Epoch: 43/99 Iteration: 3453 Training loss: 0.43492\n",
      "Epoch: 43/99 Iteration: 3454 Training loss: 0.40856\n",
      "Epoch: 43/99 Iteration: 3455 Training loss: 0.37762\n",
      "Epoch: 43/99 Iteration: 3456 Training loss: 0.24341\n",
      "Epoch: 43/99 Iteration: 3457 Training loss: 0.36282\n",
      "Epoch: 43/99 Iteration: 3458 Training loss: 0.34913\n",
      "Epoch: 43/99 Iteration: 3459 Training loss: 0.36738\n",
      "Epoch: 43/99 Iteration: 3460 Training loss: 0.24959\n",
      "Epoch: 43/99 Iteration: 3461 Training loss: 0.25458\n",
      "Epoch: 43/99 Iteration: 3462 Training loss: 0.47170\n",
      "Epoch: 43/99 Iteration: 3463 Training loss: 0.44290\n",
      "Epoch: 43/99 Iteration: 3464 Training loss: 0.37502\n",
      "Epoch: 43/99 Iteration: 3465 Training loss: 0.46802\n",
      "Epoch: 43/99 Iteration: 3466 Training loss: 0.33648\n",
      "Epoch: 43/99 Iteration: 3467 Training loss: 0.28333\n",
      "Epoch: 43/99 Iteration: 3468 Training loss: 0.40741\n",
      "Epoch: 43/99 Iteration: 3469 Training loss: 0.68976\n",
      "Epoch: 43/99 Iteration: 3470 Training loss: 0.31995\n",
      "Epoch: 43/99 Iteration: 3471 Training loss: 0.36469\n",
      "Epoch: 43/99 Iteration: 3472 Training loss: 0.60321\n",
      "Epoch: 43/99 Iteration: 3473 Training loss: 0.55345\n",
      "Epoch: 43/99 Iteration: 3474 Training loss: 0.47310\n",
      "Epoch: 43/99 Iteration: 3475 Training loss: 0.38172\n",
      "Epoch: 43/99 Iteration: 3476 Training loss: 0.68787\n",
      "Epoch: 43/99 Iteration: 3477 Training loss: 0.44361\n",
      "Epoch: 43/99 Iteration: 3478 Training loss: 0.44522\n",
      "Epoch: 43/99 Iteration: 3479 Training loss: 0.45963\n",
      "Epoch: 43/99 Iteration: 3480 Training loss: 0.55655\n",
      "Epoch: 43/99 Iteration: 3481 Training loss: 0.37809\n",
      "Epoch: 43/99 Iteration: 3482 Training loss: 0.45802\n",
      "Epoch: 43/99 Iteration: 3483 Training loss: 0.36736\n",
      "Epoch: 43/99 Iteration: 3484 Training loss: 0.43000\n",
      "Epoch: 43/99 Iteration: 3485 Training loss: 0.42562\n",
      "Epoch: 43/99 Iteration: 3486 Training loss: 0.41451\n",
      "Epoch: 43/99 Iteration: 3487 Training loss: 0.38843\n",
      "Epoch: 43/99 Iteration: 3488 Training loss: 0.57117\n",
      "Epoch: 43/99 Iteration: 3489 Training loss: 0.46823\n",
      "Epoch: 43/99 Iteration: 3490 Training loss: 0.49370\n",
      "Epoch: 43/99 Iteration: 3491 Training loss: 0.47223\n",
      "Epoch: 43/99 Iteration: 3492 Training loss: 0.38673\n",
      "Epoch: 43/99 Iteration: 3493 Training loss: 0.42298\n",
      "Epoch: 43/99 Iteration: 3494 Training loss: 0.39949\n",
      "Epoch: 43/99 Iteration: 3495 Training loss: 0.43591\n",
      "Epoch: 43/99 Iteration: 3496 Training loss: 0.47531\n",
      "Epoch: 43/99 Iteration: 3497 Training loss: 0.45230\n",
      "Epoch: 43/99 Iteration: 3498 Training loss: 0.46591\n",
      "Epoch: 43/99 Iteration: 3499 Training loss: 0.42923\n",
      "Epoch: 43/99 Iteration: 3500 Training loss: 0.37877\n",
      "***\n",
      "Epoch: 43/99 Iteration: 3500 Validation Acc: 0.8440\n",
      "***\n",
      "Epoch: 43/99 Iteration: 3501 Training loss: 0.42440\n",
      "Epoch: 43/99 Iteration: 3502 Training loss: 0.31873\n",
      "Epoch: 43/99 Iteration: 3503 Training loss: 0.43564\n",
      "Epoch: 43/99 Iteration: 3504 Training loss: 0.49074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/99 Iteration: 3505 Training loss: 0.39042\n",
      "Epoch: 43/99 Iteration: 3506 Training loss: 0.51989\n",
      "Epoch: 43/99 Iteration: 3507 Training loss: 0.52571\n",
      "Epoch: 43/99 Iteration: 3508 Training loss: 0.39199\n",
      "Epoch: 43/99 Iteration: 3509 Training loss: 0.36420\n",
      "Epoch: 43/99 Iteration: 3510 Training loss: 0.50753\n",
      "Epoch: 43/99 Iteration: 3511 Training loss: 0.40366\n",
      "Epoch: 43/99 Iteration: 3512 Training loss: 0.34827\n",
      "Epoch: 43/99 Iteration: 3513 Training loss: 0.35596\n",
      "Epoch: 43/99 Iteration: 3514 Training loss: 0.55575\n",
      "Epoch: 43/99 Iteration: 3515 Training loss: 0.32969\n",
      "Epoch: 43/99 Iteration: 3516 Training loss: 0.41020\n",
      "Epoch: 43/99 Iteration: 3517 Training loss: 0.55295\n",
      "Epoch: 43/99 Iteration: 3518 Training loss: 0.45736\n",
      "Epoch: 43/99 Iteration: 3519 Training loss: 0.46039\n",
      "Epoch: 44/99 Iteration: 3520 Training loss: 0.29331\n",
      "Epoch: 44/99 Iteration: 3521 Training loss: 0.54023\n",
      "Epoch: 44/99 Iteration: 3522 Training loss: 0.41928\n",
      "Epoch: 44/99 Iteration: 3523 Training loss: 0.46681\n",
      "Epoch: 44/99 Iteration: 3524 Training loss: 0.34310\n",
      "Epoch: 44/99 Iteration: 3525 Training loss: 0.39126\n",
      "Epoch: 44/99 Iteration: 3526 Training loss: 0.49734\n",
      "Epoch: 44/99 Iteration: 3527 Training loss: 0.36406\n",
      "Epoch: 44/99 Iteration: 3528 Training loss: 0.70823\n",
      "Epoch: 44/99 Iteration: 3529 Training loss: 0.48605\n",
      "Epoch: 44/99 Iteration: 3530 Training loss: 0.32957\n",
      "Epoch: 44/99 Iteration: 3531 Training loss: 0.40622\n",
      "Epoch: 44/99 Iteration: 3532 Training loss: 0.37126\n",
      "Epoch: 44/99 Iteration: 3533 Training loss: 0.51500\n",
      "Epoch: 44/99 Iteration: 3534 Training loss: 0.52132\n",
      "Epoch: 44/99 Iteration: 3535 Training loss: 0.32167\n",
      "Epoch: 44/99 Iteration: 3536 Training loss: 0.32775\n",
      "Epoch: 44/99 Iteration: 3537 Training loss: 0.33606\n",
      "Epoch: 44/99 Iteration: 3538 Training loss: 0.41290\n",
      "Epoch: 44/99 Iteration: 3539 Training loss: 0.39037\n",
      "Epoch: 44/99 Iteration: 3540 Training loss: 0.30418\n",
      "Epoch: 44/99 Iteration: 3541 Training loss: 0.43497\n",
      "Epoch: 44/99 Iteration: 3542 Training loss: 0.41939\n",
      "Epoch: 44/99 Iteration: 3543 Training loss: 0.35460\n",
      "Epoch: 44/99 Iteration: 3544 Training loss: 0.46792\n",
      "Epoch: 44/99 Iteration: 3545 Training loss: 0.46706\n",
      "Epoch: 44/99 Iteration: 3546 Training loss: 0.48068\n",
      "Epoch: 44/99 Iteration: 3547 Training loss: 0.31194\n",
      "Epoch: 44/99 Iteration: 3548 Training loss: 0.39672\n",
      "Epoch: 44/99 Iteration: 3549 Training loss: 0.54061\n",
      "Epoch: 44/99 Iteration: 3550 Training loss: 0.30933\n",
      "***\n",
      "Epoch: 44/99 Iteration: 3550 Validation Acc: 0.8450\n",
      "***\n",
      "Epoch: 44/99 Iteration: 3551 Training loss: 0.28154\n",
      "Epoch: 44/99 Iteration: 3552 Training loss: 0.38517\n",
      "Epoch: 44/99 Iteration: 3553 Training loss: 0.43790\n",
      "Epoch: 44/99 Iteration: 3554 Training loss: 0.34042\n",
      "Epoch: 44/99 Iteration: 3555 Training loss: 0.37770\n",
      "Epoch: 44/99 Iteration: 3556 Training loss: 0.46839\n",
      "Epoch: 44/99 Iteration: 3557 Training loss: 0.32532\n",
      "Epoch: 44/99 Iteration: 3558 Training loss: 0.34003\n",
      "Epoch: 44/99 Iteration: 3559 Training loss: 0.34899\n",
      "Epoch: 44/99 Iteration: 3560 Training loss: 0.46372\n",
      "Epoch: 44/99 Iteration: 3561 Training loss: 0.38816\n",
      "Epoch: 44/99 Iteration: 3562 Training loss: 0.37662\n",
      "Epoch: 44/99 Iteration: 3563 Training loss: 0.26233\n",
      "Epoch: 44/99 Iteration: 3564 Training loss: 0.42021\n",
      "Epoch: 44/99 Iteration: 3565 Training loss: 0.34849\n",
      "Epoch: 44/99 Iteration: 3566 Training loss: 0.57358\n",
      "Epoch: 44/99 Iteration: 3567 Training loss: 0.44020\n",
      "Epoch: 44/99 Iteration: 3568 Training loss: 0.44575\n",
      "Epoch: 44/99 Iteration: 3569 Training loss: 0.33924\n",
      "Epoch: 44/99 Iteration: 3570 Training loss: 0.33992\n",
      "Epoch: 44/99 Iteration: 3571 Training loss: 0.33033\n",
      "Epoch: 44/99 Iteration: 3572 Training loss: 0.39887\n",
      "Epoch: 44/99 Iteration: 3573 Training loss: 0.41957\n",
      "Epoch: 44/99 Iteration: 3574 Training loss: 0.49051\n",
      "Epoch: 44/99 Iteration: 3575 Training loss: 0.36045\n",
      "Epoch: 44/99 Iteration: 3576 Training loss: 0.42590\n",
      "Epoch: 44/99 Iteration: 3577 Training loss: 0.43255\n",
      "Epoch: 44/99 Iteration: 3578 Training loss: 0.38593\n",
      "Epoch: 44/99 Iteration: 3579 Training loss: 0.45614\n",
      "Epoch: 44/99 Iteration: 3580 Training loss: 0.34815\n",
      "Epoch: 44/99 Iteration: 3581 Training loss: 0.40997\n",
      "Epoch: 44/99 Iteration: 3582 Training loss: 0.39566\n",
      "Epoch: 44/99 Iteration: 3583 Training loss: 0.40731\n",
      "Epoch: 44/99 Iteration: 3584 Training loss: 0.48604\n",
      "Epoch: 44/99 Iteration: 3585 Training loss: 0.46045\n",
      "Epoch: 44/99 Iteration: 3586 Training loss: 0.46150\n",
      "Epoch: 44/99 Iteration: 3587 Training loss: 0.48785\n",
      "Epoch: 44/99 Iteration: 3588 Training loss: 0.46400\n",
      "Epoch: 44/99 Iteration: 3589 Training loss: 0.37456\n",
      "Epoch: 44/99 Iteration: 3590 Training loss: 0.48163\n",
      "Epoch: 44/99 Iteration: 3591 Training loss: 0.33887\n",
      "Epoch: 44/99 Iteration: 3592 Training loss: 0.33134\n",
      "Epoch: 44/99 Iteration: 3593 Training loss: 0.37347\n",
      "Epoch: 44/99 Iteration: 3594 Training loss: 0.40042\n",
      "Epoch: 44/99 Iteration: 3595 Training loss: 0.17753\n",
      "Epoch: 44/99 Iteration: 3596 Training loss: 0.36957\n",
      "Epoch: 44/99 Iteration: 3597 Training loss: 0.43242\n",
      "Epoch: 44/99 Iteration: 3598 Training loss: 0.49222\n",
      "Epoch: 44/99 Iteration: 3599 Training loss: 0.33520\n",
      "Epoch: 45/99 Iteration: 3600 Training loss: 0.34787\n",
      "***\n",
      "Epoch: 45/99 Iteration: 3600 Validation Acc: 0.8630\n",
      "***\n",
      "Epoch: 45/99 Iteration: 3601 Training loss: 0.58163\n",
      "Epoch: 45/99 Iteration: 3602 Training loss: 0.38987\n",
      "Epoch: 45/99 Iteration: 3603 Training loss: 0.29544\n",
      "Epoch: 45/99 Iteration: 3604 Training loss: 0.40167\n",
      "Epoch: 45/99 Iteration: 3605 Training loss: 0.34317\n",
      "Epoch: 45/99 Iteration: 3606 Training loss: 0.29586\n",
      "Epoch: 45/99 Iteration: 3607 Training loss: 0.43985\n",
      "Epoch: 45/99 Iteration: 3608 Training loss: 0.53964\n",
      "Epoch: 45/99 Iteration: 3609 Training loss: 0.27448\n",
      "Epoch: 45/99 Iteration: 3610 Training loss: 0.39996\n",
      "Epoch: 45/99 Iteration: 3611 Training loss: 0.50692\n",
      "Epoch: 45/99 Iteration: 3612 Training loss: 0.35253\n",
      "Epoch: 45/99 Iteration: 3613 Training loss: 0.56533\n",
      "Epoch: 45/99 Iteration: 3614 Training loss: 0.49446\n",
      "Epoch: 45/99 Iteration: 3615 Training loss: 0.36741\n",
      "Epoch: 45/99 Iteration: 3616 Training loss: 0.21279\n",
      "Epoch: 45/99 Iteration: 3617 Training loss: 0.20196\n",
      "Epoch: 45/99 Iteration: 3618 Training loss: 0.36927\n",
      "Epoch: 45/99 Iteration: 3619 Training loss: 0.26558\n",
      "Epoch: 45/99 Iteration: 3620 Training loss: 0.40338\n",
      "Epoch: 45/99 Iteration: 3621 Training loss: 0.33379\n",
      "Epoch: 45/99 Iteration: 3622 Training loss: 0.43933\n",
      "Epoch: 45/99 Iteration: 3623 Training loss: 0.49956\n",
      "Epoch: 45/99 Iteration: 3624 Training loss: 0.30122\n",
      "Epoch: 45/99 Iteration: 3625 Training loss: 0.30659\n",
      "Epoch: 45/99 Iteration: 3626 Training loss: 0.36755\n",
      "Epoch: 45/99 Iteration: 3627 Training loss: 0.34971\n",
      "Epoch: 45/99 Iteration: 3628 Training loss: 0.63129\n",
      "Epoch: 45/99 Iteration: 3629 Training loss: 0.40726\n",
      "Epoch: 45/99 Iteration: 3630 Training loss: 0.30152\n",
      "Epoch: 45/99 Iteration: 3631 Training loss: 0.36731\n",
      "Epoch: 45/99 Iteration: 3632 Training loss: 0.50323\n",
      "Epoch: 45/99 Iteration: 3633 Training loss: 0.31805\n",
      "Epoch: 45/99 Iteration: 3634 Training loss: 0.37525\n",
      "Epoch: 45/99 Iteration: 3635 Training loss: 0.49842\n",
      "Epoch: 45/99 Iteration: 3636 Training loss: 0.49315\n",
      "Epoch: 45/99 Iteration: 3637 Training loss: 0.24374\n",
      "Epoch: 45/99 Iteration: 3638 Training loss: 0.30494\n",
      "Epoch: 45/99 Iteration: 3639 Training loss: 0.29673\n",
      "Epoch: 45/99 Iteration: 3640 Training loss: 0.39421\n",
      "Epoch: 45/99 Iteration: 3641 Training loss: 0.44688\n",
      "Epoch: 45/99 Iteration: 3642 Training loss: 0.41376\n",
      "Epoch: 45/99 Iteration: 3643 Training loss: 0.36110\n",
      "Epoch: 45/99 Iteration: 3644 Training loss: 0.42830\n",
      "Epoch: 45/99 Iteration: 3645 Training loss: 0.40478\n",
      "Epoch: 45/99 Iteration: 3646 Training loss: 0.39121\n",
      "Epoch: 45/99 Iteration: 3647 Training loss: 0.41463\n",
      "Epoch: 45/99 Iteration: 3648 Training loss: 0.46564\n",
      "Epoch: 45/99 Iteration: 3649 Training loss: 0.37803\n",
      "Epoch: 45/99 Iteration: 3650 Training loss: 0.42316\n",
      "***\n",
      "Epoch: 45/99 Iteration: 3650 Validation Acc: 0.8290\n",
      "***\n",
      "Epoch: 45/99 Iteration: 3651 Training loss: 0.49830\n",
      "Epoch: 45/99 Iteration: 3652 Training loss: 0.31749\n",
      "Epoch: 45/99 Iteration: 3653 Training loss: 0.47318\n",
      "Epoch: 45/99 Iteration: 3654 Training loss: 0.47873\n",
      "Epoch: 45/99 Iteration: 3655 Training loss: 0.44869\n",
      "Epoch: 45/99 Iteration: 3656 Training loss: 0.43464\n",
      "Epoch: 45/99 Iteration: 3657 Training loss: 0.48766\n",
      "Epoch: 45/99 Iteration: 3658 Training loss: 0.44098\n",
      "Epoch: 45/99 Iteration: 3659 Training loss: 0.60428\n",
      "Epoch: 45/99 Iteration: 3660 Training loss: 0.51346\n",
      "Epoch: 45/99 Iteration: 3661 Training loss: 0.42904\n",
      "Epoch: 45/99 Iteration: 3662 Training loss: 0.37678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/99 Iteration: 3663 Training loss: 0.52356\n",
      "Epoch: 45/99 Iteration: 3664 Training loss: 0.53940\n",
      "Epoch: 45/99 Iteration: 3665 Training loss: 0.46438\n",
      "Epoch: 45/99 Iteration: 3666 Training loss: 0.62170\n",
      "Epoch: 45/99 Iteration: 3667 Training loss: 0.53527\n",
      "Epoch: 45/99 Iteration: 3668 Training loss: 0.46337\n",
      "Epoch: 45/99 Iteration: 3669 Training loss: 0.38097\n",
      "Epoch: 45/99 Iteration: 3670 Training loss: 0.56871\n",
      "Epoch: 45/99 Iteration: 3671 Training loss: 0.51126\n",
      "Epoch: 45/99 Iteration: 3672 Training loss: 0.43779\n",
      "Epoch: 45/99 Iteration: 3673 Training loss: 0.32925\n",
      "Epoch: 45/99 Iteration: 3674 Training loss: 0.45860\n",
      "Epoch: 45/99 Iteration: 3675 Training loss: 0.50640\n",
      "Epoch: 45/99 Iteration: 3676 Training loss: 0.46860\n",
      "Epoch: 45/99 Iteration: 3677 Training loss: 0.58367\n",
      "Epoch: 45/99 Iteration: 3678 Training loss: 0.34776\n",
      "Epoch: 45/99 Iteration: 3679 Training loss: 0.43868\n",
      "Epoch: 46/99 Iteration: 3680 Training loss: 0.40960\n",
      "Epoch: 46/99 Iteration: 3681 Training loss: 0.59331\n",
      "Epoch: 46/99 Iteration: 3682 Training loss: 0.56055\n",
      "Epoch: 46/99 Iteration: 3683 Training loss: 0.40872\n",
      "Epoch: 46/99 Iteration: 3684 Training loss: 0.47626\n",
      "Epoch: 46/99 Iteration: 3685 Training loss: 0.39533\n",
      "Epoch: 46/99 Iteration: 3686 Training loss: 0.47433\n",
      "Epoch: 46/99 Iteration: 3687 Training loss: 0.45558\n",
      "Epoch: 46/99 Iteration: 3688 Training loss: 0.78235\n",
      "Epoch: 46/99 Iteration: 3689 Training loss: 0.34392\n",
      "Epoch: 46/99 Iteration: 3690 Training loss: 0.47221\n",
      "Epoch: 46/99 Iteration: 3691 Training loss: 0.41144\n",
      "Epoch: 46/99 Iteration: 3692 Training loss: 0.35870\n",
      "Epoch: 46/99 Iteration: 3693 Training loss: 0.45124\n",
      "Epoch: 46/99 Iteration: 3694 Training loss: 0.41138\n",
      "Epoch: 46/99 Iteration: 3695 Training loss: 0.33387\n",
      "Epoch: 46/99 Iteration: 3696 Training loss: 0.24427\n",
      "Epoch: 46/99 Iteration: 3697 Training loss: 0.40861\n",
      "Epoch: 46/99 Iteration: 3698 Training loss: 0.46443\n",
      "Epoch: 46/99 Iteration: 3699 Training loss: 0.23874\n",
      "Epoch: 46/99 Iteration: 3700 Training loss: 0.48069\n",
      "***\n",
      "Epoch: 46/99 Iteration: 3700 Validation Acc: 0.8450\n",
      "***\n",
      "Epoch: 46/99 Iteration: 3701 Training loss: 0.38530\n",
      "Epoch: 46/99 Iteration: 3702 Training loss: 0.32430\n",
      "Epoch: 46/99 Iteration: 3703 Training loss: 0.50611\n",
      "Epoch: 46/99 Iteration: 3704 Training loss: 0.34002\n",
      "Epoch: 46/99 Iteration: 3705 Training loss: 0.39687\n",
      "Epoch: 46/99 Iteration: 3706 Training loss: 0.41164\n",
      "Epoch: 46/99 Iteration: 3707 Training loss: 0.33583\n",
      "Epoch: 46/99 Iteration: 3708 Training loss: 0.43429\n",
      "Epoch: 46/99 Iteration: 3709 Training loss: 0.55801\n",
      "Epoch: 46/99 Iteration: 3710 Training loss: 0.36569\n",
      "Epoch: 46/99 Iteration: 3711 Training loss: 0.39514\n",
      "Epoch: 46/99 Iteration: 3712 Training loss: 0.43107\n",
      "Epoch: 46/99 Iteration: 3713 Training loss: 0.40094\n",
      "Epoch: 46/99 Iteration: 3714 Training loss: 0.43814\n",
      "Epoch: 46/99 Iteration: 3715 Training loss: 0.45011\n",
      "Epoch: 46/99 Iteration: 3716 Training loss: 0.57894\n",
      "Epoch: 46/99 Iteration: 3717 Training loss: 0.36055\n",
      "Epoch: 46/99 Iteration: 3718 Training loss: 0.38989\n",
      "Epoch: 46/99 Iteration: 3719 Training loss: 0.40049\n",
      "Epoch: 46/99 Iteration: 3720 Training loss: 0.35983\n",
      "Epoch: 46/99 Iteration: 3721 Training loss: 0.46998\n",
      "Epoch: 46/99 Iteration: 3722 Training loss: 0.49602\n",
      "Epoch: 46/99 Iteration: 3723 Training loss: 0.41419\n",
      "Epoch: 46/99 Iteration: 3724 Training loss: 0.36355\n",
      "Epoch: 46/99 Iteration: 3725 Training loss: 0.32254\n",
      "Epoch: 46/99 Iteration: 3726 Training loss: 0.54006\n",
      "Epoch: 46/99 Iteration: 3727 Training loss: 0.47547\n",
      "Epoch: 46/99 Iteration: 3728 Training loss: 0.42540\n",
      "Epoch: 46/99 Iteration: 3729 Training loss: 0.34698\n",
      "Epoch: 46/99 Iteration: 3730 Training loss: 0.32792\n",
      "Epoch: 46/99 Iteration: 3731 Training loss: 0.52210\n",
      "Epoch: 46/99 Iteration: 3732 Training loss: 0.41504\n",
      "Epoch: 46/99 Iteration: 3733 Training loss: 0.44093\n",
      "Epoch: 46/99 Iteration: 3734 Training loss: 0.42698\n",
      "Epoch: 46/99 Iteration: 3735 Training loss: 0.42760\n",
      "Epoch: 46/99 Iteration: 3736 Training loss: 0.46018\n",
      "Epoch: 46/99 Iteration: 3737 Training loss: 0.49297\n",
      "Epoch: 46/99 Iteration: 3738 Training loss: 0.54401\n",
      "Epoch: 46/99 Iteration: 3739 Training loss: 0.41482\n",
      "Epoch: 46/99 Iteration: 3740 Training loss: 0.43728\n",
      "Epoch: 46/99 Iteration: 3741 Training loss: 0.47452\n",
      "Epoch: 46/99 Iteration: 3742 Training loss: 0.32064\n",
      "Epoch: 46/99 Iteration: 3743 Training loss: 0.44053\n",
      "Epoch: 46/99 Iteration: 3744 Training loss: 0.44000\n",
      "Epoch: 46/99 Iteration: 3745 Training loss: 0.43072\n",
      "Epoch: 46/99 Iteration: 3746 Training loss: 0.47483\n",
      "Epoch: 46/99 Iteration: 3747 Training loss: 0.42282\n",
      "Epoch: 46/99 Iteration: 3748 Training loss: 0.42914\n",
      "Epoch: 46/99 Iteration: 3749 Training loss: 0.36724\n",
      "Epoch: 46/99 Iteration: 3750 Training loss: 0.56297\n",
      "***\n",
      "Epoch: 46/99 Iteration: 3750 Validation Acc: 0.8510\n",
      "***\n",
      "Epoch: 46/99 Iteration: 3751 Training loss: 0.39155\n",
      "Epoch: 46/99 Iteration: 3752 Training loss: 0.36293\n",
      "Epoch: 46/99 Iteration: 3753 Training loss: 0.36971\n",
      "Epoch: 46/99 Iteration: 3754 Training loss: 0.35170\n",
      "Epoch: 46/99 Iteration: 3755 Training loss: 0.33316\n",
      "Epoch: 46/99 Iteration: 3756 Training loss: 0.35062\n",
      "Epoch: 46/99 Iteration: 3757 Training loss: 0.65733\n",
      "Epoch: 46/99 Iteration: 3758 Training loss: 0.38520\n",
      "Epoch: 46/99 Iteration: 3759 Training loss: 0.36088\n",
      "Epoch: 47/99 Iteration: 3760 Training loss: 0.38892\n",
      "Epoch: 47/99 Iteration: 3761 Training loss: 0.53674\n",
      "Epoch: 47/99 Iteration: 3762 Training loss: 0.34563\n",
      "Epoch: 47/99 Iteration: 3763 Training loss: 0.52570\n",
      "Epoch: 47/99 Iteration: 3764 Training loss: 0.46553\n",
      "Epoch: 47/99 Iteration: 3765 Training loss: 0.39911\n",
      "Epoch: 47/99 Iteration: 3766 Training loss: 0.37207\n",
      "Epoch: 47/99 Iteration: 3767 Training loss: 0.33527\n",
      "Epoch: 47/99 Iteration: 3768 Training loss: 0.52801\n",
      "Epoch: 47/99 Iteration: 3769 Training loss: 0.33306\n",
      "Epoch: 47/99 Iteration: 3770 Training loss: 0.31172\n",
      "Epoch: 47/99 Iteration: 3771 Training loss: 0.33009\n",
      "Epoch: 47/99 Iteration: 3772 Training loss: 0.33274\n",
      "Epoch: 47/99 Iteration: 3773 Training loss: 0.54494\n",
      "Epoch: 47/99 Iteration: 3774 Training loss: 0.55702\n",
      "Epoch: 47/99 Iteration: 3775 Training loss: 0.33701\n",
      "Epoch: 47/99 Iteration: 3776 Training loss: 0.32947\n",
      "Epoch: 47/99 Iteration: 3777 Training loss: 0.26653\n",
      "Epoch: 47/99 Iteration: 3778 Training loss: 0.37644\n",
      "Epoch: 47/99 Iteration: 3779 Training loss: 0.35060\n",
      "Epoch: 47/99 Iteration: 3780 Training loss: 0.39690\n",
      "Epoch: 47/99 Iteration: 3781 Training loss: 0.39525\n",
      "Epoch: 47/99 Iteration: 3782 Training loss: 0.37945\n",
      "Epoch: 47/99 Iteration: 3783 Training loss: 0.29597\n",
      "Epoch: 47/99 Iteration: 3784 Training loss: 0.38340\n",
      "Epoch: 47/99 Iteration: 3785 Training loss: 0.39419\n",
      "Epoch: 47/99 Iteration: 3786 Training loss: 0.45374\n",
      "Epoch: 47/99 Iteration: 3787 Training loss: 0.41033\n",
      "Epoch: 47/99 Iteration: 3788 Training loss: 0.36065\n",
      "Epoch: 47/99 Iteration: 3789 Training loss: 0.61826\n",
      "Epoch: 47/99 Iteration: 3790 Training loss: 0.36293\n",
      "Epoch: 47/99 Iteration: 3791 Training loss: 0.32710\n",
      "Epoch: 47/99 Iteration: 3792 Training loss: 0.40193\n",
      "Epoch: 47/99 Iteration: 3793 Training loss: 0.26989\n",
      "Epoch: 47/99 Iteration: 3794 Training loss: 0.37334\n",
      "Epoch: 47/99 Iteration: 3795 Training loss: 0.36413\n",
      "Epoch: 47/99 Iteration: 3796 Training loss: 0.43334\n",
      "Epoch: 47/99 Iteration: 3797 Training loss: 0.26683\n",
      "Epoch: 47/99 Iteration: 3798 Training loss: 0.39185\n",
      "Epoch: 47/99 Iteration: 3799 Training loss: 0.33569\n",
      "Epoch: 47/99 Iteration: 3800 Training loss: 0.41810\n",
      "***\n",
      "Epoch: 47/99 Iteration: 3800 Validation Acc: 0.8420\n",
      "***\n",
      "Epoch: 47/99 Iteration: 3801 Training loss: 0.33956\n",
      "Epoch: 47/99 Iteration: 3802 Training loss: 0.43980\n",
      "Epoch: 47/99 Iteration: 3803 Training loss: 0.38262\n",
      "Epoch: 47/99 Iteration: 3804 Training loss: 0.45966\n",
      "Epoch: 47/99 Iteration: 3805 Training loss: 0.43979\n",
      "Epoch: 47/99 Iteration: 3806 Training loss: 0.44935\n",
      "Epoch: 47/99 Iteration: 3807 Training loss: 0.35827\n",
      "Epoch: 47/99 Iteration: 3808 Training loss: 0.52439\n",
      "Epoch: 47/99 Iteration: 3809 Training loss: 0.27971\n",
      "Epoch: 47/99 Iteration: 3810 Training loss: 0.40363\n",
      "Epoch: 47/99 Iteration: 3811 Training loss: 0.46071\n",
      "Epoch: 47/99 Iteration: 3812 Training loss: 0.45161\n",
      "Epoch: 47/99 Iteration: 3813 Training loss: 0.49113\n",
      "Epoch: 47/99 Iteration: 3814 Training loss: 0.56467\n",
      "Epoch: 47/99 Iteration: 3815 Training loss: 0.37971\n",
      "Epoch: 47/99 Iteration: 3816 Training loss: 0.31982\n",
      "Epoch: 47/99 Iteration: 3817 Training loss: 0.35922\n",
      "Epoch: 47/99 Iteration: 3818 Training loss: 0.35021\n",
      "Epoch: 47/99 Iteration: 3819 Training loss: 0.45423\n",
      "Epoch: 47/99 Iteration: 3820 Training loss: 0.37343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/99 Iteration: 3821 Training loss: 0.43540\n",
      "Epoch: 47/99 Iteration: 3822 Training loss: 0.45530\n",
      "Epoch: 47/99 Iteration: 3823 Training loss: 0.34233\n",
      "Epoch: 47/99 Iteration: 3824 Training loss: 0.30916\n",
      "Epoch: 47/99 Iteration: 3825 Training loss: 0.37213\n",
      "Epoch: 47/99 Iteration: 3826 Training loss: 0.42603\n",
      "Epoch: 47/99 Iteration: 3827 Training loss: 0.36448\n",
      "Epoch: 47/99 Iteration: 3828 Training loss: 0.44841\n",
      "Epoch: 47/99 Iteration: 3829 Training loss: 0.39039\n",
      "Epoch: 47/99 Iteration: 3830 Training loss: 0.50489\n",
      "Epoch: 47/99 Iteration: 3831 Training loss: 0.30527\n",
      "Epoch: 47/99 Iteration: 3832 Training loss: 0.38278\n",
      "Epoch: 47/99 Iteration: 3833 Training loss: 0.32496\n",
      "Epoch: 47/99 Iteration: 3834 Training loss: 0.36905\n",
      "Epoch: 47/99 Iteration: 3835 Training loss: 0.32314\n",
      "Epoch: 47/99 Iteration: 3836 Training loss: 0.34709\n",
      "Epoch: 47/99 Iteration: 3837 Training loss: 0.45591\n",
      "Epoch: 47/99 Iteration: 3838 Training loss: 0.52574\n",
      "Epoch: 47/99 Iteration: 3839 Training loss: 0.47826\n",
      "Epoch: 48/99 Iteration: 3840 Training loss: 0.25069\n",
      "Epoch: 48/99 Iteration: 3841 Training loss: 0.49602\n",
      "Epoch: 48/99 Iteration: 3842 Training loss: 0.40134\n",
      "Epoch: 48/99 Iteration: 3843 Training loss: 0.29955\n",
      "Epoch: 48/99 Iteration: 3844 Training loss: 0.41928\n",
      "Epoch: 48/99 Iteration: 3845 Training loss: 0.44412\n",
      "Epoch: 48/99 Iteration: 3846 Training loss: 0.36977\n",
      "Epoch: 48/99 Iteration: 3847 Training loss: 0.36623\n",
      "Epoch: 48/99 Iteration: 3848 Training loss: 0.50512\n",
      "Epoch: 48/99 Iteration: 3849 Training loss: 0.33599\n",
      "Epoch: 48/99 Iteration: 3850 Training loss: 0.28079\n",
      "***\n",
      "Epoch: 48/99 Iteration: 3850 Validation Acc: 0.8570\n",
      "***\n",
      "Epoch: 48/99 Iteration: 3851 Training loss: 0.32714\n",
      "Epoch: 48/99 Iteration: 3852 Training loss: 0.25895\n",
      "Epoch: 48/99 Iteration: 3853 Training loss: 0.46864\n",
      "Epoch: 48/99 Iteration: 3854 Training loss: 0.53872\n",
      "Epoch: 48/99 Iteration: 3855 Training loss: 0.33970\n",
      "Epoch: 48/99 Iteration: 3856 Training loss: 0.26420\n",
      "Epoch: 48/99 Iteration: 3857 Training loss: 0.24049\n",
      "Epoch: 48/99 Iteration: 3858 Training loss: 0.46389\n",
      "Epoch: 48/99 Iteration: 3859 Training loss: 0.29064\n",
      "Epoch: 48/99 Iteration: 3860 Training loss: 0.38233\n",
      "Epoch: 48/99 Iteration: 3861 Training loss: 0.43139\n",
      "Epoch: 48/99 Iteration: 3862 Training loss: 0.34921\n",
      "Epoch: 48/99 Iteration: 3863 Training loss: 0.40696\n",
      "Epoch: 48/99 Iteration: 3864 Training loss: 0.22524\n",
      "Epoch: 48/99 Iteration: 3865 Training loss: 0.48411\n",
      "Epoch: 48/99 Iteration: 3866 Training loss: 0.36434\n",
      "Epoch: 48/99 Iteration: 3867 Training loss: 0.33823\n",
      "Epoch: 48/99 Iteration: 3868 Training loss: 0.44758\n",
      "Epoch: 48/99 Iteration: 3869 Training loss: 0.45637\n",
      "Epoch: 48/99 Iteration: 3870 Training loss: 0.28117\n",
      "Epoch: 48/99 Iteration: 3871 Training loss: 0.46558\n",
      "Epoch: 48/99 Iteration: 3872 Training loss: 0.39617\n",
      "Epoch: 48/99 Iteration: 3873 Training loss: 0.39286\n",
      "Epoch: 48/99 Iteration: 3874 Training loss: 0.32938\n",
      "Epoch: 48/99 Iteration: 3875 Training loss: 0.39432\n",
      "Epoch: 48/99 Iteration: 3876 Training loss: 0.40821\n",
      "Epoch: 48/99 Iteration: 3877 Training loss: 0.24242\n",
      "Epoch: 48/99 Iteration: 3878 Training loss: 0.49588\n",
      "Epoch: 48/99 Iteration: 3879 Training loss: 0.35112\n",
      "Epoch: 48/99 Iteration: 3880 Training loss: 0.43085\n",
      "Epoch: 48/99 Iteration: 3881 Training loss: 0.33260\n",
      "Epoch: 48/99 Iteration: 3882 Training loss: 0.38801\n",
      "Epoch: 48/99 Iteration: 3883 Training loss: 0.31798\n",
      "Epoch: 48/99 Iteration: 3884 Training loss: 0.29474\n",
      "Epoch: 48/99 Iteration: 3885 Training loss: 0.28750\n",
      "Epoch: 48/99 Iteration: 3886 Training loss: 0.45205\n",
      "Epoch: 48/99 Iteration: 3887 Training loss: 0.37152\n",
      "Epoch: 48/99 Iteration: 3888 Training loss: 0.41409\n",
      "Epoch: 48/99 Iteration: 3889 Training loss: 0.38559\n",
      "Epoch: 48/99 Iteration: 3890 Training loss: 0.35620\n",
      "Epoch: 48/99 Iteration: 3891 Training loss: 0.41789\n",
      "Epoch: 48/99 Iteration: 3892 Training loss: 0.34341\n",
      "Epoch: 48/99 Iteration: 3893 Training loss: 0.40505\n",
      "Epoch: 48/99 Iteration: 3894 Training loss: 0.48876\n",
      "Epoch: 48/99 Iteration: 3895 Training loss: 0.40151\n",
      "Epoch: 48/99 Iteration: 3896 Training loss: 0.53317\n",
      "Epoch: 48/99 Iteration: 3897 Training loss: 0.41638\n",
      "Epoch: 48/99 Iteration: 3898 Training loss: 0.39777\n",
      "Epoch: 48/99 Iteration: 3899 Training loss: 0.52419\n",
      "Epoch: 48/99 Iteration: 3900 Training loss: 0.40652\n",
      "***\n",
      "Epoch: 48/99 Iteration: 3900 Validation Acc: 0.8420\n",
      "***\n",
      "Epoch: 48/99 Iteration: 3901 Training loss: 0.31255\n",
      "Epoch: 48/99 Iteration: 3902 Training loss: 0.34668\n",
      "Epoch: 48/99 Iteration: 3903 Training loss: 0.45793\n",
      "Epoch: 48/99 Iteration: 3904 Training loss: 0.51055\n",
      "Epoch: 48/99 Iteration: 3905 Training loss: 0.39532\n",
      "Epoch: 48/99 Iteration: 3906 Training loss: 0.46703\n",
      "Epoch: 48/99 Iteration: 3907 Training loss: 0.36841\n",
      "Epoch: 48/99 Iteration: 3908 Training loss: 0.37354\n",
      "Epoch: 48/99 Iteration: 3909 Training loss: 0.34612\n",
      "Epoch: 48/99 Iteration: 3910 Training loss: 0.57355\n",
      "Epoch: 48/99 Iteration: 3911 Training loss: 0.37298\n",
      "Epoch: 48/99 Iteration: 3912 Training loss: 0.48595\n",
      "Epoch: 48/99 Iteration: 3913 Training loss: 0.45240\n",
      "Epoch: 48/99 Iteration: 3914 Training loss: 0.41937\n",
      "Epoch: 48/99 Iteration: 3915 Training loss: 0.33632\n",
      "Epoch: 48/99 Iteration: 3916 Training loss: 0.46102\n",
      "Epoch: 48/99 Iteration: 3917 Training loss: 0.47273\n",
      "Epoch: 48/99 Iteration: 3918 Training loss: 0.39471\n",
      "Epoch: 48/99 Iteration: 3919 Training loss: 0.44033\n",
      "Epoch: 49/99 Iteration: 3920 Training loss: 0.30942\n",
      "Epoch: 49/99 Iteration: 3921 Training loss: 0.38319\n",
      "Epoch: 49/99 Iteration: 3922 Training loss: 0.44277\n",
      "Epoch: 49/99 Iteration: 3923 Training loss: 0.58898\n",
      "Epoch: 49/99 Iteration: 3924 Training loss: 0.33769\n",
      "Epoch: 49/99 Iteration: 3925 Training loss: 0.39746\n",
      "Epoch: 49/99 Iteration: 3926 Training loss: 0.33364\n",
      "Epoch: 49/99 Iteration: 3927 Training loss: 0.40755\n",
      "Epoch: 49/99 Iteration: 3928 Training loss: 0.51415\n",
      "Epoch: 49/99 Iteration: 3929 Training loss: 0.31266\n",
      "Epoch: 49/99 Iteration: 3930 Training loss: 0.34739\n",
      "Epoch: 49/99 Iteration: 3931 Training loss: 0.39594\n",
      "Epoch: 49/99 Iteration: 3932 Training loss: 0.23531\n",
      "Epoch: 49/99 Iteration: 3933 Training loss: 0.50996\n",
      "Epoch: 49/99 Iteration: 3934 Training loss: 0.50967\n",
      "Epoch: 49/99 Iteration: 3935 Training loss: 0.25769\n",
      "Epoch: 49/99 Iteration: 3936 Training loss: 0.34276\n",
      "Epoch: 49/99 Iteration: 3937 Training loss: 0.29894\n",
      "Epoch: 49/99 Iteration: 3938 Training loss: 0.35607\n",
      "Epoch: 49/99 Iteration: 3939 Training loss: 0.31106\n",
      "Epoch: 49/99 Iteration: 3940 Training loss: 0.34412\n",
      "Epoch: 49/99 Iteration: 3941 Training loss: 0.31505\n",
      "Epoch: 49/99 Iteration: 3942 Training loss: 0.37460\n",
      "Epoch: 49/99 Iteration: 3943 Training loss: 0.34429\n",
      "Epoch: 49/99 Iteration: 3944 Training loss: 0.26572\n",
      "Epoch: 49/99 Iteration: 3945 Training loss: 0.34785\n",
      "Epoch: 49/99 Iteration: 3946 Training loss: 0.42886\n",
      "Epoch: 49/99 Iteration: 3947 Training loss: 0.31196\n",
      "Epoch: 49/99 Iteration: 3948 Training loss: 0.39388\n",
      "Epoch: 49/99 Iteration: 3949 Training loss: 0.72105\n",
      "Epoch: 49/99 Iteration: 3950 Training loss: 0.28467\n",
      "***\n",
      "Epoch: 49/99 Iteration: 3950 Validation Acc: 0.8630\n",
      "***\n",
      "Epoch: 49/99 Iteration: 3951 Training loss: 0.20668\n",
      "Epoch: 49/99 Iteration: 3952 Training loss: 0.25431\n",
      "Epoch: 49/99 Iteration: 3953 Training loss: 0.38152\n",
      "Epoch: 49/99 Iteration: 3954 Training loss: 0.36081\n",
      "Epoch: 49/99 Iteration: 3955 Training loss: 0.31037\n",
      "Epoch: 49/99 Iteration: 3956 Training loss: 0.61716\n",
      "Epoch: 49/99 Iteration: 3957 Training loss: 0.27917\n",
      "Epoch: 49/99 Iteration: 3958 Training loss: 0.30872\n",
      "Epoch: 49/99 Iteration: 3959 Training loss: 0.29358\n",
      "Epoch: 49/99 Iteration: 3960 Training loss: 0.41642\n",
      "Epoch: 49/99 Iteration: 3961 Training loss: 0.37840\n",
      "Epoch: 49/99 Iteration: 3962 Training loss: 0.36468\n",
      "Epoch: 49/99 Iteration: 3963 Training loss: 0.35972\n",
      "Epoch: 49/99 Iteration: 3964 Training loss: 0.43668\n",
      "Epoch: 49/99 Iteration: 3965 Training loss: 0.38761\n",
      "Epoch: 49/99 Iteration: 3966 Training loss: 0.39452\n",
      "Epoch: 49/99 Iteration: 3967 Training loss: 0.32363\n",
      "Epoch: 49/99 Iteration: 3968 Training loss: 0.67787\n",
      "Epoch: 49/99 Iteration: 3969 Training loss: 0.35142\n",
      "Epoch: 49/99 Iteration: 3970 Training loss: 0.42555\n",
      "Epoch: 49/99 Iteration: 3971 Training loss: 0.37979\n",
      "Epoch: 49/99 Iteration: 3972 Training loss: 0.32858\n",
      "Epoch: 49/99 Iteration: 3973 Training loss: 0.39091\n",
      "Epoch: 49/99 Iteration: 3974 Training loss: 0.39800\n",
      "Epoch: 49/99 Iteration: 3975 Training loss: 0.35272\n",
      "Epoch: 49/99 Iteration: 3976 Training loss: 0.40996\n",
      "Epoch: 49/99 Iteration: 3977 Training loss: 0.34803\n",
      "Epoch: 49/99 Iteration: 3978 Training loss: 0.54404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/99 Iteration: 3979 Training loss: 0.51323\n",
      "Epoch: 49/99 Iteration: 3980 Training loss: 0.40127\n",
      "Epoch: 49/99 Iteration: 3981 Training loss: 0.53788\n",
      "Epoch: 49/99 Iteration: 3982 Training loss: 0.35296\n",
      "Epoch: 49/99 Iteration: 3983 Training loss: 0.48064\n",
      "Epoch: 49/99 Iteration: 3984 Training loss: 0.46894\n",
      "Epoch: 49/99 Iteration: 3985 Training loss: 0.46810\n",
      "Epoch: 49/99 Iteration: 3986 Training loss: 0.46109\n",
      "Epoch: 49/99 Iteration: 3987 Training loss: 0.40959\n",
      "Epoch: 49/99 Iteration: 3988 Training loss: 0.33417\n",
      "Epoch: 49/99 Iteration: 3989 Training loss: 0.45551\n",
      "Epoch: 49/99 Iteration: 3990 Training loss: 0.46434\n",
      "Epoch: 49/99 Iteration: 3991 Training loss: 0.29928\n",
      "Epoch: 49/99 Iteration: 3992 Training loss: 0.32072\n",
      "Epoch: 49/99 Iteration: 3993 Training loss: 0.34320\n",
      "Epoch: 49/99 Iteration: 3994 Training loss: 0.35010\n",
      "Epoch: 49/99 Iteration: 3995 Training loss: 0.38477\n",
      "Epoch: 49/99 Iteration: 3996 Training loss: 0.31112\n",
      "Epoch: 49/99 Iteration: 3997 Training loss: 0.37713\n",
      "Epoch: 49/99 Iteration: 3998 Training loss: 0.36007\n",
      "Epoch: 49/99 Iteration: 3999 Training loss: 0.30152\n",
      "Epoch: 50/99 Iteration: 4000 Training loss: 0.33907\n",
      "***\n",
      "Epoch: 50/99 Iteration: 4000 Validation Acc: 0.8400\n",
      "***\n",
      "Epoch: 50/99 Iteration: 4001 Training loss: 0.54622\n",
      "Epoch: 50/99 Iteration: 4002 Training loss: 0.42285\n",
      "Epoch: 50/99 Iteration: 4003 Training loss: 0.30693\n",
      "Epoch: 50/99 Iteration: 4004 Training loss: 0.34650\n",
      "Epoch: 50/99 Iteration: 4005 Training loss: 0.36058\n",
      "Epoch: 50/99 Iteration: 4006 Training loss: 0.41314\n",
      "Epoch: 50/99 Iteration: 4007 Training loss: 0.38697\n",
      "Epoch: 50/99 Iteration: 4008 Training loss: 0.50593\n",
      "Epoch: 50/99 Iteration: 4009 Training loss: 0.31289\n",
      "Epoch: 50/99 Iteration: 4010 Training loss: 0.34740\n",
      "Epoch: 50/99 Iteration: 4011 Training loss: 0.28562\n",
      "Epoch: 50/99 Iteration: 4012 Training loss: 0.36201\n",
      "Epoch: 50/99 Iteration: 4013 Training loss: 0.48364\n",
      "Epoch: 50/99 Iteration: 4014 Training loss: 0.54870\n",
      "Epoch: 50/99 Iteration: 4015 Training loss: 0.33304\n",
      "Epoch: 50/99 Iteration: 4016 Training loss: 0.23245\n",
      "Epoch: 50/99 Iteration: 4017 Training loss: 0.36597\n",
      "Epoch: 50/99 Iteration: 4018 Training loss: 0.27543\n",
      "Epoch: 50/99 Iteration: 4019 Training loss: 0.26255\n",
      "Epoch: 50/99 Iteration: 4020 Training loss: 0.44395\n",
      "Epoch: 50/99 Iteration: 4021 Training loss: 0.46316\n",
      "Epoch: 50/99 Iteration: 4022 Training loss: 0.45001\n",
      "Epoch: 50/99 Iteration: 4023 Training loss: 0.43383\n",
      "Epoch: 50/99 Iteration: 4024 Training loss: 0.37274\n",
      "Epoch: 50/99 Iteration: 4025 Training loss: 0.46730\n",
      "Epoch: 50/99 Iteration: 4026 Training loss: 0.43666\n",
      "Epoch: 50/99 Iteration: 4027 Training loss: 0.34025\n",
      "Epoch: 50/99 Iteration: 4028 Training loss: 0.35807\n",
      "Epoch: 50/99 Iteration: 4029 Training loss: 0.46557\n",
      "Epoch: 50/99 Iteration: 4030 Training loss: 0.28701\n",
      "Epoch: 50/99 Iteration: 4031 Training loss: 0.28031\n",
      "Epoch: 50/99 Iteration: 4032 Training loss: 0.48469\n",
      "Epoch: 50/99 Iteration: 4033 Training loss: 0.39660\n",
      "Epoch: 50/99 Iteration: 4034 Training loss: 0.29175\n",
      "Epoch: 50/99 Iteration: 4035 Training loss: 0.34921\n",
      "Epoch: 50/99 Iteration: 4036 Training loss: 0.61136\n",
      "Epoch: 50/99 Iteration: 4037 Training loss: 0.25924\n",
      "Epoch: 50/99 Iteration: 4038 Training loss: 0.33835\n",
      "Epoch: 50/99 Iteration: 4039 Training loss: 0.37986\n",
      "Epoch: 50/99 Iteration: 4040 Training loss: 0.29774\n",
      "Epoch: 50/99 Iteration: 4041 Training loss: 0.39671\n",
      "Epoch: 50/99 Iteration: 4042 Training loss: 0.44846\n",
      "Epoch: 50/99 Iteration: 4043 Training loss: 0.48271\n",
      "Epoch: 50/99 Iteration: 4044 Training loss: 0.55253\n",
      "Epoch: 50/99 Iteration: 4045 Training loss: 0.41566\n",
      "Epoch: 50/99 Iteration: 4046 Training loss: 0.42284\n",
      "Epoch: 50/99 Iteration: 4047 Training loss: 0.53453\n",
      "Epoch: 50/99 Iteration: 4048 Training loss: 0.57410\n",
      "Epoch: 50/99 Iteration: 4049 Training loss: 0.34586\n",
      "Epoch: 50/99 Iteration: 4050 Training loss: 0.40351\n",
      "***\n",
      "Epoch: 50/99 Iteration: 4050 Validation Acc: 0.8430\n",
      "***\n",
      "Epoch: 50/99 Iteration: 4051 Training loss: 0.42614\n",
      "Epoch: 50/99 Iteration: 4052 Training loss: 0.37260\n",
      "Epoch: 50/99 Iteration: 4053 Training loss: 0.53948\n",
      "Epoch: 50/99 Iteration: 4054 Training loss: 0.40016\n",
      "Epoch: 50/99 Iteration: 4055 Training loss: 0.29958\n",
      "Epoch: 50/99 Iteration: 4056 Training loss: 0.48196\n",
      "Epoch: 50/99 Iteration: 4057 Training loss: 0.51910\n",
      "Epoch: 50/99 Iteration: 4058 Training loss: 0.38147\n",
      "Epoch: 50/99 Iteration: 4059 Training loss: 0.40734\n",
      "Epoch: 50/99 Iteration: 4060 Training loss: 0.35880\n",
      "Epoch: 50/99 Iteration: 4061 Training loss: 0.40322\n",
      "Epoch: 50/99 Iteration: 4062 Training loss: 0.31436\n",
      "Epoch: 50/99 Iteration: 4063 Training loss: 0.38760\n",
      "Epoch: 50/99 Iteration: 4064 Training loss: 0.33421\n",
      "Epoch: 50/99 Iteration: 4065 Training loss: 0.42533\n",
      "Epoch: 50/99 Iteration: 4066 Training loss: 0.34527\n",
      "Epoch: 50/99 Iteration: 4067 Training loss: 0.51545\n",
      "Epoch: 50/99 Iteration: 4068 Training loss: 0.42198\n",
      "Epoch: 50/99 Iteration: 4069 Training loss: 0.44421\n",
      "Epoch: 50/99 Iteration: 4070 Training loss: 0.47957\n",
      "Epoch: 50/99 Iteration: 4071 Training loss: 0.31408\n",
      "Epoch: 50/99 Iteration: 4072 Training loss: 0.35529\n",
      "Epoch: 50/99 Iteration: 4073 Training loss: 0.34273\n",
      "Epoch: 50/99 Iteration: 4074 Training loss: 0.18454\n",
      "Epoch: 50/99 Iteration: 4075 Training loss: 0.42136\n",
      "Epoch: 50/99 Iteration: 4076 Training loss: 0.48083\n",
      "Epoch: 50/99 Iteration: 4077 Training loss: 0.45230\n",
      "Epoch: 50/99 Iteration: 4078 Training loss: 0.48631\n",
      "Epoch: 50/99 Iteration: 4079 Training loss: 0.50113\n",
      "Epoch: 51/99 Iteration: 4080 Training loss: 0.26806\n",
      "Epoch: 51/99 Iteration: 4081 Training loss: 0.39786\n",
      "Epoch: 51/99 Iteration: 4082 Training loss: 0.41622\n",
      "Epoch: 51/99 Iteration: 4083 Training loss: 0.46693\n",
      "Epoch: 51/99 Iteration: 4084 Training loss: 0.39041\n",
      "Epoch: 51/99 Iteration: 4085 Training loss: 0.30411\n",
      "Epoch: 51/99 Iteration: 4086 Training loss: 0.36112\n",
      "Epoch: 51/99 Iteration: 4087 Training loss: 0.41648\n",
      "Epoch: 51/99 Iteration: 4088 Training loss: 0.48363\n",
      "Epoch: 51/99 Iteration: 4089 Training loss: 0.22832\n",
      "Epoch: 51/99 Iteration: 4090 Training loss: 0.35544\n",
      "Epoch: 51/99 Iteration: 4091 Training loss: 0.32211\n",
      "Epoch: 51/99 Iteration: 4092 Training loss: 0.37178\n",
      "Epoch: 51/99 Iteration: 4093 Training loss: 0.52125\n",
      "Epoch: 51/99 Iteration: 4094 Training loss: 0.28721\n",
      "Epoch: 51/99 Iteration: 4095 Training loss: 0.35076\n",
      "Epoch: 51/99 Iteration: 4096 Training loss: 0.33108\n",
      "Epoch: 51/99 Iteration: 4097 Training loss: 0.25295\n",
      "Epoch: 51/99 Iteration: 4098 Training loss: 0.39545\n",
      "Epoch: 51/99 Iteration: 4099 Training loss: 0.24620\n",
      "Epoch: 51/99 Iteration: 4100 Training loss: 0.38796\n",
      "***\n",
      "Epoch: 51/99 Iteration: 4100 Validation Acc: 0.8510\n",
      "***\n",
      "Epoch: 51/99 Iteration: 4101 Training loss: 0.31632\n",
      "Epoch: 51/99 Iteration: 4102 Training loss: 0.36007\n",
      "Epoch: 51/99 Iteration: 4103 Training loss: 0.36622\n",
      "Epoch: 51/99 Iteration: 4104 Training loss: 0.33388\n",
      "Epoch: 51/99 Iteration: 4105 Training loss: 0.33145\n",
      "Epoch: 51/99 Iteration: 4106 Training loss: 0.35086\n",
      "Epoch: 51/99 Iteration: 4107 Training loss: 0.31664\n",
      "Epoch: 51/99 Iteration: 4108 Training loss: 0.47897\n",
      "Epoch: 51/99 Iteration: 4109 Training loss: 0.55246\n",
      "Epoch: 51/99 Iteration: 4110 Training loss: 0.24019\n",
      "Epoch: 51/99 Iteration: 4111 Training loss: 0.44826\n",
      "Epoch: 51/99 Iteration: 4112 Training loss: 0.43323\n",
      "Epoch: 51/99 Iteration: 4113 Training loss: 0.33545\n",
      "Epoch: 51/99 Iteration: 4114 Training loss: 0.33711\n",
      "Epoch: 51/99 Iteration: 4115 Training loss: 0.32865\n",
      "Epoch: 51/99 Iteration: 4116 Training loss: 0.60656\n",
      "Epoch: 51/99 Iteration: 4117 Training loss: 0.32762\n",
      "Epoch: 51/99 Iteration: 4118 Training loss: 0.39128\n",
      "Epoch: 51/99 Iteration: 4119 Training loss: 0.35274\n",
      "Epoch: 51/99 Iteration: 4120 Training loss: 0.37969\n",
      "Epoch: 51/99 Iteration: 4121 Training loss: 0.42608\n",
      "Epoch: 51/99 Iteration: 4122 Training loss: 0.38133\n",
      "Epoch: 51/99 Iteration: 4123 Training loss: 0.35349\n",
      "Epoch: 51/99 Iteration: 4124 Training loss: 0.48052\n",
      "Epoch: 51/99 Iteration: 4125 Training loss: 0.30284\n",
      "Epoch: 51/99 Iteration: 4126 Training loss: 0.45705\n",
      "Epoch: 51/99 Iteration: 4127 Training loss: 0.46262\n",
      "Epoch: 51/99 Iteration: 4128 Training loss: 0.59916\n",
      "Epoch: 51/99 Iteration: 4129 Training loss: 0.27901\n",
      "Epoch: 51/99 Iteration: 4130 Training loss: 0.46638\n",
      "Epoch: 51/99 Iteration: 4131 Training loss: 0.59965\n",
      "Epoch: 51/99 Iteration: 4132 Training loss: 0.36595\n",
      "Epoch: 51/99 Iteration: 4133 Training loss: 0.50605\n",
      "Epoch: 51/99 Iteration: 4134 Training loss: 0.53716\n",
      "Epoch: 51/99 Iteration: 4135 Training loss: 0.33264\n",
      "Epoch: 51/99 Iteration: 4136 Training loss: 0.38106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/99 Iteration: 4137 Training loss: 0.51532\n",
      "Epoch: 51/99 Iteration: 4138 Training loss: 0.49866\n",
      "Epoch: 51/99 Iteration: 4139 Training loss: 0.41299\n",
      "Epoch: 51/99 Iteration: 4140 Training loss: 0.42837\n",
      "Epoch: 51/99 Iteration: 4141 Training loss: 0.42318\n",
      "Epoch: 51/99 Iteration: 4142 Training loss: 0.31412\n",
      "Epoch: 51/99 Iteration: 4143 Training loss: 0.47049\n",
      "Epoch: 51/99 Iteration: 4144 Training loss: 0.40667\n",
      "Epoch: 51/99 Iteration: 4145 Training loss: 0.43217\n",
      "Epoch: 51/99 Iteration: 4146 Training loss: 0.44403\n",
      "Epoch: 51/99 Iteration: 4147 Training loss: 0.58416\n",
      "Epoch: 51/99 Iteration: 4148 Training loss: 0.39488\n",
      "Epoch: 51/99 Iteration: 4149 Training loss: 0.47749\n",
      "Epoch: 51/99 Iteration: 4150 Training loss: 0.58793\n",
      "***\n",
      "Epoch: 51/99 Iteration: 4150 Validation Acc: 0.8280\n",
      "***\n",
      "Epoch: 51/99 Iteration: 4151 Training loss: 0.42125\n",
      "Epoch: 51/99 Iteration: 4152 Training loss: 0.51229\n",
      "Epoch: 51/99 Iteration: 4153 Training loss: 0.38675\n",
      "Epoch: 51/99 Iteration: 4154 Training loss: 0.39481\n",
      "Epoch: 51/99 Iteration: 4155 Training loss: 0.39568\n",
      "Epoch: 51/99 Iteration: 4156 Training loss: 0.56091\n",
      "Epoch: 51/99 Iteration: 4157 Training loss: 0.46386\n",
      "Epoch: 51/99 Iteration: 4158 Training loss: 0.46524\n",
      "Epoch: 51/99 Iteration: 4159 Training loss: 0.39669\n",
      "Epoch: 52/99 Iteration: 4160 Training loss: 0.26703\n",
      "Epoch: 52/99 Iteration: 4161 Training loss: 0.53857\n",
      "Epoch: 52/99 Iteration: 4162 Training loss: 0.45124\n",
      "Epoch: 52/99 Iteration: 4163 Training loss: 0.37615\n",
      "Epoch: 52/99 Iteration: 4164 Training loss: 0.30964\n",
      "Epoch: 52/99 Iteration: 4165 Training loss: 0.40711\n",
      "Epoch: 52/99 Iteration: 4166 Training loss: 0.40037\n",
      "Epoch: 52/99 Iteration: 4167 Training loss: 0.32507\n",
      "Epoch: 52/99 Iteration: 4168 Training loss: 0.46648\n",
      "Epoch: 52/99 Iteration: 4169 Training loss: 0.39318\n",
      "Epoch: 52/99 Iteration: 4170 Training loss: 0.26633\n",
      "Epoch: 52/99 Iteration: 4171 Training loss: 0.28842\n",
      "Epoch: 52/99 Iteration: 4172 Training loss: 0.28952\n",
      "Epoch: 52/99 Iteration: 4173 Training loss: 0.46044\n",
      "Epoch: 52/99 Iteration: 4174 Training loss: 0.31628\n",
      "Epoch: 52/99 Iteration: 4175 Training loss: 0.31008\n",
      "Epoch: 52/99 Iteration: 4176 Training loss: 0.24896\n",
      "Epoch: 52/99 Iteration: 4177 Training loss: 0.27165\n",
      "Epoch: 52/99 Iteration: 4178 Training loss: 0.47472\n",
      "Epoch: 52/99 Iteration: 4179 Training loss: 0.30353\n",
      "Epoch: 52/99 Iteration: 4180 Training loss: 0.28729\n",
      "Epoch: 52/99 Iteration: 4181 Training loss: 0.36725\n",
      "Epoch: 52/99 Iteration: 4182 Training loss: 0.34147\n",
      "Epoch: 52/99 Iteration: 4183 Training loss: 0.32072\n",
      "Epoch: 52/99 Iteration: 4184 Training loss: 0.22389\n",
      "Epoch: 52/99 Iteration: 4185 Training loss: 0.38883\n",
      "Epoch: 52/99 Iteration: 4186 Training loss: 0.30852\n",
      "Epoch: 52/99 Iteration: 4187 Training loss: 0.22691\n",
      "Epoch: 52/99 Iteration: 4188 Training loss: 0.39221\n",
      "Epoch: 52/99 Iteration: 4189 Training loss: 0.48776\n",
      "Epoch: 52/99 Iteration: 4190 Training loss: 0.25759\n",
      "Epoch: 52/99 Iteration: 4191 Training loss: 0.42938\n",
      "Epoch: 52/99 Iteration: 4192 Training loss: 0.43196\n",
      "Epoch: 52/99 Iteration: 4193 Training loss: 0.40793\n",
      "Epoch: 52/99 Iteration: 4194 Training loss: 0.37845\n",
      "Epoch: 52/99 Iteration: 4195 Training loss: 0.40310\n",
      "Epoch: 52/99 Iteration: 4196 Training loss: 0.49517\n",
      "Epoch: 52/99 Iteration: 4197 Training loss: 0.16719\n",
      "Epoch: 52/99 Iteration: 4198 Training loss: 0.26940\n",
      "Epoch: 52/99 Iteration: 4199 Training loss: 0.38476\n",
      "Epoch: 52/99 Iteration: 4200 Training loss: 0.28249\n",
      "***\n",
      "Epoch: 52/99 Iteration: 4200 Validation Acc: 0.8500\n",
      "***\n",
      "Epoch: 52/99 Iteration: 4201 Training loss: 0.38325\n",
      "Epoch: 52/99 Iteration: 4202 Training loss: 0.46740\n",
      "Epoch: 52/99 Iteration: 4203 Training loss: 0.41613\n",
      "Epoch: 52/99 Iteration: 4204 Training loss: 0.47572\n",
      "Epoch: 52/99 Iteration: 4205 Training loss: 0.37582\n",
      "Epoch: 52/99 Iteration: 4206 Training loss: 0.40057\n",
      "Epoch: 52/99 Iteration: 4207 Training loss: 0.51453\n",
      "Epoch: 52/99 Iteration: 4208 Training loss: 0.54706\n",
      "Epoch: 52/99 Iteration: 4209 Training loss: 0.29141\n",
      "Epoch: 52/99 Iteration: 4210 Training loss: 0.36125\n",
      "Epoch: 52/99 Iteration: 4211 Training loss: 0.38136\n",
      "Epoch: 52/99 Iteration: 4212 Training loss: 0.34179\n",
      "Epoch: 52/99 Iteration: 4213 Training loss: 0.40841\n",
      "Epoch: 52/99 Iteration: 4214 Training loss: 0.41097\n",
      "Epoch: 52/99 Iteration: 4215 Training loss: 0.33517\n",
      "Epoch: 52/99 Iteration: 4216 Training loss: 0.41116\n",
      "Epoch: 52/99 Iteration: 4217 Training loss: 0.27882\n",
      "Epoch: 52/99 Iteration: 4218 Training loss: 0.50204\n",
      "Epoch: 52/99 Iteration: 4219 Training loss: 0.45205\n",
      "Epoch: 52/99 Iteration: 4220 Training loss: 0.33093\n",
      "Epoch: 52/99 Iteration: 4221 Training loss: 0.45542\n",
      "Epoch: 52/99 Iteration: 4222 Training loss: 0.28076\n",
      "Epoch: 52/99 Iteration: 4223 Training loss: 0.39988\n",
      "Epoch: 52/99 Iteration: 4224 Training loss: 0.39181\n",
      "Epoch: 52/99 Iteration: 4225 Training loss: 0.52032\n",
      "Epoch: 52/99 Iteration: 4226 Training loss: 0.39930\n",
      "Epoch: 52/99 Iteration: 4227 Training loss: 0.41875\n",
      "Epoch: 52/99 Iteration: 4228 Training loss: 0.36243\n",
      "Epoch: 52/99 Iteration: 4229 Training loss: 0.37644\n",
      "Epoch: 52/99 Iteration: 4230 Training loss: 0.50820\n",
      "Epoch: 52/99 Iteration: 4231 Training loss: 0.46281\n",
      "Epoch: 52/99 Iteration: 4232 Training loss: 0.46042\n",
      "Epoch: 52/99 Iteration: 4233 Training loss: 0.36571\n",
      "Epoch: 52/99 Iteration: 4234 Training loss: 0.41622\n",
      "Epoch: 52/99 Iteration: 4235 Training loss: 0.30880\n",
      "Epoch: 52/99 Iteration: 4236 Training loss: 0.41695\n",
      "Epoch: 52/99 Iteration: 4237 Training loss: 0.38610\n",
      "Epoch: 52/99 Iteration: 4238 Training loss: 0.33119\n",
      "Epoch: 52/99 Iteration: 4239 Training loss: 0.43723\n",
      "Epoch: 53/99 Iteration: 4240 Training loss: 0.36275\n",
      "Epoch: 53/99 Iteration: 4241 Training loss: 0.46844\n",
      "Epoch: 53/99 Iteration: 4242 Training loss: 0.38926\n",
      "Epoch: 53/99 Iteration: 4243 Training loss: 0.63471\n",
      "Epoch: 53/99 Iteration: 4244 Training loss: 0.33584\n",
      "Epoch: 53/99 Iteration: 4245 Training loss: 0.39033\n",
      "Epoch: 53/99 Iteration: 4246 Training loss: 0.34011\n",
      "Epoch: 53/99 Iteration: 4247 Training loss: 0.45984\n",
      "Epoch: 53/99 Iteration: 4248 Training loss: 0.46043\n",
      "Epoch: 53/99 Iteration: 4249 Training loss: 0.27257\n",
      "Epoch: 53/99 Iteration: 4250 Training loss: 0.42300\n",
      "***\n",
      "Epoch: 53/99 Iteration: 4250 Validation Acc: 0.8330\n",
      "***\n",
      "Epoch: 53/99 Iteration: 4251 Training loss: 0.28135\n",
      "Epoch: 53/99 Iteration: 4252 Training loss: 0.40055\n",
      "Epoch: 53/99 Iteration: 4253 Training loss: 0.47108\n",
      "Epoch: 53/99 Iteration: 4254 Training loss: 0.39594\n",
      "Epoch: 53/99 Iteration: 4255 Training loss: 0.39652\n",
      "Epoch: 53/99 Iteration: 4256 Training loss: 0.31219\n",
      "Epoch: 53/99 Iteration: 4257 Training loss: 0.26047\n",
      "Epoch: 53/99 Iteration: 4258 Training loss: 0.47739\n",
      "Epoch: 53/99 Iteration: 4259 Training loss: 0.38739\n",
      "Epoch: 53/99 Iteration: 4260 Training loss: 0.45112\n",
      "Epoch: 53/99 Iteration: 4261 Training loss: 0.29360\n",
      "Epoch: 53/99 Iteration: 4262 Training loss: 0.44419\n",
      "Epoch: 53/99 Iteration: 4263 Training loss: 0.43912\n",
      "Epoch: 53/99 Iteration: 4264 Training loss: 0.35221\n",
      "Epoch: 53/99 Iteration: 4265 Training loss: 0.40781\n",
      "Epoch: 53/99 Iteration: 4266 Training loss: 0.33999\n",
      "Epoch: 53/99 Iteration: 4267 Training loss: 0.32391\n",
      "Epoch: 53/99 Iteration: 4268 Training loss: 0.50769\n",
      "Epoch: 53/99 Iteration: 4269 Training loss: 0.53774\n",
      "Epoch: 53/99 Iteration: 4270 Training loss: 0.34945\n",
      "Epoch: 53/99 Iteration: 4271 Training loss: 0.35487\n",
      "Epoch: 53/99 Iteration: 4272 Training loss: 0.60012\n",
      "Epoch: 53/99 Iteration: 4273 Training loss: 0.34991\n",
      "Epoch: 53/99 Iteration: 4274 Training loss: 0.40288\n",
      "Epoch: 53/99 Iteration: 4275 Training loss: 0.39448\n",
      "Epoch: 53/99 Iteration: 4276 Training loss: 0.65185\n",
      "Epoch: 53/99 Iteration: 4277 Training loss: 0.36956\n",
      "Epoch: 53/99 Iteration: 4278 Training loss: 0.40660\n",
      "Epoch: 53/99 Iteration: 4279 Training loss: 0.38623\n",
      "Epoch: 53/99 Iteration: 4280 Training loss: 0.48053\n",
      "Epoch: 53/99 Iteration: 4281 Training loss: 0.57630\n",
      "Epoch: 53/99 Iteration: 4282 Training loss: 0.48440\n",
      "Epoch: 53/99 Iteration: 4283 Training loss: 0.29713\n",
      "Epoch: 53/99 Iteration: 4284 Training loss: 0.52629\n",
      "Epoch: 53/99 Iteration: 4285 Training loss: 0.33207\n",
      "Epoch: 53/99 Iteration: 4286 Training loss: 0.51041\n",
      "Epoch: 53/99 Iteration: 4287 Training loss: 0.41797\n",
      "Epoch: 53/99 Iteration: 4288 Training loss: 0.55448\n",
      "Epoch: 53/99 Iteration: 4289 Training loss: 0.44660\n",
      "Epoch: 53/99 Iteration: 4290 Training loss: 0.41305\n",
      "Epoch: 53/99 Iteration: 4291 Training loss: 0.55610\n",
      "Epoch: 53/99 Iteration: 4292 Training loss: 0.45676\n",
      "Epoch: 53/99 Iteration: 4293 Training loss: 0.47178\n",
      "Epoch: 53/99 Iteration: 4294 Training loss: 0.51245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/99 Iteration: 4295 Training loss: 0.28622\n",
      "Epoch: 53/99 Iteration: 4296 Training loss: 0.34908\n",
      "Epoch: 53/99 Iteration: 4297 Training loss: 0.37986\n",
      "Epoch: 53/99 Iteration: 4298 Training loss: 0.54451\n",
      "Epoch: 53/99 Iteration: 4299 Training loss: 0.60793\n",
      "Epoch: 53/99 Iteration: 4300 Training loss: 0.42950\n",
      "***\n",
      "Epoch: 53/99 Iteration: 4300 Validation Acc: 0.8360\n",
      "***\n",
      "Epoch: 53/99 Iteration: 4301 Training loss: 0.47212\n",
      "Epoch: 53/99 Iteration: 4302 Training loss: 0.45841\n",
      "Epoch: 53/99 Iteration: 4303 Training loss: 0.41649\n",
      "Epoch: 53/99 Iteration: 4304 Training loss: 0.41502\n",
      "Epoch: 53/99 Iteration: 4305 Training loss: 0.46871\n",
      "Epoch: 53/99 Iteration: 4306 Training loss: 0.49220\n",
      "Epoch: 53/99 Iteration: 4307 Training loss: 0.44481\n",
      "Epoch: 53/99 Iteration: 4308 Training loss: 0.33236\n",
      "Epoch: 53/99 Iteration: 4309 Training loss: 0.47188\n",
      "Epoch: 53/99 Iteration: 4310 Training loss: 0.57341\n",
      "Epoch: 53/99 Iteration: 4311 Training loss: 0.42732\n",
      "Epoch: 53/99 Iteration: 4312 Training loss: 0.49391\n",
      "Epoch: 53/99 Iteration: 4313 Training loss: 0.24192\n",
      "Epoch: 53/99 Iteration: 4314 Training loss: 0.50396\n",
      "Epoch: 53/99 Iteration: 4315 Training loss: 0.27683\n",
      "Epoch: 53/99 Iteration: 4316 Training loss: 0.47233\n",
      "Epoch: 53/99 Iteration: 4317 Training loss: 0.59314\n",
      "Epoch: 53/99 Iteration: 4318 Training loss: 0.52100\n",
      "Epoch: 53/99 Iteration: 4319 Training loss: 0.46058\n",
      "Epoch: 54/99 Iteration: 4320 Training loss: 0.35225\n",
      "Epoch: 54/99 Iteration: 4321 Training loss: 0.54550\n",
      "Epoch: 54/99 Iteration: 4322 Training loss: 0.46913\n",
      "Epoch: 54/99 Iteration: 4323 Training loss: 0.46326\n",
      "Epoch: 54/99 Iteration: 4324 Training loss: 0.37742\n",
      "Epoch: 54/99 Iteration: 4325 Training loss: 0.35005\n",
      "Epoch: 54/99 Iteration: 4326 Training loss: 0.45114\n",
      "Epoch: 54/99 Iteration: 4327 Training loss: 0.31940\n",
      "Epoch: 54/99 Iteration: 4328 Training loss: 0.57398\n",
      "Epoch: 54/99 Iteration: 4329 Training loss: 0.40699\n",
      "Epoch: 54/99 Iteration: 4330 Training loss: 0.36787\n",
      "Epoch: 54/99 Iteration: 4331 Training loss: 0.43916\n",
      "Epoch: 54/99 Iteration: 4332 Training loss: 0.38541\n",
      "Epoch: 54/99 Iteration: 4333 Training loss: 0.55782\n",
      "Epoch: 54/99 Iteration: 4334 Training loss: 0.49096\n",
      "Epoch: 54/99 Iteration: 4335 Training loss: 0.34876\n",
      "Epoch: 54/99 Iteration: 4336 Training loss: 0.39160\n",
      "Epoch: 54/99 Iteration: 4337 Training loss: 0.34909\n",
      "Epoch: 54/99 Iteration: 4338 Training loss: 0.34571\n",
      "Epoch: 54/99 Iteration: 4339 Training loss: 0.58178\n",
      "Epoch: 54/99 Iteration: 4340 Training loss: 0.30546\n",
      "Epoch: 54/99 Iteration: 4341 Training loss: 0.51564\n",
      "Epoch: 54/99 Iteration: 4342 Training loss: 0.57630\n",
      "Epoch: 54/99 Iteration: 4343 Training loss: 0.40051\n",
      "Epoch: 54/99 Iteration: 4344 Training loss: 0.36823\n",
      "Epoch: 54/99 Iteration: 4345 Training loss: 0.61234\n",
      "Epoch: 54/99 Iteration: 4346 Training loss: 0.41697\n",
      "Epoch: 54/99 Iteration: 4347 Training loss: 0.43237\n",
      "Epoch: 54/99 Iteration: 4348 Training loss: 0.49357\n",
      "Epoch: 54/99 Iteration: 4349 Training loss: 0.57320\n",
      "Epoch: 54/99 Iteration: 4350 Training loss: 0.30288\n",
      "***\n",
      "Epoch: 54/99 Iteration: 4350 Validation Acc: 0.8300\n",
      "***\n",
      "Epoch: 54/99 Iteration: 4351 Training loss: 0.34928\n",
      "Epoch: 54/99 Iteration: 4352 Training loss: 0.52033\n",
      "Epoch: 54/99 Iteration: 4353 Training loss: 0.48201\n",
      "Epoch: 54/99 Iteration: 4354 Training loss: 0.38317\n",
      "Epoch: 54/99 Iteration: 4355 Training loss: 0.37270\n",
      "Epoch: 54/99 Iteration: 4356 Training loss: 0.44716\n",
      "Epoch: 54/99 Iteration: 4357 Training loss: 0.34991\n",
      "Epoch: 54/99 Iteration: 4358 Training loss: 0.31521\n",
      "Epoch: 54/99 Iteration: 4359 Training loss: 0.36936\n",
      "Epoch: 54/99 Iteration: 4360 Training loss: 0.47902\n",
      "Epoch: 54/99 Iteration: 4361 Training loss: 0.42850\n",
      "Epoch: 54/99 Iteration: 4362 Training loss: 0.35380\n",
      "Epoch: 54/99 Iteration: 4363 Training loss: 0.38448\n",
      "Epoch: 54/99 Iteration: 4364 Training loss: 0.42266\n",
      "Epoch: 54/99 Iteration: 4365 Training loss: 0.36902\n",
      "Epoch: 54/99 Iteration: 4366 Training loss: 0.50215\n",
      "Epoch: 54/99 Iteration: 4367 Training loss: 0.41723\n",
      "Epoch: 54/99 Iteration: 4368 Training loss: 0.54152\n",
      "Epoch: 54/99 Iteration: 4369 Training loss: 0.38742\n",
      "Epoch: 54/99 Iteration: 4370 Training loss: 0.36372\n",
      "Epoch: 54/99 Iteration: 4371 Training loss: 0.43166\n",
      "Epoch: 54/99 Iteration: 4372 Training loss: 0.40159\n",
      "Epoch: 54/99 Iteration: 4373 Training loss: 0.48999\n",
      "Epoch: 54/99 Iteration: 4374 Training loss: 0.47769\n",
      "Epoch: 54/99 Iteration: 4375 Training loss: 0.31707\n",
      "Epoch: 54/99 Iteration: 4376 Training loss: 0.34065\n",
      "Epoch: 54/99 Iteration: 4377 Training loss: 0.42876\n",
      "Epoch: 54/99 Iteration: 4378 Training loss: 0.53578\n",
      "Epoch: 54/99 Iteration: 4379 Training loss: 0.29995\n",
      "Epoch: 54/99 Iteration: 4380 Training loss: 0.43347\n",
      "Epoch: 54/99 Iteration: 4381 Training loss: 0.35157\n",
      "Epoch: 54/99 Iteration: 4382 Training loss: 0.39156\n",
      "Epoch: 54/99 Iteration: 4383 Training loss: 0.42767\n",
      "Epoch: 54/99 Iteration: 4384 Training loss: 0.39590\n",
      "Epoch: 54/99 Iteration: 4385 Training loss: 0.42477\n",
      "Epoch: 54/99 Iteration: 4386 Training loss: 0.43437\n",
      "Epoch: 54/99 Iteration: 4387 Training loss: 0.49997\n",
      "Epoch: 54/99 Iteration: 4388 Training loss: 0.35050\n",
      "Epoch: 54/99 Iteration: 4389 Training loss: 0.30564\n",
      "Epoch: 54/99 Iteration: 4390 Training loss: 0.42485\n",
      "Epoch: 54/99 Iteration: 4391 Training loss: 0.36782\n",
      "Epoch: 54/99 Iteration: 4392 Training loss: 0.49067\n",
      "Epoch: 54/99 Iteration: 4393 Training loss: 0.34152\n",
      "Epoch: 54/99 Iteration: 4394 Training loss: 0.40480\n",
      "Epoch: 54/99 Iteration: 4395 Training loss: 0.29079\n",
      "Epoch: 54/99 Iteration: 4396 Training loss: 0.30406\n",
      "Epoch: 54/99 Iteration: 4397 Training loss: 0.44090\n",
      "Epoch: 54/99 Iteration: 4398 Training loss: 0.41194\n",
      "Epoch: 54/99 Iteration: 4399 Training loss: 0.39438\n",
      "Epoch: 55/99 Iteration: 4400 Training loss: 0.27739\n",
      "***\n",
      "Epoch: 55/99 Iteration: 4400 Validation Acc: 0.8440\n",
      "***\n",
      "Epoch: 55/99 Iteration: 4401 Training loss: 0.57158\n",
      "Epoch: 55/99 Iteration: 4402 Training loss: 0.32180\n",
      "Epoch: 55/99 Iteration: 4403 Training loss: 0.48726\n",
      "Epoch: 55/99 Iteration: 4404 Training loss: 0.34437\n",
      "Epoch: 55/99 Iteration: 4405 Training loss: 0.44949\n",
      "Epoch: 55/99 Iteration: 4406 Training loss: 0.48048\n",
      "Epoch: 55/99 Iteration: 4407 Training loss: 0.40750\n",
      "Epoch: 55/99 Iteration: 4408 Training loss: 0.41876\n",
      "Epoch: 55/99 Iteration: 4409 Training loss: 0.38169\n",
      "Epoch: 55/99 Iteration: 4410 Training loss: 0.31856\n",
      "Epoch: 55/99 Iteration: 4411 Training loss: 0.24091\n",
      "Epoch: 55/99 Iteration: 4412 Training loss: 0.32954\n",
      "Epoch: 55/99 Iteration: 4413 Training loss: 0.36748\n",
      "Epoch: 55/99 Iteration: 4414 Training loss: 0.41657\n",
      "Epoch: 55/99 Iteration: 4415 Training loss: 0.29711\n",
      "Epoch: 55/99 Iteration: 4416 Training loss: 0.29405\n",
      "Epoch: 55/99 Iteration: 4417 Training loss: 0.48458\n",
      "Epoch: 55/99 Iteration: 4418 Training loss: 0.45139\n",
      "Epoch: 55/99 Iteration: 4419 Training loss: 0.40240\n",
      "Epoch: 55/99 Iteration: 4420 Training loss: 0.31686\n",
      "Epoch: 55/99 Iteration: 4421 Training loss: 0.39268\n",
      "Epoch: 55/99 Iteration: 4422 Training loss: 0.46801\n",
      "Epoch: 55/99 Iteration: 4423 Training loss: 0.34344\n",
      "Epoch: 55/99 Iteration: 4424 Training loss: 0.30161\n",
      "Epoch: 55/99 Iteration: 4425 Training loss: 0.29965\n",
      "Epoch: 55/99 Iteration: 4426 Training loss: 0.42279\n",
      "Epoch: 55/99 Iteration: 4427 Training loss: 0.29762\n",
      "Epoch: 55/99 Iteration: 4428 Training loss: 0.55046\n",
      "Epoch: 55/99 Iteration: 4429 Training loss: 0.48049\n",
      "Epoch: 55/99 Iteration: 4430 Training loss: 0.30397\n",
      "Epoch: 55/99 Iteration: 4431 Training loss: 0.42818\n",
      "Epoch: 55/99 Iteration: 4432 Training loss: 0.26649\n",
      "Epoch: 55/99 Iteration: 4433 Training loss: 0.39344\n",
      "Epoch: 55/99 Iteration: 4434 Training loss: 0.48419\n",
      "Epoch: 55/99 Iteration: 4435 Training loss: 0.39626\n",
      "Epoch: 55/99 Iteration: 4436 Training loss: 0.40002\n",
      "Epoch: 55/99 Iteration: 4437 Training loss: 0.31929\n",
      "Epoch: 55/99 Iteration: 4438 Training loss: 0.27273\n",
      "Epoch: 55/99 Iteration: 4439 Training loss: 0.33560\n",
      "Epoch: 55/99 Iteration: 4440 Training loss: 0.39386\n",
      "Epoch: 55/99 Iteration: 4441 Training loss: 0.37693\n",
      "Epoch: 55/99 Iteration: 4442 Training loss: 0.37817\n",
      "Epoch: 55/99 Iteration: 4443 Training loss: 0.23429\n",
      "Epoch: 55/99 Iteration: 4444 Training loss: 0.30944\n",
      "Epoch: 55/99 Iteration: 4445 Training loss: 0.23941\n",
      "Epoch: 55/99 Iteration: 4446 Training loss: 0.47792\n",
      "Epoch: 55/99 Iteration: 4447 Training loss: 0.33651\n",
      "Epoch: 55/99 Iteration: 4448 Training loss: 0.46565\n",
      "Epoch: 55/99 Iteration: 4449 Training loss: 0.23842\n",
      "Epoch: 55/99 Iteration: 4450 Training loss: 0.30007\n",
      "***\n",
      "Epoch: 55/99 Iteration: 4450 Validation Acc: 0.8590\n",
      "***\n",
      "Epoch: 55/99 Iteration: 4451 Training loss: 0.30514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/99 Iteration: 4452 Training loss: 0.24647\n",
      "Epoch: 55/99 Iteration: 4453 Training loss: 0.42920\n",
      "Epoch: 55/99 Iteration: 4454 Training loss: 0.49772\n",
      "Epoch: 55/99 Iteration: 4455 Training loss: 0.31374\n",
      "Epoch: 55/99 Iteration: 4456 Training loss: 0.45189\n",
      "Epoch: 55/99 Iteration: 4457 Training loss: 0.35466\n",
      "Epoch: 55/99 Iteration: 4458 Training loss: 0.30213\n",
      "Epoch: 55/99 Iteration: 4459 Training loss: 0.43851\n",
      "Epoch: 55/99 Iteration: 4460 Training loss: 0.28431\n",
      "Epoch: 55/99 Iteration: 4461 Training loss: 0.35206\n",
      "Epoch: 55/99 Iteration: 4462 Training loss: 0.38385\n",
      "Epoch: 55/99 Iteration: 4463 Training loss: 0.36859\n",
      "Epoch: 55/99 Iteration: 4464 Training loss: 0.28608\n",
      "Epoch: 55/99 Iteration: 4465 Training loss: 0.46895\n",
      "Epoch: 55/99 Iteration: 4466 Training loss: 0.40303\n",
      "Epoch: 55/99 Iteration: 4467 Training loss: 0.38800\n",
      "Epoch: 55/99 Iteration: 4468 Training loss: 0.26372\n",
      "Epoch: 55/99 Iteration: 4469 Training loss: 0.36422\n",
      "Epoch: 55/99 Iteration: 4470 Training loss: 0.55180\n",
      "Epoch: 55/99 Iteration: 4471 Training loss: 0.33901\n",
      "Epoch: 55/99 Iteration: 4472 Training loss: 0.49991\n",
      "Epoch: 55/99 Iteration: 4473 Training loss: 0.34675\n",
      "Epoch: 55/99 Iteration: 4474 Training loss: 0.36520\n",
      "Epoch: 55/99 Iteration: 4475 Training loss: 0.37447\n",
      "Epoch: 55/99 Iteration: 4476 Training loss: 0.36039\n",
      "Epoch: 55/99 Iteration: 4477 Training loss: 0.51724\n",
      "Epoch: 55/99 Iteration: 4478 Training loss: 0.52456\n",
      "Epoch: 55/99 Iteration: 4479 Training loss: 0.43714\n",
      "Epoch: 56/99 Iteration: 4480 Training loss: 0.25053\n",
      "Epoch: 56/99 Iteration: 4481 Training loss: 0.46489\n",
      "Epoch: 56/99 Iteration: 4482 Training loss: 0.47093\n",
      "Epoch: 56/99 Iteration: 4483 Training loss: 0.31979\n",
      "Epoch: 56/99 Iteration: 4484 Training loss: 0.25813\n",
      "Epoch: 56/99 Iteration: 4485 Training loss: 0.36964\n",
      "Epoch: 56/99 Iteration: 4486 Training loss: 0.44590\n",
      "Epoch: 56/99 Iteration: 4487 Training loss: 0.49594\n",
      "Epoch: 56/99 Iteration: 4488 Training loss: 0.38657\n",
      "Epoch: 56/99 Iteration: 4489 Training loss: 0.32432\n",
      "Epoch: 56/99 Iteration: 4490 Training loss: 0.25132\n",
      "Epoch: 56/99 Iteration: 4491 Training loss: 0.37801\n",
      "Epoch: 56/99 Iteration: 4492 Training loss: 0.27401\n",
      "Epoch: 56/99 Iteration: 4493 Training loss: 0.44087\n",
      "Epoch: 56/99 Iteration: 4494 Training loss: 0.35121\n",
      "Epoch: 56/99 Iteration: 4495 Training loss: 0.30985\n",
      "Epoch: 56/99 Iteration: 4496 Training loss: 0.25437\n",
      "Epoch: 56/99 Iteration: 4497 Training loss: 0.34710\n",
      "Epoch: 56/99 Iteration: 4498 Training loss: 0.44487\n",
      "Epoch: 56/99 Iteration: 4499 Training loss: 0.29064\n",
      "Epoch: 56/99 Iteration: 4500 Training loss: 0.43110\n",
      "***\n",
      "Epoch: 56/99 Iteration: 4500 Validation Acc: 0.8570\n",
      "***\n",
      "Epoch: 56/99 Iteration: 4501 Training loss: 0.43174\n",
      "Epoch: 56/99 Iteration: 4502 Training loss: 0.31719\n",
      "Epoch: 56/99 Iteration: 4503 Training loss: 0.28713\n",
      "Epoch: 56/99 Iteration: 4504 Training loss: 0.33591\n",
      "Epoch: 56/99 Iteration: 4505 Training loss: 0.30905\n",
      "Epoch: 56/99 Iteration: 4506 Training loss: 0.50703\n",
      "Epoch: 56/99 Iteration: 4507 Training loss: 0.37937\n",
      "Epoch: 56/99 Iteration: 4508 Training loss: 0.51582\n",
      "Epoch: 56/99 Iteration: 4509 Training loss: 0.39935\n",
      "Epoch: 56/99 Iteration: 4510 Training loss: 0.31295\n",
      "Epoch: 56/99 Iteration: 4511 Training loss: 0.30424\n",
      "Epoch: 56/99 Iteration: 4512 Training loss: 0.47682\n",
      "Epoch: 56/99 Iteration: 4513 Training loss: 0.38176\n",
      "Epoch: 56/99 Iteration: 4514 Training loss: 0.33983\n",
      "Epoch: 56/99 Iteration: 4515 Training loss: 0.33035\n",
      "Epoch: 56/99 Iteration: 4516 Training loss: 0.63023\n",
      "Epoch: 56/99 Iteration: 4517 Training loss: 0.19086\n",
      "Epoch: 56/99 Iteration: 4518 Training loss: 0.35758\n",
      "Epoch: 56/99 Iteration: 4519 Training loss: 0.37280\n",
      "Epoch: 56/99 Iteration: 4520 Training loss: 0.28657\n",
      "Epoch: 56/99 Iteration: 4521 Training loss: 0.35195\n",
      "Epoch: 56/99 Iteration: 4522 Training loss: 0.35285\n",
      "Epoch: 56/99 Iteration: 4523 Training loss: 0.29746\n",
      "Epoch: 56/99 Iteration: 4524 Training loss: 0.28846\n",
      "Epoch: 56/99 Iteration: 4525 Training loss: 0.40195\n",
      "Epoch: 56/99 Iteration: 4526 Training loss: 0.52027\n",
      "Epoch: 56/99 Iteration: 4527 Training loss: 0.42795\n",
      "Epoch: 56/99 Iteration: 4528 Training loss: 0.40215\n",
      "Epoch: 56/99 Iteration: 4529 Training loss: 0.34712\n",
      "Epoch: 56/99 Iteration: 4530 Training loss: 0.31015\n",
      "Epoch: 56/99 Iteration: 4531 Training loss: 0.43231\n",
      "Epoch: 56/99 Iteration: 4532 Training loss: 0.34096\n",
      "Epoch: 56/99 Iteration: 4533 Training loss: 0.31502\n",
      "Epoch: 56/99 Iteration: 4534 Training loss: 0.46326\n",
      "Epoch: 56/99 Iteration: 4535 Training loss: 0.36640\n",
      "Epoch: 56/99 Iteration: 4536 Training loss: 0.29637\n",
      "Epoch: 56/99 Iteration: 4537 Training loss: 0.36631\n",
      "Epoch: 56/99 Iteration: 4538 Training loss: 0.26931\n",
      "Epoch: 56/99 Iteration: 4539 Training loss: 0.51526\n",
      "Epoch: 56/99 Iteration: 4540 Training loss: 0.35465\n",
      "Epoch: 56/99 Iteration: 4541 Training loss: 0.36741\n",
      "Epoch: 56/99 Iteration: 4542 Training loss: 0.27602\n",
      "Epoch: 56/99 Iteration: 4543 Training loss: 0.34598\n",
      "Epoch: 56/99 Iteration: 4544 Training loss: 0.39773\n",
      "Epoch: 56/99 Iteration: 4545 Training loss: 0.51937\n",
      "Epoch: 56/99 Iteration: 4546 Training loss: 0.43958\n",
      "Epoch: 56/99 Iteration: 4547 Training loss: 0.40636\n",
      "Epoch: 56/99 Iteration: 4548 Training loss: 0.40311\n",
      "Epoch: 56/99 Iteration: 4549 Training loss: 0.40100\n",
      "Epoch: 56/99 Iteration: 4550 Training loss: 0.43809\n",
      "***\n",
      "Epoch: 56/99 Iteration: 4550 Validation Acc: 0.8640\n",
      "***\n",
      "Epoch: 56/99 Iteration: 4551 Training loss: 0.38178\n",
      "Epoch: 56/99 Iteration: 4552 Training loss: 0.35536\n",
      "Epoch: 56/99 Iteration: 4553 Training loss: 0.34587\n",
      "Epoch: 56/99 Iteration: 4554 Training loss: 0.29473\n",
      "Epoch: 56/99 Iteration: 4555 Training loss: 0.26276\n",
      "Epoch: 56/99 Iteration: 4556 Training loss: 0.45924\n",
      "Epoch: 56/99 Iteration: 4557 Training loss: 0.41416\n",
      "Epoch: 56/99 Iteration: 4558 Training loss: 0.38935\n",
      "Epoch: 56/99 Iteration: 4559 Training loss: 0.36656\n",
      "Epoch: 57/99 Iteration: 4560 Training loss: 0.35045\n",
      "Epoch: 57/99 Iteration: 4561 Training loss: 0.41560\n",
      "Epoch: 57/99 Iteration: 4562 Training loss: 0.32998\n",
      "Epoch: 57/99 Iteration: 4563 Training loss: 0.24064\n",
      "Epoch: 57/99 Iteration: 4564 Training loss: 0.30918\n",
      "Epoch: 57/99 Iteration: 4565 Training loss: 0.38618\n",
      "Epoch: 57/99 Iteration: 4566 Training loss: 0.28130\n",
      "Epoch: 57/99 Iteration: 4567 Training loss: 0.32402\n",
      "Epoch: 57/99 Iteration: 4568 Training loss: 0.49626\n",
      "Epoch: 57/99 Iteration: 4569 Training loss: 0.29925\n",
      "Epoch: 57/99 Iteration: 4570 Training loss: 0.39459\n",
      "Epoch: 57/99 Iteration: 4571 Training loss: 0.27312\n",
      "Epoch: 57/99 Iteration: 4572 Training loss: 0.35041\n",
      "Epoch: 57/99 Iteration: 4573 Training loss: 0.36206\n",
      "Epoch: 57/99 Iteration: 4574 Training loss: 0.30822\n",
      "Epoch: 57/99 Iteration: 4575 Training loss: 0.28550\n",
      "Epoch: 57/99 Iteration: 4576 Training loss: 0.29135\n",
      "Epoch: 57/99 Iteration: 4577 Training loss: 0.30198\n",
      "Epoch: 57/99 Iteration: 4578 Training loss: 0.35434\n",
      "Epoch: 57/99 Iteration: 4579 Training loss: 0.28225\n",
      "Epoch: 57/99 Iteration: 4580 Training loss: 0.29571\n",
      "Epoch: 57/99 Iteration: 4581 Training loss: 0.28928\n",
      "Epoch: 57/99 Iteration: 4582 Training loss: 0.36281\n",
      "Epoch: 57/99 Iteration: 4583 Training loss: 0.42741\n",
      "Epoch: 57/99 Iteration: 4584 Training loss: 0.34089\n",
      "Epoch: 57/99 Iteration: 4585 Training loss: 0.31165\n",
      "Epoch: 57/99 Iteration: 4586 Training loss: 0.37560\n",
      "Epoch: 57/99 Iteration: 4587 Training loss: 0.24153\n",
      "Epoch: 57/99 Iteration: 4588 Training loss: 0.26123\n",
      "Epoch: 57/99 Iteration: 4589 Training loss: 0.46138\n",
      "Epoch: 57/99 Iteration: 4590 Training loss: 0.34202\n",
      "Epoch: 57/99 Iteration: 4591 Training loss: 0.34337\n",
      "Epoch: 57/99 Iteration: 4592 Training loss: 0.46806\n",
      "Epoch: 57/99 Iteration: 4593 Training loss: 0.26802\n",
      "Epoch: 57/99 Iteration: 4594 Training loss: 0.34046\n",
      "Epoch: 57/99 Iteration: 4595 Training loss: 0.26790\n",
      "Epoch: 57/99 Iteration: 4596 Training loss: 0.46286\n",
      "Epoch: 57/99 Iteration: 4597 Training loss: 0.25468\n",
      "Epoch: 57/99 Iteration: 4598 Training loss: 0.40859\n",
      "Epoch: 57/99 Iteration: 4599 Training loss: 0.19575\n",
      "Epoch: 57/99 Iteration: 4600 Training loss: 0.36230\n",
      "***\n",
      "Epoch: 57/99 Iteration: 4600 Validation Acc: 0.8560\n",
      "***\n",
      "Epoch: 57/99 Iteration: 4601 Training loss: 0.36835\n",
      "Epoch: 57/99 Iteration: 4602 Training loss: 0.31115\n",
      "Epoch: 57/99 Iteration: 4603 Training loss: 0.37685\n",
      "Epoch: 57/99 Iteration: 4604 Training loss: 0.44666\n",
      "Epoch: 57/99 Iteration: 4605 Training loss: 0.27758\n",
      "Epoch: 57/99 Iteration: 4606 Training loss: 0.46384\n",
      "Epoch: 57/99 Iteration: 4607 Training loss: 0.33529\n",
      "Epoch: 57/99 Iteration: 4608 Training loss: 0.42307\n",
      "Epoch: 57/99 Iteration: 4609 Training loss: 0.23851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/99 Iteration: 4610 Training loss: 0.35442\n",
      "Epoch: 57/99 Iteration: 4611 Training loss: 0.24636\n",
      "Epoch: 57/99 Iteration: 4612 Training loss: 0.45560\n",
      "Epoch: 57/99 Iteration: 4613 Training loss: 0.36267\n",
      "Epoch: 57/99 Iteration: 4614 Training loss: 0.40846\n",
      "Epoch: 57/99 Iteration: 4615 Training loss: 0.34664\n",
      "Epoch: 57/99 Iteration: 4616 Training loss: 0.60762\n",
      "Epoch: 57/99 Iteration: 4617 Training loss: 0.37862\n",
      "Epoch: 57/99 Iteration: 4618 Training loss: 0.32559\n",
      "Epoch: 57/99 Iteration: 4619 Training loss: 0.31265\n",
      "Epoch: 57/99 Iteration: 4620 Training loss: 0.43540\n",
      "Epoch: 57/99 Iteration: 4621 Training loss: 0.41457\n",
      "Epoch: 57/99 Iteration: 4622 Training loss: 0.31120\n",
      "Epoch: 57/99 Iteration: 4623 Training loss: 0.46067\n",
      "Epoch: 57/99 Iteration: 4624 Training loss: 0.36185\n",
      "Epoch: 57/99 Iteration: 4625 Training loss: 0.35452\n",
      "Epoch: 57/99 Iteration: 4626 Training loss: 0.44807\n",
      "Epoch: 57/99 Iteration: 4627 Training loss: 0.41526\n",
      "Epoch: 57/99 Iteration: 4628 Training loss: 0.40368\n",
      "Epoch: 57/99 Iteration: 4629 Training loss: 0.33002\n",
      "Epoch: 57/99 Iteration: 4630 Training loss: 0.43107\n",
      "Epoch: 57/99 Iteration: 4631 Training loss: 0.47423\n",
      "Epoch: 57/99 Iteration: 4632 Training loss: 0.37559\n",
      "Epoch: 57/99 Iteration: 4633 Training loss: 0.30537\n",
      "Epoch: 57/99 Iteration: 4634 Training loss: 0.39610\n",
      "Epoch: 57/99 Iteration: 4635 Training loss: 0.27873\n",
      "Epoch: 57/99 Iteration: 4636 Training loss: 0.30628\n",
      "Epoch: 57/99 Iteration: 4637 Training loss: 0.41426\n",
      "Epoch: 57/99 Iteration: 4638 Training loss: 0.29585\n",
      "Epoch: 57/99 Iteration: 4639 Training loss: 0.37671\n",
      "Epoch: 58/99 Iteration: 4640 Training loss: 0.31047\n",
      "Epoch: 58/99 Iteration: 4641 Training loss: 0.46254\n",
      "Epoch: 58/99 Iteration: 4642 Training loss: 0.48838\n",
      "Epoch: 58/99 Iteration: 4643 Training loss: 0.42965\n",
      "Epoch: 58/99 Iteration: 4644 Training loss: 0.35742\n",
      "Epoch: 58/99 Iteration: 4645 Training loss: 0.40480\n",
      "Epoch: 58/99 Iteration: 4646 Training loss: 0.44606\n",
      "Epoch: 58/99 Iteration: 4647 Training loss: 0.39196\n",
      "Epoch: 58/99 Iteration: 4648 Training loss: 0.52572\n",
      "Epoch: 58/99 Iteration: 4649 Training loss: 0.25093\n",
      "Epoch: 58/99 Iteration: 4650 Training loss: 0.31323\n",
      "***\n",
      "Epoch: 58/99 Iteration: 4650 Validation Acc: 0.8630\n",
      "***\n",
      "Epoch: 58/99 Iteration: 4651 Training loss: 0.38533\n",
      "Epoch: 58/99 Iteration: 4652 Training loss: 0.39634\n",
      "Epoch: 58/99 Iteration: 4653 Training loss: 0.45577\n",
      "Epoch: 58/99 Iteration: 4654 Training loss: 0.44955\n",
      "Epoch: 58/99 Iteration: 4655 Training loss: 0.38256\n",
      "Epoch: 58/99 Iteration: 4656 Training loss: 0.31619\n",
      "Epoch: 58/99 Iteration: 4657 Training loss: 0.27575\n",
      "Epoch: 58/99 Iteration: 4658 Training loss: 0.34295\n",
      "Epoch: 58/99 Iteration: 4659 Training loss: 0.28935\n",
      "Epoch: 58/99 Iteration: 4660 Training loss: 0.42566\n",
      "Epoch: 58/99 Iteration: 4661 Training loss: 0.29248\n",
      "Epoch: 58/99 Iteration: 4662 Training loss: 0.43717\n",
      "Epoch: 58/99 Iteration: 4663 Training loss: 0.33281\n",
      "Epoch: 58/99 Iteration: 4664 Training loss: 0.32782\n",
      "Epoch: 58/99 Iteration: 4665 Training loss: 0.45413\n",
      "Epoch: 58/99 Iteration: 4666 Training loss: 0.33316\n",
      "Epoch: 58/99 Iteration: 4667 Training loss: 0.21678\n",
      "Epoch: 58/99 Iteration: 4668 Training loss: 0.40851\n",
      "Epoch: 58/99 Iteration: 4669 Training loss: 0.52018\n",
      "Epoch: 58/99 Iteration: 4670 Training loss: 0.31952\n",
      "Epoch: 58/99 Iteration: 4671 Training loss: 0.35100\n",
      "Epoch: 58/99 Iteration: 4672 Training loss: 0.23684\n",
      "Epoch: 58/99 Iteration: 4673 Training loss: 0.34751\n",
      "Epoch: 58/99 Iteration: 4674 Training loss: 0.49502\n",
      "Epoch: 58/99 Iteration: 4675 Training loss: 0.29872\n",
      "Epoch: 58/99 Iteration: 4676 Training loss: 0.62668\n",
      "Epoch: 58/99 Iteration: 4677 Training loss: 0.33725\n",
      "Epoch: 58/99 Iteration: 4678 Training loss: 0.33578\n",
      "Epoch: 58/99 Iteration: 4679 Training loss: 0.36651\n",
      "Epoch: 58/99 Iteration: 4680 Training loss: 0.32911\n",
      "Epoch: 58/99 Iteration: 4681 Training loss: 0.38274\n",
      "Epoch: 58/99 Iteration: 4682 Training loss: 0.40530\n",
      "Epoch: 58/99 Iteration: 4683 Training loss: 0.31225\n",
      "Epoch: 58/99 Iteration: 4684 Training loss: 0.34819\n",
      "Epoch: 58/99 Iteration: 4685 Training loss: 0.28809\n",
      "Epoch: 58/99 Iteration: 4686 Training loss: 0.49255\n",
      "Epoch: 58/99 Iteration: 4687 Training loss: 0.52008\n",
      "Epoch: 58/99 Iteration: 4688 Training loss: 0.47224\n",
      "Epoch: 58/99 Iteration: 4689 Training loss: 0.42834\n",
      "Epoch: 58/99 Iteration: 4690 Training loss: 0.47342\n",
      "Epoch: 58/99 Iteration: 4691 Training loss: 0.40767\n",
      "Epoch: 58/99 Iteration: 4692 Training loss: 0.39018\n",
      "Epoch: 58/99 Iteration: 4693 Training loss: 0.47549\n",
      "Epoch: 58/99 Iteration: 4694 Training loss: 0.37799\n",
      "Epoch: 58/99 Iteration: 4695 Training loss: 0.45888\n",
      "Epoch: 58/99 Iteration: 4696 Training loss: 0.45549\n",
      "Epoch: 58/99 Iteration: 4697 Training loss: 0.36024\n",
      "Epoch: 58/99 Iteration: 4698 Training loss: 0.44370\n",
      "Epoch: 58/99 Iteration: 4699 Training loss: 0.42706\n",
      "Epoch: 58/99 Iteration: 4700 Training loss: 0.36710\n",
      "***\n",
      "Epoch: 58/99 Iteration: 4700 Validation Acc: 0.8440\n",
      "***\n",
      "Epoch: 58/99 Iteration: 4701 Training loss: 0.38688\n",
      "Epoch: 58/99 Iteration: 4702 Training loss: 0.37330\n",
      "Epoch: 58/99 Iteration: 4703 Training loss: 0.40342\n",
      "Epoch: 58/99 Iteration: 4704 Training loss: 0.43997\n",
      "Epoch: 58/99 Iteration: 4705 Training loss: 0.50366\n",
      "Epoch: 58/99 Iteration: 4706 Training loss: 0.39638\n",
      "Epoch: 58/99 Iteration: 4707 Training loss: 0.39229\n",
      "Epoch: 58/99 Iteration: 4708 Training loss: 0.43278\n",
      "Epoch: 58/99 Iteration: 4709 Training loss: 0.39951\n",
      "Epoch: 58/99 Iteration: 4710 Training loss: 0.50202\n",
      "Epoch: 58/99 Iteration: 4711 Training loss: 0.31195\n",
      "Epoch: 58/99 Iteration: 4712 Training loss: 0.32451\n",
      "Epoch: 58/99 Iteration: 4713 Training loss: 0.49138\n",
      "Epoch: 58/99 Iteration: 4714 Training loss: 0.45466\n",
      "Epoch: 58/99 Iteration: 4715 Training loss: 0.40445\n",
      "Epoch: 58/99 Iteration: 4716 Training loss: 0.39575\n",
      "Epoch: 58/99 Iteration: 4717 Training loss: 0.44227\n",
      "Epoch: 58/99 Iteration: 4718 Training loss: 0.39266\n",
      "Epoch: 58/99 Iteration: 4719 Training loss: 0.41701\n",
      "Epoch: 59/99 Iteration: 4720 Training loss: 0.32552\n",
      "Epoch: 59/99 Iteration: 4721 Training loss: 0.49268\n",
      "Epoch: 59/99 Iteration: 4722 Training loss: 0.42462\n",
      "Epoch: 59/99 Iteration: 4723 Training loss: 0.53505\n",
      "Epoch: 59/99 Iteration: 4724 Training loss: 0.38326\n",
      "Epoch: 59/99 Iteration: 4725 Training loss: 0.36498\n",
      "Epoch: 59/99 Iteration: 4726 Training loss: 0.43957\n",
      "Epoch: 59/99 Iteration: 4727 Training loss: 0.34392\n",
      "Epoch: 59/99 Iteration: 4728 Training loss: 0.57348\n",
      "Epoch: 59/99 Iteration: 4729 Training loss: 0.31878\n",
      "Epoch: 59/99 Iteration: 4730 Training loss: 0.25803\n",
      "Epoch: 59/99 Iteration: 4731 Training loss: 0.42576\n",
      "Epoch: 59/99 Iteration: 4732 Training loss: 0.31577\n",
      "Epoch: 59/99 Iteration: 4733 Training loss: 0.42890\n",
      "Epoch: 59/99 Iteration: 4734 Training loss: 0.36528\n",
      "Epoch: 59/99 Iteration: 4735 Training loss: 0.30733\n",
      "Epoch: 59/99 Iteration: 4736 Training loss: 0.24948\n",
      "Epoch: 59/99 Iteration: 4737 Training loss: 0.26318\n",
      "Epoch: 59/99 Iteration: 4738 Training loss: 0.40971\n",
      "Epoch: 59/99 Iteration: 4739 Training loss: 0.27052\n",
      "Epoch: 59/99 Iteration: 4740 Training loss: 0.32557\n",
      "Epoch: 59/99 Iteration: 4741 Training loss: 0.31534\n",
      "Epoch: 59/99 Iteration: 4742 Training loss: 0.32238\n",
      "Epoch: 59/99 Iteration: 4743 Training loss: 0.43061\n",
      "Epoch: 59/99 Iteration: 4744 Training loss: 0.28291\n",
      "Epoch: 59/99 Iteration: 4745 Training loss: 0.34867\n",
      "Epoch: 59/99 Iteration: 4746 Training loss: 0.48702\n",
      "Epoch: 59/99 Iteration: 4747 Training loss: 0.26841\n",
      "Epoch: 59/99 Iteration: 4748 Training loss: 0.44412\n",
      "Epoch: 59/99 Iteration: 4749 Training loss: 0.30402\n",
      "Epoch: 59/99 Iteration: 4750 Training loss: 0.30593\n",
      "***\n",
      "Epoch: 59/99 Iteration: 4750 Validation Acc: 0.8570\n",
      "***\n",
      "Epoch: 59/99 Iteration: 4751 Training loss: 0.28064\n",
      "Epoch: 59/99 Iteration: 4752 Training loss: 0.30038\n",
      "Epoch: 59/99 Iteration: 4753 Training loss: 0.37597\n",
      "Epoch: 59/99 Iteration: 4754 Training loss: 0.50336\n",
      "Epoch: 59/99 Iteration: 4755 Training loss: 0.38667\n",
      "Epoch: 59/99 Iteration: 4756 Training loss: 0.43660\n",
      "Epoch: 59/99 Iteration: 4757 Training loss: 0.27128\n",
      "Epoch: 59/99 Iteration: 4758 Training loss: 0.27106\n",
      "Epoch: 59/99 Iteration: 4759 Training loss: 0.41734\n",
      "Epoch: 59/99 Iteration: 4760 Training loss: 0.41955\n",
      "Epoch: 59/99 Iteration: 4761 Training loss: 0.35078\n",
      "Epoch: 59/99 Iteration: 4762 Training loss: 0.41368\n",
      "Epoch: 59/99 Iteration: 4763 Training loss: 0.26471\n",
      "Epoch: 59/99 Iteration: 4764 Training loss: 0.41817\n",
      "Epoch: 59/99 Iteration: 4765 Training loss: 0.46367\n",
      "Epoch: 59/99 Iteration: 4766 Training loss: 0.34652\n",
      "Epoch: 59/99 Iteration: 4767 Training loss: 0.45294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/99 Iteration: 4768 Training loss: 0.55134\n",
      "Epoch: 59/99 Iteration: 4769 Training loss: 0.30649\n",
      "Epoch: 59/99 Iteration: 4770 Training loss: 0.30606\n",
      "Epoch: 59/99 Iteration: 4771 Training loss: 0.37059\n",
      "Epoch: 59/99 Iteration: 4772 Training loss: 0.38301\n",
      "Epoch: 59/99 Iteration: 4773 Training loss: 0.32559\n",
      "Epoch: 59/99 Iteration: 4774 Training loss: 0.36844\n",
      "Epoch: 59/99 Iteration: 4775 Training loss: 0.41073\n",
      "Epoch: 59/99 Iteration: 4776 Training loss: 0.40202\n",
      "Epoch: 59/99 Iteration: 4777 Training loss: 0.45618\n",
      "Epoch: 59/99 Iteration: 4778 Training loss: 0.42763\n",
      "Epoch: 59/99 Iteration: 4779 Training loss: 0.40235\n",
      "Epoch: 59/99 Iteration: 4780 Training loss: 0.23310\n",
      "Epoch: 59/99 Iteration: 4781 Training loss: 0.47185\n",
      "Epoch: 59/99 Iteration: 4782 Training loss: 0.37941\n",
      "Epoch: 59/99 Iteration: 4783 Training loss: 0.49776\n",
      "Epoch: 59/99 Iteration: 4784 Training loss: 0.45910\n",
      "Epoch: 59/99 Iteration: 4785 Training loss: 0.50160\n",
      "Epoch: 59/99 Iteration: 4786 Training loss: 0.43261\n",
      "Epoch: 59/99 Iteration: 4787 Training loss: 0.37894\n",
      "Epoch: 59/99 Iteration: 4788 Training loss: 0.26556\n",
      "Epoch: 59/99 Iteration: 4789 Training loss: 0.34450\n",
      "Epoch: 59/99 Iteration: 4790 Training loss: 0.52489\n",
      "Epoch: 59/99 Iteration: 4791 Training loss: 0.36334\n",
      "Epoch: 59/99 Iteration: 4792 Training loss: 0.46585\n",
      "Epoch: 59/99 Iteration: 4793 Training loss: 0.32963\n",
      "Epoch: 59/99 Iteration: 4794 Training loss: 0.40111\n",
      "Epoch: 59/99 Iteration: 4795 Training loss: 0.26059\n",
      "Epoch: 59/99 Iteration: 4796 Training loss: 0.21618\n",
      "Epoch: 59/99 Iteration: 4797 Training loss: 0.40903\n",
      "Epoch: 59/99 Iteration: 4798 Training loss: 0.32276\n",
      "Epoch: 59/99 Iteration: 4799 Training loss: 0.22768\n",
      "Epoch: 60/99 Iteration: 4800 Training loss: 0.29202\n",
      "***\n",
      "Epoch: 60/99 Iteration: 4800 Validation Acc: 0.8530\n",
      "***\n",
      "Epoch: 60/99 Iteration: 4801 Training loss: 0.33864\n",
      "Epoch: 60/99 Iteration: 4802 Training loss: 0.33043\n",
      "Epoch: 60/99 Iteration: 4803 Training loss: 0.50947\n",
      "Epoch: 60/99 Iteration: 4804 Training loss: 0.30185\n",
      "Epoch: 60/99 Iteration: 4805 Training loss: 0.44728\n",
      "Epoch: 60/99 Iteration: 4806 Training loss: 0.27310\n",
      "Epoch: 60/99 Iteration: 4807 Training loss: 0.44252\n",
      "Epoch: 60/99 Iteration: 4808 Training loss: 0.42866\n",
      "Epoch: 60/99 Iteration: 4809 Training loss: 0.36280\n",
      "Epoch: 60/99 Iteration: 4810 Training loss: 0.27232\n",
      "Epoch: 60/99 Iteration: 4811 Training loss: 0.25169\n",
      "Epoch: 60/99 Iteration: 4812 Training loss: 0.40399\n",
      "Epoch: 60/99 Iteration: 4813 Training loss: 0.35607\n",
      "Epoch: 60/99 Iteration: 4814 Training loss: 0.35484\n",
      "Epoch: 60/99 Iteration: 4815 Training loss: 0.27436\n",
      "Epoch: 60/99 Iteration: 4816 Training loss: 0.28313\n",
      "Epoch: 60/99 Iteration: 4817 Training loss: 0.33440\n",
      "Epoch: 60/99 Iteration: 4818 Training loss: 0.37454\n",
      "Epoch: 60/99 Iteration: 4819 Training loss: 0.34021\n",
      "Epoch: 60/99 Iteration: 4820 Training loss: 0.25562\n",
      "Epoch: 60/99 Iteration: 4821 Training loss: 0.41557\n",
      "Epoch: 60/99 Iteration: 4822 Training loss: 0.34063\n",
      "Epoch: 60/99 Iteration: 4823 Training loss: 0.38196\n",
      "Epoch: 60/99 Iteration: 4824 Training loss: 0.36341\n",
      "Epoch: 60/99 Iteration: 4825 Training loss: 0.44808\n",
      "Epoch: 60/99 Iteration: 4826 Training loss: 0.41608\n",
      "Epoch: 60/99 Iteration: 4827 Training loss: 0.28051\n",
      "Epoch: 60/99 Iteration: 4828 Training loss: 0.41221\n",
      "Epoch: 60/99 Iteration: 4829 Training loss: 0.39147\n",
      "Epoch: 60/99 Iteration: 4830 Training loss: 0.26702\n",
      "Epoch: 60/99 Iteration: 4831 Training loss: 0.33727\n",
      "Epoch: 60/99 Iteration: 4832 Training loss: 0.34598\n",
      "Epoch: 60/99 Iteration: 4833 Training loss: 0.31334\n",
      "Epoch: 60/99 Iteration: 4834 Training loss: 0.41163\n",
      "Epoch: 60/99 Iteration: 4835 Training loss: 0.38151\n",
      "Epoch: 60/99 Iteration: 4836 Training loss: 0.57981\n",
      "Epoch: 60/99 Iteration: 4837 Training loss: 0.33668\n",
      "Epoch: 60/99 Iteration: 4838 Training loss: 0.27875\n",
      "Epoch: 60/99 Iteration: 4839 Training loss: 0.36603\n",
      "Epoch: 60/99 Iteration: 4840 Training loss: 0.41260\n",
      "Epoch: 60/99 Iteration: 4841 Training loss: 0.49906\n",
      "Epoch: 60/99 Iteration: 4842 Training loss: 0.30789\n",
      "Epoch: 60/99 Iteration: 4843 Training loss: 0.24072\n",
      "Epoch: 60/99 Iteration: 4844 Training loss: 0.27105\n",
      "Epoch: 60/99 Iteration: 4845 Training loss: 0.29941\n",
      "Epoch: 60/99 Iteration: 4846 Training loss: 0.35519\n",
      "Epoch: 60/99 Iteration: 4847 Training loss: 0.45970\n",
      "Epoch: 60/99 Iteration: 4848 Training loss: 0.57069\n",
      "Epoch: 60/99 Iteration: 4849 Training loss: 0.34276\n",
      "Epoch: 60/99 Iteration: 4850 Training loss: 0.30621\n",
      "***\n",
      "Epoch: 60/99 Iteration: 4850 Validation Acc: 0.8500\n",
      "***\n",
      "Epoch: 60/99 Iteration: 4851 Training loss: 0.36443\n",
      "Epoch: 60/99 Iteration: 4852 Training loss: 0.43080\n",
      "Epoch: 60/99 Iteration: 4853 Training loss: 0.35648\n",
      "Epoch: 60/99 Iteration: 4854 Training loss: 0.39082\n",
      "Epoch: 60/99 Iteration: 4855 Training loss: 0.25396\n",
      "Epoch: 60/99 Iteration: 4856 Training loss: 0.32827\n",
      "Epoch: 60/99 Iteration: 4857 Training loss: 0.41235\n",
      "Epoch: 60/99 Iteration: 4858 Training loss: 0.41505\n",
      "Epoch: 60/99 Iteration: 4859 Training loss: 0.48502\n",
      "Epoch: 60/99 Iteration: 4860 Training loss: 0.32390\n",
      "Epoch: 60/99 Iteration: 4861 Training loss: 0.33546\n",
      "Epoch: 60/99 Iteration: 4862 Training loss: 0.32422\n",
      "Epoch: 60/99 Iteration: 4863 Training loss: 0.31714\n",
      "Epoch: 60/99 Iteration: 4864 Training loss: 0.38801\n",
      "Epoch: 60/99 Iteration: 4865 Training loss: 0.40658\n",
      "Epoch: 60/99 Iteration: 4866 Training loss: 0.35895\n",
      "Epoch: 60/99 Iteration: 4867 Training loss: 0.51531\n",
      "Epoch: 60/99 Iteration: 4868 Training loss: 0.30625\n",
      "Epoch: 60/99 Iteration: 4869 Training loss: 0.29262\n",
      "Epoch: 60/99 Iteration: 4870 Training loss: 0.36357\n",
      "Epoch: 60/99 Iteration: 4871 Training loss: 0.37682\n",
      "Epoch: 60/99 Iteration: 4872 Training loss: 0.29080\n",
      "Epoch: 60/99 Iteration: 4873 Training loss: 0.40378\n",
      "Epoch: 60/99 Iteration: 4874 Training loss: 0.41855\n",
      "Epoch: 60/99 Iteration: 4875 Training loss: 0.33368\n",
      "Epoch: 60/99 Iteration: 4876 Training loss: 0.34458\n",
      "Epoch: 60/99 Iteration: 4877 Training loss: 0.51933\n",
      "Epoch: 60/99 Iteration: 4878 Training loss: 0.31396\n",
      "Epoch: 60/99 Iteration: 4879 Training loss: 0.34886\n",
      "Epoch: 61/99 Iteration: 4880 Training loss: 0.32756\n",
      "Epoch: 61/99 Iteration: 4881 Training loss: 0.47584\n",
      "Epoch: 61/99 Iteration: 4882 Training loss: 0.39102\n",
      "Epoch: 61/99 Iteration: 4883 Training loss: 0.28854\n",
      "Epoch: 61/99 Iteration: 4884 Training loss: 0.35922\n",
      "Epoch: 61/99 Iteration: 4885 Training loss: 0.27908\n",
      "Epoch: 61/99 Iteration: 4886 Training loss: 0.42823\n",
      "Epoch: 61/99 Iteration: 4887 Training loss: 0.41697\n",
      "Epoch: 61/99 Iteration: 4888 Training loss: 0.51474\n",
      "Epoch: 61/99 Iteration: 4889 Training loss: 0.24031\n",
      "Epoch: 61/99 Iteration: 4890 Training loss: 0.25766\n",
      "Epoch: 61/99 Iteration: 4891 Training loss: 0.31590\n",
      "Epoch: 61/99 Iteration: 4892 Training loss: 0.32431\n",
      "Epoch: 61/99 Iteration: 4893 Training loss: 0.46564\n",
      "Epoch: 61/99 Iteration: 4894 Training loss: 0.43267\n",
      "Epoch: 61/99 Iteration: 4895 Training loss: 0.35146\n",
      "Epoch: 61/99 Iteration: 4896 Training loss: 0.26382\n",
      "Epoch: 61/99 Iteration: 4897 Training loss: 0.34899\n",
      "Epoch: 61/99 Iteration: 4898 Training loss: 0.34383\n",
      "Epoch: 61/99 Iteration: 4899 Training loss: 0.26292\n",
      "Epoch: 61/99 Iteration: 4900 Training loss: 0.36493\n",
      "***\n",
      "Epoch: 61/99 Iteration: 4900 Validation Acc: 0.8650\n",
      "***\n",
      "Epoch: 61/99 Iteration: 4901 Training loss: 0.33644\n",
      "Epoch: 61/99 Iteration: 4902 Training loss: 0.28464\n",
      "Epoch: 61/99 Iteration: 4903 Training loss: 0.38931\n",
      "Epoch: 61/99 Iteration: 4904 Training loss: 0.29550\n",
      "Epoch: 61/99 Iteration: 4905 Training loss: 0.35397\n",
      "Epoch: 61/99 Iteration: 4906 Training loss: 0.34093\n",
      "Epoch: 61/99 Iteration: 4907 Training loss: 0.24907\n",
      "Epoch: 61/99 Iteration: 4908 Training loss: 0.40927\n",
      "Epoch: 61/99 Iteration: 4909 Training loss: 0.43910\n",
      "Epoch: 61/99 Iteration: 4910 Training loss: 0.25369\n",
      "Epoch: 61/99 Iteration: 4911 Training loss: 0.31948\n",
      "Epoch: 61/99 Iteration: 4912 Training loss: 0.34253\n",
      "Epoch: 61/99 Iteration: 4913 Training loss: 0.41101\n",
      "Epoch: 61/99 Iteration: 4914 Training loss: 0.27948\n",
      "Epoch: 61/99 Iteration: 4915 Training loss: 0.30879\n",
      "Epoch: 61/99 Iteration: 4916 Training loss: 0.51977\n",
      "Epoch: 61/99 Iteration: 4917 Training loss: 0.28453\n",
      "Epoch: 61/99 Iteration: 4918 Training loss: 0.51637\n",
      "Epoch: 61/99 Iteration: 4919 Training loss: 0.38624\n",
      "Epoch: 61/99 Iteration: 4920 Training loss: 0.27616\n",
      "Epoch: 61/99 Iteration: 4921 Training loss: 0.28873\n",
      "Epoch: 61/99 Iteration: 4922 Training loss: 0.43852\n",
      "Epoch: 61/99 Iteration: 4923 Training loss: 0.28527\n",
      "Epoch: 61/99 Iteration: 4924 Training loss: 0.39285\n",
      "Epoch: 61/99 Iteration: 4925 Training loss: 0.44721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/99 Iteration: 4926 Training loss: 0.45924\n",
      "Epoch: 61/99 Iteration: 4927 Training loss: 0.31369\n",
      "Epoch: 61/99 Iteration: 4928 Training loss: 0.56947\n",
      "Epoch: 61/99 Iteration: 4929 Training loss: 0.26514\n",
      "Epoch: 61/99 Iteration: 4930 Training loss: 0.43479\n",
      "Epoch: 61/99 Iteration: 4931 Training loss: 0.35053\n",
      "Epoch: 61/99 Iteration: 4932 Training loss: 0.26861\n",
      "Epoch: 61/99 Iteration: 4933 Training loss: 0.43673\n",
      "Epoch: 61/99 Iteration: 4934 Training loss: 0.44465\n",
      "Epoch: 61/99 Iteration: 4935 Training loss: 0.34896\n",
      "Epoch: 61/99 Iteration: 4936 Training loss: 0.32333\n",
      "Epoch: 61/99 Iteration: 4937 Training loss: 0.41193\n",
      "Epoch: 61/99 Iteration: 4938 Training loss: 0.33561\n",
      "Epoch: 61/99 Iteration: 4939 Training loss: 0.35858\n",
      "Epoch: 61/99 Iteration: 4940 Training loss: 0.30723\n",
      "Epoch: 61/99 Iteration: 4941 Training loss: 0.36453\n",
      "Epoch: 61/99 Iteration: 4942 Training loss: 0.27701\n",
      "Epoch: 61/99 Iteration: 4943 Training loss: 0.42078\n",
      "Epoch: 61/99 Iteration: 4944 Training loss: 0.42518\n",
      "Epoch: 61/99 Iteration: 4945 Training loss: 0.35317\n",
      "Epoch: 61/99 Iteration: 4946 Training loss: 0.45762\n",
      "Epoch: 61/99 Iteration: 4947 Training loss: 0.31706\n",
      "Epoch: 61/99 Iteration: 4948 Training loss: 0.34696\n",
      "Epoch: 61/99 Iteration: 4949 Training loss: 0.28268\n",
      "Epoch: 61/99 Iteration: 4950 Training loss: 0.39890\n",
      "***\n",
      "Epoch: 61/99 Iteration: 4950 Validation Acc: 0.8600\n",
      "***\n",
      "Epoch: 61/99 Iteration: 4951 Training loss: 0.42093\n",
      "Epoch: 61/99 Iteration: 4952 Training loss: 0.39296\n",
      "Epoch: 61/99 Iteration: 4953 Training loss: 0.31037\n",
      "Epoch: 61/99 Iteration: 4954 Training loss: 0.30058\n",
      "Epoch: 61/99 Iteration: 4955 Training loss: 0.43162\n",
      "Epoch: 61/99 Iteration: 4956 Training loss: 0.37277\n",
      "Epoch: 61/99 Iteration: 4957 Training loss: 0.38689\n",
      "Epoch: 61/99 Iteration: 4958 Training loss: 0.48739\n",
      "Epoch: 61/99 Iteration: 4959 Training loss: 0.40545\n",
      "Epoch: 62/99 Iteration: 4960 Training loss: 0.33796\n",
      "Epoch: 62/99 Iteration: 4961 Training loss: 0.38150\n",
      "Epoch: 62/99 Iteration: 4962 Training loss: 0.40017\n",
      "Epoch: 62/99 Iteration: 4963 Training loss: 0.45149\n",
      "Epoch: 62/99 Iteration: 4964 Training loss: 0.36271\n",
      "Epoch: 62/99 Iteration: 4965 Training loss: 0.24880\n",
      "Epoch: 62/99 Iteration: 4966 Training loss: 0.37741\n",
      "Epoch: 62/99 Iteration: 4967 Training loss: 0.38400\n",
      "Epoch: 62/99 Iteration: 4968 Training loss: 0.44913\n",
      "Epoch: 62/99 Iteration: 4969 Training loss: 0.26328\n",
      "Epoch: 62/99 Iteration: 4970 Training loss: 0.37489\n",
      "Epoch: 62/99 Iteration: 4971 Training loss: 0.24237\n",
      "Epoch: 62/99 Iteration: 4972 Training loss: 0.34932\n",
      "Epoch: 62/99 Iteration: 4973 Training loss: 0.48940\n",
      "Epoch: 62/99 Iteration: 4974 Training loss: 0.42749\n",
      "Epoch: 62/99 Iteration: 4975 Training loss: 0.27871\n",
      "Epoch: 62/99 Iteration: 4976 Training loss: 0.44139\n",
      "Epoch: 62/99 Iteration: 4977 Training loss: 0.29953\n",
      "Epoch: 62/99 Iteration: 4978 Training loss: 0.43819\n",
      "Epoch: 62/99 Iteration: 4979 Training loss: 0.46379\n",
      "Epoch: 62/99 Iteration: 4980 Training loss: 0.36092\n",
      "Epoch: 62/99 Iteration: 4981 Training loss: 0.32500\n",
      "Epoch: 62/99 Iteration: 4982 Training loss: 0.35050\n",
      "Epoch: 62/99 Iteration: 4983 Training loss: 0.37149\n",
      "Epoch: 62/99 Iteration: 4984 Training loss: 0.27192\n",
      "Epoch: 62/99 Iteration: 4985 Training loss: 0.46289\n",
      "Epoch: 62/99 Iteration: 4986 Training loss: 0.42517\n",
      "Epoch: 62/99 Iteration: 4987 Training loss: 0.35685\n",
      "Epoch: 62/99 Iteration: 4988 Training loss: 0.45915\n",
      "Epoch: 62/99 Iteration: 4989 Training loss: 0.50065\n",
      "Epoch: 62/99 Iteration: 4990 Training loss: 0.33468\n",
      "Epoch: 62/99 Iteration: 4991 Training loss: 0.34369\n",
      "Epoch: 62/99 Iteration: 4992 Training loss: 0.33008\n",
      "Epoch: 62/99 Iteration: 4993 Training loss: 0.35440\n",
      "Epoch: 62/99 Iteration: 4994 Training loss: 0.34868\n",
      "Epoch: 62/99 Iteration: 4995 Training loss: 0.32443\n",
      "Epoch: 62/99 Iteration: 4996 Training loss: 0.52020\n",
      "Epoch: 62/99 Iteration: 4997 Training loss: 0.24671\n",
      "Epoch: 62/99 Iteration: 4998 Training loss: 0.33132\n",
      "Epoch: 62/99 Iteration: 4999 Training loss: 0.30759\n",
      "Epoch: 62/99 Iteration: 5000 Training loss: 0.47313\n",
      "***\n",
      "Epoch: 62/99 Iteration: 5000 Validation Acc: 0.8530\n",
      "***\n",
      "Epoch: 62/99 Iteration: 5001 Training loss: 0.36072\n",
      "Epoch: 62/99 Iteration: 5002 Training loss: 0.28162\n",
      "Epoch: 62/99 Iteration: 5003 Training loss: 0.34045\n",
      "Epoch: 62/99 Iteration: 5004 Training loss: 0.42728\n",
      "Epoch: 62/99 Iteration: 5005 Training loss: 0.57500\n",
      "Epoch: 62/99 Iteration: 5006 Training loss: 0.60544\n",
      "Epoch: 62/99 Iteration: 5007 Training loss: 0.38485\n",
      "Epoch: 62/99 Iteration: 5008 Training loss: 0.57264\n",
      "Epoch: 62/99 Iteration: 5009 Training loss: 0.40647\n",
      "Epoch: 62/99 Iteration: 5010 Training loss: 0.29082\n",
      "Epoch: 62/99 Iteration: 5011 Training loss: 0.43003\n",
      "Epoch: 62/99 Iteration: 5012 Training loss: 0.37676\n",
      "Epoch: 62/99 Iteration: 5013 Training loss: 0.33286\n",
      "Epoch: 62/99 Iteration: 5014 Training loss: 0.48094\n",
      "Epoch: 62/99 Iteration: 5015 Training loss: 0.33244\n",
      "Epoch: 62/99 Iteration: 5016 Training loss: 0.46621\n",
      "Epoch: 62/99 Iteration: 5017 Training loss: 0.55473\n",
      "Epoch: 62/99 Iteration: 5018 Training loss: 0.40842\n",
      "Epoch: 62/99 Iteration: 5019 Training loss: 0.41406\n",
      "Epoch: 62/99 Iteration: 5020 Training loss: 0.43894\n",
      "Epoch: 62/99 Iteration: 5021 Training loss: 0.48357\n",
      "Epoch: 62/99 Iteration: 5022 Training loss: 0.38806\n",
      "Epoch: 62/99 Iteration: 5023 Training loss: 0.36941\n",
      "Epoch: 62/99 Iteration: 5024 Training loss: 0.47679\n",
      "Epoch: 62/99 Iteration: 5025 Training loss: 0.35340\n",
      "Epoch: 62/99 Iteration: 5026 Training loss: 0.52156\n",
      "Epoch: 62/99 Iteration: 5027 Training loss: 0.49222\n",
      "Epoch: 62/99 Iteration: 5028 Training loss: 0.31331\n",
      "Epoch: 62/99 Iteration: 5029 Training loss: 0.34203\n",
      "Epoch: 62/99 Iteration: 5030 Training loss: 0.46732\n",
      "Epoch: 62/99 Iteration: 5031 Training loss: 0.32648\n",
      "Epoch: 62/99 Iteration: 5032 Training loss: 0.33125\n",
      "Epoch: 62/99 Iteration: 5033 Training loss: 0.36007\n",
      "Epoch: 62/99 Iteration: 5034 Training loss: 0.30608\n",
      "Epoch: 62/99 Iteration: 5035 Training loss: 0.35651\n",
      "Epoch: 62/99 Iteration: 5036 Training loss: 0.28139\n",
      "Epoch: 62/99 Iteration: 5037 Training loss: 0.49929\n",
      "Epoch: 62/99 Iteration: 5038 Training loss: 0.48789\n",
      "Epoch: 62/99 Iteration: 5039 Training loss: 0.30385\n",
      "Epoch: 63/99 Iteration: 5040 Training loss: 0.21538\n",
      "Epoch: 63/99 Iteration: 5041 Training loss: 0.38476\n",
      "Epoch: 63/99 Iteration: 5042 Training loss: 0.39508\n",
      "Epoch: 63/99 Iteration: 5043 Training loss: 0.37781\n",
      "Epoch: 63/99 Iteration: 5044 Training loss: 0.35821\n",
      "Epoch: 63/99 Iteration: 5045 Training loss: 0.35954\n",
      "Epoch: 63/99 Iteration: 5046 Training loss: 0.34197\n",
      "Epoch: 63/99 Iteration: 5047 Training loss: 0.31220\n",
      "Epoch: 63/99 Iteration: 5048 Training loss: 0.43169\n",
      "Epoch: 63/99 Iteration: 5049 Training loss: 0.41208\n",
      "Epoch: 63/99 Iteration: 5050 Training loss: 0.28714\n",
      "***\n",
      "Epoch: 63/99 Iteration: 5050 Validation Acc: 0.8600\n",
      "***\n",
      "Epoch: 63/99 Iteration: 5051 Training loss: 0.23542\n",
      "Epoch: 63/99 Iteration: 5052 Training loss: 0.29971\n",
      "Epoch: 63/99 Iteration: 5053 Training loss: 0.40886\n",
      "Epoch: 63/99 Iteration: 5054 Training loss: 0.56225\n",
      "Epoch: 63/99 Iteration: 5055 Training loss: 0.28590\n",
      "Epoch: 63/99 Iteration: 5056 Training loss: 0.33449\n",
      "Epoch: 63/99 Iteration: 5057 Training loss: 0.23519\n",
      "Epoch: 63/99 Iteration: 5058 Training loss: 0.35201\n",
      "Epoch: 63/99 Iteration: 5059 Training loss: 0.47815\n",
      "Epoch: 63/99 Iteration: 5060 Training loss: 0.31676\n",
      "Epoch: 63/99 Iteration: 5061 Training loss: 0.25922\n",
      "Epoch: 63/99 Iteration: 5062 Training loss: 0.35938\n",
      "Epoch: 63/99 Iteration: 5063 Training loss: 0.36707\n",
      "Epoch: 63/99 Iteration: 5064 Training loss: 0.31669\n",
      "Epoch: 63/99 Iteration: 5065 Training loss: 0.42052\n",
      "Epoch: 63/99 Iteration: 5066 Training loss: 0.27436\n",
      "Epoch: 63/99 Iteration: 5067 Training loss: 0.30050\n",
      "Epoch: 63/99 Iteration: 5068 Training loss: 0.39917\n",
      "Epoch: 63/99 Iteration: 5069 Training loss: 0.37142\n",
      "Epoch: 63/99 Iteration: 5070 Training loss: 0.29316\n",
      "Epoch: 63/99 Iteration: 5071 Training loss: 0.39057\n",
      "Epoch: 63/99 Iteration: 5072 Training loss: 0.35221\n",
      "Epoch: 63/99 Iteration: 5073 Training loss: 0.29487\n",
      "Epoch: 63/99 Iteration: 5074 Training loss: 0.28198\n",
      "Epoch: 63/99 Iteration: 5075 Training loss: 0.46483\n",
      "Epoch: 63/99 Iteration: 5076 Training loss: 0.42958\n",
      "Epoch: 63/99 Iteration: 5077 Training loss: 0.38431\n",
      "Epoch: 63/99 Iteration: 5078 Training loss: 0.27184\n",
      "Epoch: 63/99 Iteration: 5079 Training loss: 0.29486\n",
      "Epoch: 63/99 Iteration: 5080 Training loss: 0.30519\n",
      "Epoch: 63/99 Iteration: 5081 Training loss: 0.36636\n",
      "Epoch: 63/99 Iteration: 5082 Training loss: 0.32133\n",
      "Epoch: 63/99 Iteration: 5083 Training loss: 0.19758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/99 Iteration: 5084 Training loss: 0.30956\n",
      "Epoch: 63/99 Iteration: 5085 Training loss: 0.25544\n",
      "Epoch: 63/99 Iteration: 5086 Training loss: 0.29840\n",
      "Epoch: 63/99 Iteration: 5087 Training loss: 0.46361\n",
      "Epoch: 63/99 Iteration: 5088 Training loss: 0.35236\n",
      "Epoch: 63/99 Iteration: 5089 Training loss: 0.21449\n",
      "Epoch: 63/99 Iteration: 5090 Training loss: 0.27234\n",
      "Epoch: 63/99 Iteration: 5091 Training loss: 0.47569\n",
      "Epoch: 63/99 Iteration: 5092 Training loss: 0.38783\n",
      "Epoch: 63/99 Iteration: 5093 Training loss: 0.33113\n",
      "Epoch: 63/99 Iteration: 5094 Training loss: 0.50834\n",
      "Epoch: 63/99 Iteration: 5095 Training loss: 0.33266\n",
      "Epoch: 63/99 Iteration: 5096 Training loss: 0.34857\n",
      "Epoch: 63/99 Iteration: 5097 Training loss: 0.24265\n",
      "Epoch: 63/99 Iteration: 5098 Training loss: 0.28740\n",
      "Epoch: 63/99 Iteration: 5099 Training loss: 0.30675\n",
      "Epoch: 63/99 Iteration: 5100 Training loss: 0.38897\n",
      "***\n",
      "Epoch: 63/99 Iteration: 5100 Validation Acc: 0.8680\n",
      "***\n",
      "Epoch: 63/99 Iteration: 5101 Training loss: 0.45410\n",
      "Epoch: 63/99 Iteration: 5102 Training loss: 0.39132\n",
      "Epoch: 63/99 Iteration: 5103 Training loss: 0.39865\n",
      "Epoch: 63/99 Iteration: 5104 Training loss: 0.27101\n",
      "Epoch: 63/99 Iteration: 5105 Training loss: 0.43288\n",
      "Epoch: 63/99 Iteration: 5106 Training loss: 0.44036\n",
      "Epoch: 63/99 Iteration: 5107 Training loss: 0.31836\n",
      "Epoch: 63/99 Iteration: 5108 Training loss: 0.23743\n",
      "Epoch: 63/99 Iteration: 5109 Training loss: 0.32280\n",
      "Epoch: 63/99 Iteration: 5110 Training loss: 0.52365\n",
      "Epoch: 63/99 Iteration: 5111 Training loss: 0.32375\n",
      "Epoch: 63/99 Iteration: 5112 Training loss: 0.22842\n",
      "Epoch: 63/99 Iteration: 5113 Training loss: 0.30211\n",
      "Epoch: 63/99 Iteration: 5114 Training loss: 0.31516\n",
      "Epoch: 63/99 Iteration: 5115 Training loss: 0.27404\n",
      "Epoch: 63/99 Iteration: 5116 Training loss: 0.25607\n",
      "Epoch: 63/99 Iteration: 5117 Training loss: 0.35562\n",
      "Epoch: 63/99 Iteration: 5118 Training loss: 0.41919\n",
      "Epoch: 63/99 Iteration: 5119 Training loss: 0.35344\n",
      "Epoch: 64/99 Iteration: 5120 Training loss: 0.17629\n",
      "Epoch: 64/99 Iteration: 5121 Training loss: 0.44665\n",
      "Epoch: 64/99 Iteration: 5122 Training loss: 0.28143\n",
      "Epoch: 64/99 Iteration: 5123 Training loss: 0.34852\n",
      "Epoch: 64/99 Iteration: 5124 Training loss: 0.29484\n",
      "Epoch: 64/99 Iteration: 5125 Training loss: 0.49459\n",
      "Epoch: 64/99 Iteration: 5126 Training loss: 0.32322\n",
      "Epoch: 64/99 Iteration: 5127 Training loss: 0.30818\n",
      "Epoch: 64/99 Iteration: 5128 Training loss: 0.55464\n",
      "Epoch: 64/99 Iteration: 5129 Training loss: 0.31628\n",
      "Epoch: 64/99 Iteration: 5130 Training loss: 0.28964\n",
      "Epoch: 64/99 Iteration: 5131 Training loss: 0.23134\n",
      "Epoch: 64/99 Iteration: 5132 Training loss: 0.33360\n",
      "Epoch: 64/99 Iteration: 5133 Training loss: 0.45555\n",
      "Epoch: 64/99 Iteration: 5134 Training loss: 0.41361\n",
      "Epoch: 64/99 Iteration: 5135 Training loss: 0.45553\n",
      "Epoch: 64/99 Iteration: 5136 Training loss: 0.20598\n",
      "Epoch: 64/99 Iteration: 5137 Training loss: 0.34326\n",
      "Epoch: 64/99 Iteration: 5138 Training loss: 0.38983\n",
      "Epoch: 64/99 Iteration: 5139 Training loss: 0.41721\n",
      "Epoch: 64/99 Iteration: 5140 Training loss: 0.37678\n",
      "Epoch: 64/99 Iteration: 5141 Training loss: 0.35179\n",
      "Epoch: 64/99 Iteration: 5142 Training loss: 0.42872\n",
      "Epoch: 64/99 Iteration: 5143 Training loss: 0.29963\n",
      "Epoch: 64/99 Iteration: 5144 Training loss: 0.36584\n",
      "Epoch: 64/99 Iteration: 5145 Training loss: 0.39989\n",
      "Epoch: 64/99 Iteration: 5146 Training loss: 0.29945\n",
      "Epoch: 64/99 Iteration: 5147 Training loss: 0.32482\n",
      "Epoch: 64/99 Iteration: 5148 Training loss: 0.42155\n",
      "Epoch: 64/99 Iteration: 5149 Training loss: 0.46461\n",
      "Epoch: 64/99 Iteration: 5150 Training loss: 0.19116\n",
      "***\n",
      "Epoch: 64/99 Iteration: 5150 Validation Acc: 0.8510\n",
      "***\n",
      "Epoch: 64/99 Iteration: 5151 Training loss: 0.35972\n",
      "Epoch: 64/99 Iteration: 5152 Training loss: 0.41414\n",
      "Epoch: 64/99 Iteration: 5153 Training loss: 0.34904\n",
      "Epoch: 64/99 Iteration: 5154 Training loss: 0.37970\n",
      "Epoch: 64/99 Iteration: 5155 Training loss: 0.36265\n",
      "Epoch: 64/99 Iteration: 5156 Training loss: 0.58791\n",
      "Epoch: 64/99 Iteration: 5157 Training loss: 0.33497\n",
      "Epoch: 64/99 Iteration: 5158 Training loss: 0.35577\n",
      "Epoch: 64/99 Iteration: 5159 Training loss: 0.33137\n",
      "Epoch: 64/99 Iteration: 5160 Training loss: 0.40559\n",
      "Epoch: 64/99 Iteration: 5161 Training loss: 0.29918\n",
      "Epoch: 64/99 Iteration: 5162 Training loss: 0.35465\n",
      "Epoch: 64/99 Iteration: 5163 Training loss: 0.33523\n",
      "Epoch: 64/99 Iteration: 5164 Training loss: 0.36189\n",
      "Epoch: 64/99 Iteration: 5165 Training loss: 0.32068\n",
      "Epoch: 64/99 Iteration: 5166 Training loss: 0.30310\n",
      "Epoch: 64/99 Iteration: 5167 Training loss: 0.34486\n",
      "Epoch: 64/99 Iteration: 5168 Training loss: 0.52305\n",
      "Epoch: 64/99 Iteration: 5169 Training loss: 0.23095\n",
      "Epoch: 64/99 Iteration: 5170 Training loss: 0.32065\n",
      "Epoch: 64/99 Iteration: 5171 Training loss: 0.37266\n",
      "Epoch: 64/99 Iteration: 5172 Training loss: 0.31079\n",
      "Epoch: 64/99 Iteration: 5173 Training loss: 0.56982\n",
      "Epoch: 64/99 Iteration: 5174 Training loss: 0.40432\n",
      "Epoch: 64/99 Iteration: 5175 Training loss: 0.30491\n",
      "Epoch: 64/99 Iteration: 5176 Training loss: 0.30430\n",
      "Epoch: 64/99 Iteration: 5177 Training loss: 0.32085\n",
      "Epoch: 64/99 Iteration: 5178 Training loss: 0.39513\n",
      "Epoch: 64/99 Iteration: 5179 Training loss: 0.48337\n",
      "Epoch: 64/99 Iteration: 5180 Training loss: 0.39133\n",
      "Epoch: 64/99 Iteration: 5181 Training loss: 0.26873\n",
      "Epoch: 64/99 Iteration: 5182 Training loss: 0.27273\n",
      "Epoch: 64/99 Iteration: 5183 Training loss: 0.46811\n",
      "Epoch: 64/99 Iteration: 5184 Training loss: 0.29986\n",
      "Epoch: 64/99 Iteration: 5185 Training loss: 0.48621\n",
      "Epoch: 64/99 Iteration: 5186 Training loss: 0.48781\n",
      "Epoch: 64/99 Iteration: 5187 Training loss: 0.46626\n",
      "Epoch: 64/99 Iteration: 5188 Training loss: 0.27187\n",
      "Epoch: 64/99 Iteration: 5189 Training loss: 0.40520\n",
      "Epoch: 64/99 Iteration: 5190 Training loss: 0.42457\n",
      "Epoch: 64/99 Iteration: 5191 Training loss: 0.31657\n",
      "Epoch: 64/99 Iteration: 5192 Training loss: 0.38601\n",
      "Epoch: 64/99 Iteration: 5193 Training loss: 0.24176\n",
      "Epoch: 64/99 Iteration: 5194 Training loss: 0.33796\n",
      "Epoch: 64/99 Iteration: 5195 Training loss: 0.26530\n",
      "Epoch: 64/99 Iteration: 5196 Training loss: 0.29617\n",
      "Epoch: 64/99 Iteration: 5197 Training loss: 0.29904\n",
      "Epoch: 64/99 Iteration: 5198 Training loss: 0.28534\n",
      "Epoch: 64/99 Iteration: 5199 Training loss: 0.30304\n",
      "Epoch: 65/99 Iteration: 5200 Training loss: 0.25504\n",
      "***\n",
      "Epoch: 65/99 Iteration: 5200 Validation Acc: 0.8520\n",
      "***\n",
      "Epoch: 65/99 Iteration: 5201 Training loss: 0.38504\n",
      "Epoch: 65/99 Iteration: 5202 Training loss: 0.40666\n",
      "Epoch: 65/99 Iteration: 5203 Training loss: 0.38835\n",
      "Epoch: 65/99 Iteration: 5204 Training loss: 0.23350\n",
      "Epoch: 65/99 Iteration: 5205 Training loss: 0.28993\n",
      "Epoch: 65/99 Iteration: 5206 Training loss: 0.29861\n",
      "Epoch: 65/99 Iteration: 5207 Training loss: 0.32257\n",
      "Epoch: 65/99 Iteration: 5208 Training loss: 0.46482\n",
      "Epoch: 65/99 Iteration: 5209 Training loss: 0.23250\n",
      "Epoch: 65/99 Iteration: 5210 Training loss: 0.30878\n",
      "Epoch: 65/99 Iteration: 5211 Training loss: 0.30917\n",
      "Epoch: 65/99 Iteration: 5212 Training loss: 0.30578\n",
      "Epoch: 65/99 Iteration: 5213 Training loss: 0.37161\n",
      "Epoch: 65/99 Iteration: 5214 Training loss: 0.40824\n",
      "Epoch: 65/99 Iteration: 5215 Training loss: 0.28652\n",
      "Epoch: 65/99 Iteration: 5216 Training loss: 0.32560\n",
      "Epoch: 65/99 Iteration: 5217 Training loss: 0.17123\n",
      "Epoch: 65/99 Iteration: 5218 Training loss: 0.41606\n",
      "Epoch: 65/99 Iteration: 5219 Training loss: 0.38871\n",
      "Epoch: 65/99 Iteration: 5220 Training loss: 0.48145\n",
      "Epoch: 65/99 Iteration: 5221 Training loss: 0.33064\n",
      "Epoch: 65/99 Iteration: 5222 Training loss: 0.28114\n",
      "Epoch: 65/99 Iteration: 5223 Training loss: 0.38166\n",
      "Epoch: 65/99 Iteration: 5224 Training loss: 0.22511\n",
      "Epoch: 65/99 Iteration: 5225 Training loss: 0.42469\n",
      "Epoch: 65/99 Iteration: 5226 Training loss: 0.33800\n",
      "Epoch: 65/99 Iteration: 5227 Training loss: 0.35587\n",
      "Epoch: 65/99 Iteration: 5228 Training loss: 0.35645\n",
      "Epoch: 65/99 Iteration: 5229 Training loss: 0.47644\n",
      "Epoch: 65/99 Iteration: 5230 Training loss: 0.39388\n",
      "Epoch: 65/99 Iteration: 5231 Training loss: 0.33664\n",
      "Epoch: 65/99 Iteration: 5232 Training loss: 0.45171\n",
      "Epoch: 65/99 Iteration: 5233 Training loss: 0.41815\n",
      "Epoch: 65/99 Iteration: 5234 Training loss: 0.46063\n",
      "Epoch: 65/99 Iteration: 5235 Training loss: 0.37305\n",
      "Epoch: 65/99 Iteration: 5236 Training loss: 0.54392\n",
      "Epoch: 65/99 Iteration: 5237 Training loss: 0.37129\n",
      "Epoch: 65/99 Iteration: 5238 Training loss: 0.42604\n",
      "Epoch: 65/99 Iteration: 5239 Training loss: 0.42082\n",
      "Epoch: 65/99 Iteration: 5240 Training loss: 0.39912\n",
      "Epoch: 65/99 Iteration: 5241 Training loss: 0.38320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/99 Iteration: 5242 Training loss: 0.33455\n",
      "Epoch: 65/99 Iteration: 5243 Training loss: 0.28184\n",
      "Epoch: 65/99 Iteration: 5244 Training loss: 0.48467\n",
      "Epoch: 65/99 Iteration: 5245 Training loss: 0.42354\n",
      "Epoch: 65/99 Iteration: 5246 Training loss: 0.40135\n",
      "Epoch: 65/99 Iteration: 5247 Training loss: 0.39920\n",
      "Epoch: 65/99 Iteration: 5248 Training loss: 0.53252\n",
      "Epoch: 65/99 Iteration: 5249 Training loss: 0.33847\n",
      "Epoch: 65/99 Iteration: 5250 Training loss: 0.32579\n",
      "***\n",
      "Epoch: 65/99 Iteration: 5250 Validation Acc: 0.8420\n",
      "***\n",
      "Epoch: 65/99 Iteration: 5251 Training loss: 0.40563\n",
      "Epoch: 65/99 Iteration: 5252 Training loss: 0.38752\n",
      "Epoch: 65/99 Iteration: 5253 Training loss: 0.33840\n",
      "Epoch: 65/99 Iteration: 5254 Training loss: 0.42941\n",
      "Epoch: 65/99 Iteration: 5255 Training loss: 0.33654\n",
      "Epoch: 65/99 Iteration: 5256 Training loss: 0.38455\n",
      "Epoch: 65/99 Iteration: 5257 Training loss: 0.31291\n",
      "Epoch: 65/99 Iteration: 5258 Training loss: 0.39961\n",
      "Epoch: 65/99 Iteration: 5259 Training loss: 0.44892\n",
      "Epoch: 65/99 Iteration: 5260 Training loss: 0.28132\n",
      "Epoch: 65/99 Iteration: 5261 Training loss: 0.48135\n",
      "Epoch: 65/99 Iteration: 5262 Training loss: 0.45271\n",
      "Epoch: 65/99 Iteration: 5263 Training loss: 0.31778\n",
      "Epoch: 65/99 Iteration: 5264 Training loss: 0.42587\n",
      "Epoch: 65/99 Iteration: 5265 Training loss: 0.34111\n",
      "Epoch: 65/99 Iteration: 5266 Training loss: 0.46363\n",
      "Epoch: 65/99 Iteration: 5267 Training loss: 0.40202\n",
      "Epoch: 65/99 Iteration: 5268 Training loss: 0.32815\n",
      "Epoch: 65/99 Iteration: 5269 Training loss: 0.44059\n",
      "Epoch: 65/99 Iteration: 5270 Training loss: 0.46262\n",
      "Epoch: 65/99 Iteration: 5271 Training loss: 0.25467\n",
      "Epoch: 65/99 Iteration: 5272 Training loss: 0.41431\n",
      "Epoch: 65/99 Iteration: 5273 Training loss: 0.30321\n",
      "Epoch: 65/99 Iteration: 5274 Training loss: 0.33866\n",
      "Epoch: 65/99 Iteration: 5275 Training loss: 0.39433\n",
      "Epoch: 65/99 Iteration: 5276 Training loss: 0.26608\n",
      "Epoch: 65/99 Iteration: 5277 Training loss: 0.42932\n",
      "Epoch: 65/99 Iteration: 5278 Training loss: 0.45429\n",
      "Epoch: 65/99 Iteration: 5279 Training loss: 0.25397\n",
      "Epoch: 66/99 Iteration: 5280 Training loss: 0.19608\n",
      "Epoch: 66/99 Iteration: 5281 Training loss: 0.45630\n",
      "Epoch: 66/99 Iteration: 5282 Training loss: 0.42035\n",
      "Epoch: 66/99 Iteration: 5283 Training loss: 0.41344\n",
      "Epoch: 66/99 Iteration: 5284 Training loss: 0.31987\n",
      "Epoch: 66/99 Iteration: 5285 Training loss: 0.34203\n",
      "Epoch: 66/99 Iteration: 5286 Training loss: 0.35485\n",
      "Epoch: 66/99 Iteration: 5287 Training loss: 0.48264\n",
      "Epoch: 66/99 Iteration: 5288 Training loss: 0.42702\n",
      "Epoch: 66/99 Iteration: 5289 Training loss: 0.20769\n",
      "Epoch: 66/99 Iteration: 5290 Training loss: 0.27697\n",
      "Epoch: 66/99 Iteration: 5291 Training loss: 0.29492\n",
      "Epoch: 66/99 Iteration: 5292 Training loss: 0.24995\n",
      "Epoch: 66/99 Iteration: 5293 Training loss: 0.33786\n",
      "Epoch: 66/99 Iteration: 5294 Training loss: 0.48735\n",
      "Epoch: 66/99 Iteration: 5295 Training loss: 0.32671\n",
      "Epoch: 66/99 Iteration: 5296 Training loss: 0.27387\n",
      "Epoch: 66/99 Iteration: 5297 Training loss: 0.27559\n",
      "Epoch: 66/99 Iteration: 5298 Training loss: 0.50153\n",
      "Epoch: 66/99 Iteration: 5299 Training loss: 0.32764\n",
      "Epoch: 66/99 Iteration: 5300 Training loss: 0.31462\n",
      "***\n",
      "Epoch: 66/99 Iteration: 5300 Validation Acc: 0.8430\n",
      "***\n",
      "Epoch: 66/99 Iteration: 5301 Training loss: 0.34516\n",
      "Epoch: 66/99 Iteration: 5302 Training loss: 0.44717\n",
      "Epoch: 66/99 Iteration: 5303 Training loss: 0.41974\n",
      "Epoch: 66/99 Iteration: 5304 Training loss: 0.29134\n",
      "Epoch: 66/99 Iteration: 5305 Training loss: 0.44455\n",
      "Epoch: 66/99 Iteration: 5306 Training loss: 0.35839\n",
      "Epoch: 66/99 Iteration: 5307 Training loss: 0.37572\n",
      "Epoch: 66/99 Iteration: 5308 Training loss: 0.51923\n",
      "Epoch: 66/99 Iteration: 5309 Training loss: 0.65380\n",
      "Epoch: 66/99 Iteration: 5310 Training loss: 0.42512\n",
      "Epoch: 66/99 Iteration: 5311 Training loss: 0.36686\n",
      "Epoch: 66/99 Iteration: 5312 Training loss: 0.44300\n",
      "Epoch: 66/99 Iteration: 5313 Training loss: 0.32340\n",
      "Epoch: 66/99 Iteration: 5314 Training loss: 0.37888\n",
      "Epoch: 66/99 Iteration: 5315 Training loss: 0.37936\n",
      "Epoch: 66/99 Iteration: 5316 Training loss: 0.47345\n",
      "Epoch: 66/99 Iteration: 5317 Training loss: 0.34259\n",
      "Epoch: 66/99 Iteration: 5318 Training loss: 0.41499\n",
      "Epoch: 66/99 Iteration: 5319 Training loss: 0.34089\n",
      "Epoch: 66/99 Iteration: 5320 Training loss: 0.41824\n",
      "Epoch: 66/99 Iteration: 5321 Training loss: 0.34737\n",
      "Epoch: 66/99 Iteration: 5322 Training loss: 0.47327\n",
      "Epoch: 66/99 Iteration: 5323 Training loss: 0.25793\n",
      "Epoch: 66/99 Iteration: 5324 Training loss: 0.43857\n",
      "Epoch: 66/99 Iteration: 5325 Training loss: 0.35953\n",
      "Epoch: 66/99 Iteration: 5326 Training loss: 0.42189\n",
      "Epoch: 66/99 Iteration: 5327 Training loss: 0.33850\n",
      "Epoch: 66/99 Iteration: 5328 Training loss: 0.46986\n",
      "Epoch: 66/99 Iteration: 5329 Training loss: 0.35724\n",
      "Epoch: 66/99 Iteration: 5330 Training loss: 0.32943\n",
      "Epoch: 66/99 Iteration: 5331 Training loss: 0.32497\n",
      "Epoch: 66/99 Iteration: 5332 Training loss: 0.42528\n",
      "Epoch: 66/99 Iteration: 5333 Training loss: 0.38390\n",
      "Epoch: 66/99 Iteration: 5334 Training loss: 0.37969\n",
      "Epoch: 66/99 Iteration: 5335 Training loss: 0.26368\n",
      "Epoch: 66/99 Iteration: 5336 Training loss: 0.31690\n",
      "Epoch: 66/99 Iteration: 5337 Training loss: 0.47707\n",
      "Epoch: 66/99 Iteration: 5338 Training loss: 0.23886\n",
      "Epoch: 66/99 Iteration: 5339 Training loss: 0.52801\n",
      "Epoch: 66/99 Iteration: 5340 Training loss: 0.35674\n",
      "Epoch: 66/99 Iteration: 5341 Training loss: 0.33315\n",
      "Epoch: 66/99 Iteration: 5342 Training loss: 0.46736\n",
      "Epoch: 66/99 Iteration: 5343 Training loss: 0.43624\n",
      "Epoch: 66/99 Iteration: 5344 Training loss: 0.34010\n",
      "Epoch: 66/99 Iteration: 5345 Training loss: 0.50520\n",
      "Epoch: 66/99 Iteration: 5346 Training loss: 0.43906\n",
      "Epoch: 66/99 Iteration: 5347 Training loss: 0.48913\n",
      "Epoch: 66/99 Iteration: 5348 Training loss: 0.24404\n",
      "Epoch: 66/99 Iteration: 5349 Training loss: 0.50622\n",
      "Epoch: 66/99 Iteration: 5350 Training loss: 0.48240\n",
      "***\n",
      "Epoch: 66/99 Iteration: 5350 Validation Acc: 0.8460\n",
      "***\n",
      "Epoch: 66/99 Iteration: 5351 Training loss: 0.30355\n",
      "Epoch: 66/99 Iteration: 5352 Training loss: 0.36073\n",
      "Epoch: 66/99 Iteration: 5353 Training loss: 0.45201\n",
      "Epoch: 66/99 Iteration: 5354 Training loss: 0.32281\n",
      "Epoch: 66/99 Iteration: 5355 Training loss: 0.41363\n",
      "Epoch: 66/99 Iteration: 5356 Training loss: 0.48139\n",
      "Epoch: 66/99 Iteration: 5357 Training loss: 0.34970\n",
      "Epoch: 66/99 Iteration: 5358 Training loss: 0.40409\n",
      "Epoch: 66/99 Iteration: 5359 Training loss: 0.28747\n",
      "Epoch: 67/99 Iteration: 5360 Training loss: 0.26420\n",
      "Epoch: 67/99 Iteration: 5361 Training loss: 0.48500\n",
      "Epoch: 67/99 Iteration: 5362 Training loss: 0.37826\n",
      "Epoch: 67/99 Iteration: 5363 Training loss: 0.39072\n",
      "Epoch: 67/99 Iteration: 5364 Training loss: 0.36867\n",
      "Epoch: 67/99 Iteration: 5365 Training loss: 0.33337\n",
      "Epoch: 67/99 Iteration: 5366 Training loss: 0.24484\n",
      "Epoch: 67/99 Iteration: 5367 Training loss: 0.31216\n",
      "Epoch: 67/99 Iteration: 5368 Training loss: 0.51132\n",
      "Epoch: 67/99 Iteration: 5369 Training loss: 0.29199\n",
      "Epoch: 67/99 Iteration: 5370 Training loss: 0.26629\n",
      "Epoch: 67/99 Iteration: 5371 Training loss: 0.34935\n",
      "Epoch: 67/99 Iteration: 5372 Training loss: 0.34342\n",
      "Epoch: 67/99 Iteration: 5373 Training loss: 0.39632\n",
      "Epoch: 67/99 Iteration: 5374 Training loss: 0.31528\n",
      "Epoch: 67/99 Iteration: 5375 Training loss: 0.31678\n",
      "Epoch: 67/99 Iteration: 5376 Training loss: 0.23519\n",
      "Epoch: 67/99 Iteration: 5377 Training loss: 0.25534\n",
      "Epoch: 67/99 Iteration: 5378 Training loss: 0.41043\n",
      "Epoch: 67/99 Iteration: 5379 Training loss: 0.32546\n",
      "Epoch: 67/99 Iteration: 5380 Training loss: 0.38589\n",
      "Epoch: 67/99 Iteration: 5381 Training loss: 0.25078\n",
      "Epoch: 67/99 Iteration: 5382 Training loss: 0.47297\n",
      "Epoch: 67/99 Iteration: 5383 Training loss: 0.38546\n",
      "Epoch: 67/99 Iteration: 5384 Training loss: 0.27799\n",
      "Epoch: 67/99 Iteration: 5385 Training loss: 0.35950\n",
      "Epoch: 67/99 Iteration: 5386 Training loss: 0.29895\n",
      "Epoch: 67/99 Iteration: 5387 Training loss: 0.30468\n",
      "Epoch: 67/99 Iteration: 5388 Training loss: 0.38960\n",
      "Epoch: 67/99 Iteration: 5389 Training loss: 0.41705\n",
      "Epoch: 67/99 Iteration: 5390 Training loss: 0.24085\n",
      "Epoch: 67/99 Iteration: 5391 Training loss: 0.33138\n",
      "Epoch: 67/99 Iteration: 5392 Training loss: 0.29481\n",
      "Epoch: 67/99 Iteration: 5393 Training loss: 0.38656\n",
      "Epoch: 67/99 Iteration: 5394 Training loss: 0.31667\n",
      "Epoch: 67/99 Iteration: 5395 Training loss: 0.26719\n",
      "Epoch: 67/99 Iteration: 5396 Training loss: 0.46034\n",
      "Epoch: 67/99 Iteration: 5397 Training loss: 0.41313\n",
      "Epoch: 67/99 Iteration: 5398 Training loss: 0.26449\n",
      "Epoch: 67/99 Iteration: 5399 Training loss: 0.47297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/99 Iteration: 5400 Training loss: 0.39530\n",
      "***\n",
      "Epoch: 67/99 Iteration: 5400 Validation Acc: 0.8330\n",
      "***\n",
      "Epoch: 67/99 Iteration: 5401 Training loss: 0.46685\n",
      "Epoch: 67/99 Iteration: 5402 Training loss: 0.35995\n",
      "Epoch: 67/99 Iteration: 5403 Training loss: 0.27361\n",
      "Epoch: 67/99 Iteration: 5404 Training loss: 0.44434\n",
      "Epoch: 67/99 Iteration: 5405 Training loss: 0.27845\n",
      "Epoch: 67/99 Iteration: 5406 Training loss: 0.48331\n",
      "Epoch: 67/99 Iteration: 5407 Training loss: 0.39338\n",
      "Epoch: 67/99 Iteration: 5408 Training loss: 0.51379\n",
      "Epoch: 67/99 Iteration: 5409 Training loss: 0.34287\n",
      "Epoch: 67/99 Iteration: 5410 Training loss: 0.39665\n",
      "Epoch: 67/99 Iteration: 5411 Training loss: 0.35246\n",
      "Epoch: 67/99 Iteration: 5412 Training loss: 0.24537\n",
      "Epoch: 67/99 Iteration: 5413 Training loss: 0.33795\n",
      "Epoch: 67/99 Iteration: 5414 Training loss: 0.39264\n",
      "Epoch: 67/99 Iteration: 5415 Training loss: 0.34615\n",
      "Epoch: 67/99 Iteration: 5416 Training loss: 0.31091\n",
      "Epoch: 67/99 Iteration: 5417 Training loss: 0.37387\n",
      "Epoch: 67/99 Iteration: 5418 Training loss: 0.30344\n",
      "Epoch: 67/99 Iteration: 5419 Training loss: 0.38128\n",
      "Epoch: 67/99 Iteration: 5420 Training loss: 0.43857\n",
      "Epoch: 67/99 Iteration: 5421 Training loss: 0.49616\n",
      "Epoch: 67/99 Iteration: 5422 Training loss: 0.32899\n",
      "Epoch: 67/99 Iteration: 5423 Training loss: 0.30179\n",
      "Epoch: 67/99 Iteration: 5424 Training loss: 0.38339\n",
      "Epoch: 67/99 Iteration: 5425 Training loss: 0.41538\n",
      "Epoch: 67/99 Iteration: 5426 Training loss: 0.35303\n",
      "Epoch: 67/99 Iteration: 5427 Training loss: 0.34791\n",
      "Epoch: 67/99 Iteration: 5428 Training loss: 0.29148\n",
      "Epoch: 67/99 Iteration: 5429 Training loss: 0.24310\n",
      "Epoch: 67/99 Iteration: 5430 Training loss: 0.31906\n",
      "Epoch: 67/99 Iteration: 5431 Training loss: 0.39642\n",
      "Epoch: 67/99 Iteration: 5432 Training loss: 0.45597\n",
      "Epoch: 67/99 Iteration: 5433 Training loss: 0.22052\n",
      "Epoch: 67/99 Iteration: 5434 Training loss: 0.37313\n",
      "Epoch: 67/99 Iteration: 5435 Training loss: 0.29170\n",
      "Epoch: 67/99 Iteration: 5436 Training loss: 0.29680\n",
      "Epoch: 67/99 Iteration: 5437 Training loss: 0.35032\n",
      "Epoch: 67/99 Iteration: 5438 Training loss: 0.26887\n",
      "Epoch: 67/99 Iteration: 5439 Training loss: 0.35325\n",
      "Epoch: 68/99 Iteration: 5440 Training loss: 0.36758\n",
      "Epoch: 68/99 Iteration: 5441 Training loss: 0.41142\n",
      "Epoch: 68/99 Iteration: 5442 Training loss: 0.38605\n",
      "Epoch: 68/99 Iteration: 5443 Training loss: 0.34877\n",
      "Epoch: 68/99 Iteration: 5444 Training loss: 0.25238\n",
      "Epoch: 68/99 Iteration: 5445 Training loss: 0.33712\n",
      "Epoch: 68/99 Iteration: 5446 Training loss: 0.21951\n",
      "Epoch: 68/99 Iteration: 5447 Training loss: 0.31801\n",
      "Epoch: 68/99 Iteration: 5448 Training loss: 0.41937\n",
      "Epoch: 68/99 Iteration: 5449 Training loss: 0.22797\n",
      "Epoch: 68/99 Iteration: 5450 Training loss: 0.20189\n",
      "***\n",
      "Epoch: 68/99 Iteration: 5450 Validation Acc: 0.8650\n",
      "***\n",
      "Epoch: 68/99 Iteration: 5451 Training loss: 0.25908\n",
      "Epoch: 68/99 Iteration: 5452 Training loss: 0.27300\n",
      "Epoch: 68/99 Iteration: 5453 Training loss: 0.37337\n",
      "Epoch: 68/99 Iteration: 5454 Training loss: 0.32700\n",
      "Epoch: 68/99 Iteration: 5455 Training loss: 0.15833\n",
      "Epoch: 68/99 Iteration: 5456 Training loss: 0.35614\n",
      "Epoch: 68/99 Iteration: 5457 Training loss: 0.25220\n",
      "Epoch: 68/99 Iteration: 5458 Training loss: 0.41416\n",
      "Epoch: 68/99 Iteration: 5459 Training loss: 0.33494\n",
      "Epoch: 68/99 Iteration: 5460 Training loss: 0.28398\n",
      "Epoch: 68/99 Iteration: 5461 Training loss: 0.26588\n",
      "Epoch: 68/99 Iteration: 5462 Training loss: 0.33184\n",
      "Epoch: 68/99 Iteration: 5463 Training loss: 0.22064\n",
      "Epoch: 68/99 Iteration: 5464 Training loss: 0.35391\n",
      "Epoch: 68/99 Iteration: 5465 Training loss: 0.35949\n",
      "Epoch: 68/99 Iteration: 5466 Training loss: 0.29216\n",
      "Epoch: 68/99 Iteration: 5467 Training loss: 0.28710\n",
      "Epoch: 68/99 Iteration: 5468 Training loss: 0.41151\n",
      "Epoch: 68/99 Iteration: 5469 Training loss: 0.34702\n",
      "Epoch: 68/99 Iteration: 5470 Training loss: 0.34117\n",
      "Epoch: 68/99 Iteration: 5471 Training loss: 0.32371\n",
      "Epoch: 68/99 Iteration: 5472 Training loss: 0.24366\n",
      "Epoch: 68/99 Iteration: 5473 Training loss: 0.34849\n",
      "Epoch: 68/99 Iteration: 5474 Training loss: 0.23491\n",
      "Epoch: 68/99 Iteration: 5475 Training loss: 0.37257\n",
      "Epoch: 68/99 Iteration: 5476 Training loss: 0.47389\n",
      "Epoch: 68/99 Iteration: 5477 Training loss: 0.30020\n",
      "Epoch: 68/99 Iteration: 5478 Training loss: 0.24155\n",
      "Epoch: 68/99 Iteration: 5479 Training loss: 0.27738\n",
      "Epoch: 68/99 Iteration: 5480 Training loss: 0.33407\n",
      "Epoch: 68/99 Iteration: 5481 Training loss: 0.36375\n",
      "Epoch: 68/99 Iteration: 5482 Training loss: 0.35405\n",
      "Epoch: 68/99 Iteration: 5483 Training loss: 0.36368\n",
      "Epoch: 68/99 Iteration: 5484 Training loss: 0.34100\n",
      "Epoch: 68/99 Iteration: 5485 Training loss: 0.27544\n",
      "Epoch: 68/99 Iteration: 5486 Training loss: 0.39366\n",
      "Epoch: 68/99 Iteration: 5487 Training loss: 0.26806\n",
      "Epoch: 68/99 Iteration: 5488 Training loss: 0.40559\n",
      "Epoch: 68/99 Iteration: 5489 Training loss: 0.18811\n",
      "Epoch: 68/99 Iteration: 5490 Training loss: 0.25294\n",
      "Epoch: 68/99 Iteration: 5491 Training loss: 0.41850\n",
      "Epoch: 68/99 Iteration: 5492 Training loss: 0.37042\n",
      "Epoch: 68/99 Iteration: 5493 Training loss: 0.31158\n",
      "Epoch: 68/99 Iteration: 5494 Training loss: 0.22434\n",
      "Epoch: 68/99 Iteration: 5495 Training loss: 0.52533\n",
      "Epoch: 68/99 Iteration: 5496 Training loss: 0.43884\n",
      "Epoch: 68/99 Iteration: 5497 Training loss: 0.39684\n",
      "Epoch: 68/99 Iteration: 5498 Training loss: 0.34010\n",
      "Epoch: 68/99 Iteration: 5499 Training loss: 0.38516\n",
      "Epoch: 68/99 Iteration: 5500 Training loss: 0.33017\n",
      "***\n",
      "Epoch: 68/99 Iteration: 5500 Validation Acc: 0.8560\n",
      "***\n",
      "Epoch: 68/99 Iteration: 5501 Training loss: 0.35382\n",
      "Epoch: 68/99 Iteration: 5502 Training loss: 0.32577\n",
      "Epoch: 68/99 Iteration: 5503 Training loss: 0.20676\n",
      "Epoch: 68/99 Iteration: 5504 Training loss: 0.22897\n",
      "Epoch: 68/99 Iteration: 5505 Training loss: 0.32701\n",
      "Epoch: 68/99 Iteration: 5506 Training loss: 0.35976\n",
      "Epoch: 68/99 Iteration: 5507 Training loss: 0.36044\n",
      "Epoch: 68/99 Iteration: 5508 Training loss: 0.34402\n",
      "Epoch: 68/99 Iteration: 5509 Training loss: 0.24427\n",
      "Epoch: 68/99 Iteration: 5510 Training loss: 0.42387\n",
      "Epoch: 68/99 Iteration: 5511 Training loss: 0.17525\n",
      "Epoch: 68/99 Iteration: 5512 Training loss: 0.39635\n",
      "Epoch: 68/99 Iteration: 5513 Training loss: 0.25561\n",
      "Epoch: 68/99 Iteration: 5514 Training loss: 0.21895\n",
      "Epoch: 68/99 Iteration: 5515 Training loss: 0.17955\n",
      "Epoch: 68/99 Iteration: 5516 Training loss: 0.33043\n",
      "Epoch: 68/99 Iteration: 5517 Training loss: 0.37900\n",
      "Epoch: 68/99 Iteration: 5518 Training loss: 0.34672\n",
      "Epoch: 68/99 Iteration: 5519 Training loss: 0.28485\n",
      "Epoch: 69/99 Iteration: 5520 Training loss: 0.19188\n",
      "Epoch: 69/99 Iteration: 5521 Training loss: 0.35001\n",
      "Epoch: 69/99 Iteration: 5522 Training loss: 0.29036\n",
      "Epoch: 69/99 Iteration: 5523 Training loss: 0.39274\n",
      "Epoch: 69/99 Iteration: 5524 Training loss: 0.35723\n",
      "Epoch: 69/99 Iteration: 5525 Training loss: 0.36188\n",
      "Epoch: 69/99 Iteration: 5526 Training loss: 0.21529\n",
      "Epoch: 69/99 Iteration: 5527 Training loss: 0.27845\n",
      "Epoch: 69/99 Iteration: 5528 Training loss: 0.38730\n",
      "Epoch: 69/99 Iteration: 5529 Training loss: 0.17692\n",
      "Epoch: 69/99 Iteration: 5530 Training loss: 0.28502\n",
      "Epoch: 69/99 Iteration: 5531 Training loss: 0.20296\n",
      "Epoch: 69/99 Iteration: 5532 Training loss: 0.27184\n",
      "Epoch: 69/99 Iteration: 5533 Training loss: 0.42152\n",
      "Epoch: 69/99 Iteration: 5534 Training loss: 0.31224\n",
      "Epoch: 69/99 Iteration: 5535 Training loss: 0.28304\n",
      "Epoch: 69/99 Iteration: 5536 Training loss: 0.24278\n",
      "Epoch: 69/99 Iteration: 5537 Training loss: 0.15670\n",
      "Epoch: 69/99 Iteration: 5538 Training loss: 0.41759\n",
      "Epoch: 69/99 Iteration: 5539 Training loss: 0.27840\n",
      "Epoch: 69/99 Iteration: 5540 Training loss: 0.33619\n",
      "Epoch: 69/99 Iteration: 5541 Training loss: 0.31996\n",
      "Epoch: 69/99 Iteration: 5542 Training loss: 0.32429\n",
      "Epoch: 69/99 Iteration: 5543 Training loss: 0.32883\n",
      "Epoch: 69/99 Iteration: 5544 Training loss: 0.31744\n",
      "Epoch: 69/99 Iteration: 5545 Training loss: 0.24726\n",
      "Epoch: 69/99 Iteration: 5546 Training loss: 0.36453\n",
      "Epoch: 69/99 Iteration: 5547 Training loss: 0.29318\n",
      "Epoch: 69/99 Iteration: 5548 Training loss: 0.52057\n",
      "Epoch: 69/99 Iteration: 5549 Training loss: 0.40662\n",
      "Epoch: 69/99 Iteration: 5550 Training loss: 0.40845\n",
      "***\n",
      "Epoch: 69/99 Iteration: 5550 Validation Acc: 0.8520\n",
      "***\n",
      "Epoch: 69/99 Iteration: 5551 Training loss: 0.40938\n",
      "Epoch: 69/99 Iteration: 5552 Training loss: 0.42883\n",
      "Epoch: 69/99 Iteration: 5553 Training loss: 0.27811\n",
      "Epoch: 69/99 Iteration: 5554 Training loss: 0.25198\n",
      "Epoch: 69/99 Iteration: 5555 Training loss: 0.31891\n",
      "Epoch: 69/99 Iteration: 5556 Training loss: 0.44496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/99 Iteration: 5557 Training loss: 0.24318\n",
      "Epoch: 69/99 Iteration: 5558 Training loss: 0.30860\n",
      "Epoch: 69/99 Iteration: 5559 Training loss: 0.33927\n",
      "Epoch: 69/99 Iteration: 5560 Training loss: 0.33437\n",
      "Epoch: 69/99 Iteration: 5561 Training loss: 0.38529\n",
      "Epoch: 69/99 Iteration: 5562 Training loss: 0.43562\n",
      "Epoch: 69/99 Iteration: 5563 Training loss: 0.27081\n",
      "Epoch: 69/99 Iteration: 5564 Training loss: 0.47083\n",
      "Epoch: 69/99 Iteration: 5565 Training loss: 0.40044\n",
      "Epoch: 69/99 Iteration: 5566 Training loss: 0.39922\n",
      "Epoch: 69/99 Iteration: 5567 Training loss: 0.34426\n",
      "Epoch: 69/99 Iteration: 5568 Training loss: 0.48669\n",
      "Epoch: 69/99 Iteration: 5569 Training loss: 0.38703\n",
      "Epoch: 69/99 Iteration: 5570 Training loss: 0.50510\n",
      "Epoch: 69/99 Iteration: 5571 Training loss: 0.42996\n",
      "Epoch: 69/99 Iteration: 5572 Training loss: 0.38954\n",
      "Epoch: 69/99 Iteration: 5573 Training loss: 0.43221\n",
      "Epoch: 69/99 Iteration: 5574 Training loss: 0.38467\n",
      "Epoch: 69/99 Iteration: 5575 Training loss: 0.28290\n",
      "Epoch: 69/99 Iteration: 5576 Training loss: 0.36650\n",
      "Epoch: 69/99 Iteration: 5577 Training loss: 0.33704\n",
      "Epoch: 69/99 Iteration: 5578 Training loss: 0.36935\n",
      "Epoch: 69/99 Iteration: 5579 Training loss: 0.41812\n",
      "Epoch: 69/99 Iteration: 5580 Training loss: 0.21180\n",
      "Epoch: 69/99 Iteration: 5581 Training loss: 0.37117\n",
      "Epoch: 69/99 Iteration: 5582 Training loss: 0.39958\n",
      "Epoch: 69/99 Iteration: 5583 Training loss: 0.46996\n",
      "Epoch: 69/99 Iteration: 5584 Training loss: 0.26046\n",
      "Epoch: 69/99 Iteration: 5585 Training loss: 0.42226\n",
      "Epoch: 69/99 Iteration: 5586 Training loss: 0.31480\n",
      "Epoch: 69/99 Iteration: 5587 Training loss: 0.36302\n",
      "Epoch: 69/99 Iteration: 5588 Training loss: 0.37613\n",
      "Epoch: 69/99 Iteration: 5589 Training loss: 0.32040\n",
      "Epoch: 69/99 Iteration: 5590 Training loss: 0.42822\n",
      "Epoch: 69/99 Iteration: 5591 Training loss: 0.42013\n",
      "Epoch: 69/99 Iteration: 5592 Training loss: 0.40588\n",
      "Epoch: 69/99 Iteration: 5593 Training loss: 0.37936\n",
      "Epoch: 69/99 Iteration: 5594 Training loss: 0.41505\n",
      "Epoch: 69/99 Iteration: 5595 Training loss: 0.36640\n",
      "Epoch: 69/99 Iteration: 5596 Training loss: 0.46771\n",
      "Epoch: 69/99 Iteration: 5597 Training loss: 0.52649\n",
      "Epoch: 69/99 Iteration: 5598 Training loss: 0.31289\n",
      "Epoch: 69/99 Iteration: 5599 Training loss: 0.33133\n",
      "Epoch: 70/99 Iteration: 5600 Training loss: 0.32213\n",
      "***\n",
      "Epoch: 70/99 Iteration: 5600 Validation Acc: 0.8570\n",
      "***\n",
      "Epoch: 70/99 Iteration: 5601 Training loss: 0.47935\n",
      "Epoch: 70/99 Iteration: 5602 Training loss: 0.41088\n",
      "Epoch: 70/99 Iteration: 5603 Training loss: 0.44709\n",
      "Epoch: 70/99 Iteration: 5604 Training loss: 0.23980\n",
      "Epoch: 70/99 Iteration: 5605 Training loss: 0.30670\n",
      "Epoch: 70/99 Iteration: 5606 Training loss: 0.35177\n",
      "Epoch: 70/99 Iteration: 5607 Training loss: 0.24868\n",
      "Epoch: 70/99 Iteration: 5608 Training loss: 0.50406\n",
      "Epoch: 70/99 Iteration: 5609 Training loss: 0.26463\n",
      "Epoch: 70/99 Iteration: 5610 Training loss: 0.34516\n",
      "Epoch: 70/99 Iteration: 5611 Training loss: 0.24984\n",
      "Epoch: 70/99 Iteration: 5612 Training loss: 0.27315\n",
      "Epoch: 70/99 Iteration: 5613 Training loss: 0.39041\n",
      "Epoch: 70/99 Iteration: 5614 Training loss: 0.39303\n",
      "Epoch: 70/99 Iteration: 5615 Training loss: 0.21383\n",
      "Epoch: 70/99 Iteration: 5616 Training loss: 0.34293\n",
      "Epoch: 70/99 Iteration: 5617 Training loss: 0.20757\n",
      "Epoch: 70/99 Iteration: 5618 Training loss: 0.25214\n",
      "Epoch: 70/99 Iteration: 5619 Training loss: 0.28577\n",
      "Epoch: 70/99 Iteration: 5620 Training loss: 0.38079\n",
      "Epoch: 70/99 Iteration: 5621 Training loss: 0.31919\n",
      "Epoch: 70/99 Iteration: 5622 Training loss: 0.34789\n",
      "Epoch: 70/99 Iteration: 5623 Training loss: 0.23472\n",
      "Epoch: 70/99 Iteration: 5624 Training loss: 0.26229\n",
      "Epoch: 70/99 Iteration: 5625 Training loss: 0.24599\n",
      "Epoch: 70/99 Iteration: 5626 Training loss: 0.30638\n",
      "Epoch: 70/99 Iteration: 5627 Training loss: 0.22969\n",
      "Epoch: 70/99 Iteration: 5628 Training loss: 0.28193\n",
      "Epoch: 70/99 Iteration: 5629 Training loss: 0.35279\n",
      "Epoch: 70/99 Iteration: 5630 Training loss: 0.24428\n",
      "Epoch: 70/99 Iteration: 5631 Training loss: 0.22072\n",
      "Epoch: 70/99 Iteration: 5632 Training loss: 0.36110\n",
      "Epoch: 70/99 Iteration: 5633 Training loss: 0.28417\n",
      "Epoch: 70/99 Iteration: 5634 Training loss: 0.22080\n",
      "Epoch: 70/99 Iteration: 5635 Training loss: 0.30229\n",
      "Epoch: 70/99 Iteration: 5636 Training loss: 0.45385\n",
      "Epoch: 70/99 Iteration: 5637 Training loss: 0.30183\n",
      "Epoch: 70/99 Iteration: 5638 Training loss: 0.37446\n",
      "Epoch: 70/99 Iteration: 5639 Training loss: 0.36127\n",
      "Epoch: 70/99 Iteration: 5640 Training loss: 0.32798\n",
      "Epoch: 70/99 Iteration: 5641 Training loss: 0.37067\n",
      "Epoch: 70/99 Iteration: 5642 Training loss: 0.50200\n",
      "Epoch: 70/99 Iteration: 5643 Training loss: 0.28500\n",
      "Epoch: 70/99 Iteration: 5644 Training loss: 0.32401\n",
      "Epoch: 70/99 Iteration: 5645 Training loss: 0.30068\n",
      "Epoch: 70/99 Iteration: 5646 Training loss: 0.27046\n",
      "Epoch: 70/99 Iteration: 5647 Training loss: 0.31927\n",
      "Epoch: 70/99 Iteration: 5648 Training loss: 0.47175\n",
      "Epoch: 70/99 Iteration: 5649 Training loss: 0.25094\n",
      "Epoch: 70/99 Iteration: 5650 Training loss: 0.31417\n",
      "***\n",
      "Epoch: 70/99 Iteration: 5650 Validation Acc: 0.8470\n",
      "***\n",
      "Epoch: 70/99 Iteration: 5651 Training loss: 0.32260\n",
      "Epoch: 70/99 Iteration: 5652 Training loss: 0.18881\n",
      "Epoch: 70/99 Iteration: 5653 Training loss: 0.29841\n",
      "Epoch: 70/99 Iteration: 5654 Training loss: 0.40375\n",
      "Epoch: 70/99 Iteration: 5655 Training loss: 0.34754\n",
      "Epoch: 70/99 Iteration: 5656 Training loss: 0.48762\n",
      "Epoch: 70/99 Iteration: 5657 Training loss: 0.25509\n",
      "Epoch: 70/99 Iteration: 5658 Training loss: 0.34254\n",
      "Epoch: 70/99 Iteration: 5659 Training loss: 0.33764\n",
      "Epoch: 70/99 Iteration: 5660 Training loss: 0.18790\n",
      "Epoch: 70/99 Iteration: 5661 Training loss: 0.46624\n",
      "Epoch: 70/99 Iteration: 5662 Training loss: 0.36257\n",
      "Epoch: 70/99 Iteration: 5663 Training loss: 0.36902\n",
      "Epoch: 70/99 Iteration: 5664 Training loss: 0.37534\n",
      "Epoch: 70/99 Iteration: 5665 Training loss: 0.48349\n",
      "Epoch: 70/99 Iteration: 5666 Training loss: 0.34645\n",
      "Epoch: 70/99 Iteration: 5667 Training loss: 0.40904\n",
      "Epoch: 70/99 Iteration: 5668 Training loss: 0.36213\n",
      "Epoch: 70/99 Iteration: 5669 Training loss: 0.32749\n",
      "Epoch: 70/99 Iteration: 5670 Training loss: 0.45193\n",
      "Epoch: 70/99 Iteration: 5671 Training loss: 0.32706\n",
      "Epoch: 70/99 Iteration: 5672 Training loss: 0.40559\n",
      "Epoch: 70/99 Iteration: 5673 Training loss: 0.35153\n",
      "Epoch: 70/99 Iteration: 5674 Training loss: 0.51435\n",
      "Epoch: 70/99 Iteration: 5675 Training loss: 0.36317\n",
      "Epoch: 70/99 Iteration: 5676 Training loss: 0.47514\n",
      "Epoch: 70/99 Iteration: 5677 Training loss: 0.51745\n",
      "Epoch: 70/99 Iteration: 5678 Training loss: 0.50826\n",
      "Epoch: 70/99 Iteration: 5679 Training loss: 0.53470\n",
      "Epoch: 71/99 Iteration: 5680 Training loss: 0.29216\n",
      "Epoch: 71/99 Iteration: 5681 Training loss: 0.41794\n",
      "Epoch: 71/99 Iteration: 5682 Training loss: 0.43486\n",
      "Epoch: 71/99 Iteration: 5683 Training loss: 0.54373\n",
      "Epoch: 71/99 Iteration: 5684 Training loss: 0.51875\n",
      "Epoch: 71/99 Iteration: 5685 Training loss: 0.36051\n",
      "Epoch: 71/99 Iteration: 5686 Training loss: 0.46899\n",
      "Epoch: 71/99 Iteration: 5687 Training loss: 0.44188\n",
      "Epoch: 71/99 Iteration: 5688 Training loss: 0.38592\n",
      "Epoch: 71/99 Iteration: 5689 Training loss: 0.33074\n",
      "Epoch: 71/99 Iteration: 5690 Training loss: 0.36277\n",
      "Epoch: 71/99 Iteration: 5691 Training loss: 0.32279\n",
      "Epoch: 71/99 Iteration: 5692 Training loss: 0.46383\n",
      "Epoch: 71/99 Iteration: 5693 Training loss: 0.46936\n",
      "Epoch: 71/99 Iteration: 5694 Training loss: 0.49150\n",
      "Epoch: 71/99 Iteration: 5695 Training loss: 0.39665\n",
      "Epoch: 71/99 Iteration: 5696 Training loss: 0.27358\n",
      "Epoch: 71/99 Iteration: 5697 Training loss: 0.22647\n",
      "Epoch: 71/99 Iteration: 5698 Training loss: 0.42688\n",
      "Epoch: 71/99 Iteration: 5699 Training loss: 0.41632\n",
      "Epoch: 71/99 Iteration: 5700 Training loss: 0.42289\n",
      "***\n",
      "Epoch: 71/99 Iteration: 5700 Validation Acc: 0.8470\n",
      "***\n",
      "Epoch: 71/99 Iteration: 5701 Training loss: 0.30517\n",
      "Epoch: 71/99 Iteration: 5702 Training loss: 0.36526\n",
      "Epoch: 71/99 Iteration: 5703 Training loss: 0.67361\n",
      "Epoch: 71/99 Iteration: 5704 Training loss: 0.52986\n",
      "Epoch: 71/99 Iteration: 5705 Training loss: 0.39611\n",
      "Epoch: 71/99 Iteration: 5706 Training loss: 0.39004\n",
      "Epoch: 71/99 Iteration: 5707 Training loss: 0.32776\n",
      "Epoch: 71/99 Iteration: 5708 Training loss: 0.43658\n",
      "Epoch: 71/99 Iteration: 5709 Training loss: 0.52450\n",
      "Epoch: 71/99 Iteration: 5710 Training loss: 0.28792\n",
      "Epoch: 71/99 Iteration: 5711 Training loss: 0.26445\n",
      "Epoch: 71/99 Iteration: 5712 Training loss: 0.43832\n",
      "Epoch: 71/99 Iteration: 5713 Training loss: 0.33604\n",
      "Epoch: 71/99 Iteration: 5714 Training loss: 0.28025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/99 Iteration: 5715 Training loss: 0.36640\n",
      "Epoch: 71/99 Iteration: 5716 Training loss: 0.40686\n",
      "Epoch: 71/99 Iteration: 5717 Training loss: 0.35767\n",
      "Epoch: 71/99 Iteration: 5718 Training loss: 0.48966\n",
      "Epoch: 71/99 Iteration: 5719 Training loss: 0.44525\n",
      "Epoch: 71/99 Iteration: 5720 Training loss: 0.34547\n",
      "Epoch: 71/99 Iteration: 5721 Training loss: 0.36154\n",
      "Epoch: 71/99 Iteration: 5722 Training loss: 0.42252\n",
      "Epoch: 71/99 Iteration: 5723 Training loss: 0.35021\n",
      "Epoch: 71/99 Iteration: 5724 Training loss: 0.44082\n",
      "Epoch: 71/99 Iteration: 5725 Training loss: 0.54619\n",
      "Epoch: 71/99 Iteration: 5726 Training loss: 0.58718\n",
      "Epoch: 71/99 Iteration: 5727 Training loss: 0.35948\n",
      "Epoch: 71/99 Iteration: 5728 Training loss: 0.60053\n",
      "Epoch: 71/99 Iteration: 5729 Training loss: 0.38914\n",
      "Epoch: 71/99 Iteration: 5730 Training loss: 0.33179\n",
      "Epoch: 71/99 Iteration: 5731 Training loss: 0.38787\n",
      "Epoch: 71/99 Iteration: 5732 Training loss: 0.49405\n",
      "Epoch: 71/99 Iteration: 5733 Training loss: 0.39809\n",
      "Epoch: 71/99 Iteration: 5734 Training loss: 0.41488\n",
      "Epoch: 71/99 Iteration: 5735 Training loss: 0.44618\n",
      "Epoch: 71/99 Iteration: 5736 Training loss: 0.36738\n",
      "Epoch: 71/99 Iteration: 5737 Training loss: 0.40768\n",
      "Epoch: 71/99 Iteration: 5738 Training loss: 0.40523\n",
      "Epoch: 71/99 Iteration: 5739 Training loss: 0.59724\n",
      "Epoch: 71/99 Iteration: 5740 Training loss: 0.32241\n",
      "Epoch: 71/99 Iteration: 5741 Training loss: 0.39859\n",
      "Epoch: 71/99 Iteration: 5742 Training loss: 0.35447\n",
      "Epoch: 71/99 Iteration: 5743 Training loss: 0.43395\n",
      "Epoch: 71/99 Iteration: 5744 Training loss: 0.35218\n",
      "Epoch: 71/99 Iteration: 5745 Training loss: 0.38110\n",
      "Epoch: 71/99 Iteration: 5746 Training loss: 0.53090\n",
      "Epoch: 71/99 Iteration: 5747 Training loss: 0.43422\n",
      "Epoch: 71/99 Iteration: 5748 Training loss: 0.33713\n",
      "Epoch: 71/99 Iteration: 5749 Training loss: 0.30895\n",
      "Epoch: 71/99 Iteration: 5750 Training loss: 0.44429\n",
      "***\n",
      "Epoch: 71/99 Iteration: 5750 Validation Acc: 0.8570\n",
      "***\n",
      "Epoch: 71/99 Iteration: 5751 Training loss: 0.35711\n",
      "Epoch: 71/99 Iteration: 5752 Training loss: 0.31581\n",
      "Epoch: 71/99 Iteration: 5753 Training loss: 0.42323\n",
      "Epoch: 71/99 Iteration: 5754 Training loss: 0.31491\n",
      "Epoch: 71/99 Iteration: 5755 Training loss: 0.27953\n",
      "Epoch: 71/99 Iteration: 5756 Training loss: 0.54110\n",
      "Epoch: 71/99 Iteration: 5757 Training loss: 0.43300\n",
      "Epoch: 71/99 Iteration: 5758 Training loss: 0.42089\n",
      "Epoch: 71/99 Iteration: 5759 Training loss: 0.31124\n",
      "Epoch: 72/99 Iteration: 5760 Training loss: 0.26521\n",
      "Epoch: 72/99 Iteration: 5761 Training loss: 0.48316\n",
      "Epoch: 72/99 Iteration: 5762 Training loss: 0.37944\n",
      "Epoch: 72/99 Iteration: 5763 Training loss: 0.42605\n",
      "Epoch: 72/99 Iteration: 5764 Training loss: 0.35613\n",
      "Epoch: 72/99 Iteration: 5765 Training loss: 0.28202\n",
      "Epoch: 72/99 Iteration: 5766 Training loss: 0.32917\n",
      "Epoch: 72/99 Iteration: 5767 Training loss: 0.23606\n",
      "Epoch: 72/99 Iteration: 5768 Training loss: 0.48091\n",
      "Epoch: 72/99 Iteration: 5769 Training loss: 0.32058\n",
      "Epoch: 72/99 Iteration: 5770 Training loss: 0.45276\n",
      "Epoch: 72/99 Iteration: 5771 Training loss: 0.40934\n",
      "Epoch: 72/99 Iteration: 5772 Training loss: 0.22504\n",
      "Epoch: 72/99 Iteration: 5773 Training loss: 0.46142\n",
      "Epoch: 72/99 Iteration: 5774 Training loss: 0.40123\n",
      "Epoch: 72/99 Iteration: 5775 Training loss: 0.31649\n",
      "Epoch: 72/99 Iteration: 5776 Training loss: 0.38241\n",
      "Epoch: 72/99 Iteration: 5777 Training loss: 0.38741\n",
      "Epoch: 72/99 Iteration: 5778 Training loss: 0.31833\n",
      "Epoch: 72/99 Iteration: 5779 Training loss: 0.44880\n",
      "Epoch: 72/99 Iteration: 5780 Training loss: 0.29954\n",
      "Epoch: 72/99 Iteration: 5781 Training loss: 0.42394\n",
      "Epoch: 72/99 Iteration: 5782 Training loss: 0.61965\n",
      "Epoch: 72/99 Iteration: 5783 Training loss: 0.34761\n",
      "Epoch: 72/99 Iteration: 5784 Training loss: 0.28140\n",
      "Epoch: 72/99 Iteration: 5785 Training loss: 0.44378\n",
      "Epoch: 72/99 Iteration: 5786 Training loss: 0.39890\n",
      "Epoch: 72/99 Iteration: 5787 Training loss: 0.25818\n",
      "Epoch: 72/99 Iteration: 5788 Training loss: 0.46863\n",
      "Epoch: 72/99 Iteration: 5789 Training loss: 0.56065\n",
      "Epoch: 72/99 Iteration: 5790 Training loss: 0.36158\n",
      "Epoch: 72/99 Iteration: 5791 Training loss: 0.35225\n",
      "Epoch: 72/99 Iteration: 5792 Training loss: 0.35498\n",
      "Epoch: 72/99 Iteration: 5793 Training loss: 0.30697\n",
      "Epoch: 72/99 Iteration: 5794 Training loss: 0.21574\n",
      "Epoch: 72/99 Iteration: 5795 Training loss: 0.38492\n",
      "Epoch: 72/99 Iteration: 5796 Training loss: 0.58106\n",
      "Epoch: 72/99 Iteration: 5797 Training loss: 0.49671\n",
      "Epoch: 72/99 Iteration: 5798 Training loss: 0.36939\n",
      "Epoch: 72/99 Iteration: 5799 Training loss: 0.39056\n",
      "Epoch: 72/99 Iteration: 5800 Training loss: 0.33418\n",
      "***\n",
      "Epoch: 72/99 Iteration: 5800 Validation Acc: 0.8330\n",
      "***\n",
      "Epoch: 72/99 Iteration: 5801 Training loss: 0.30274\n",
      "Epoch: 72/99 Iteration: 5802 Training loss: 0.36466\n",
      "Epoch: 72/99 Iteration: 5803 Training loss: 0.44535\n",
      "Epoch: 72/99 Iteration: 5804 Training loss: 0.37237\n",
      "Epoch: 72/99 Iteration: 5805 Training loss: 0.35757\n",
      "Epoch: 72/99 Iteration: 5806 Training loss: 0.39430\n",
      "Epoch: 72/99 Iteration: 5807 Training loss: 0.38766\n",
      "Epoch: 72/99 Iteration: 5808 Training loss: 0.54512\n",
      "Epoch: 72/99 Iteration: 5809 Training loss: 0.41951\n",
      "Epoch: 72/99 Iteration: 5810 Training loss: 0.27242\n",
      "Epoch: 72/99 Iteration: 5811 Training loss: 0.44147\n",
      "Epoch: 72/99 Iteration: 5812 Training loss: 0.38910\n",
      "Epoch: 72/99 Iteration: 5813 Training loss: 0.35289\n",
      "Epoch: 72/99 Iteration: 5814 Training loss: 0.42915\n",
      "Epoch: 72/99 Iteration: 5815 Training loss: 0.34624\n",
      "Epoch: 72/99 Iteration: 5816 Training loss: 0.36762\n",
      "Epoch: 72/99 Iteration: 5817 Training loss: 0.43381\n",
      "Epoch: 72/99 Iteration: 5818 Training loss: 0.39099\n",
      "Epoch: 72/99 Iteration: 5819 Training loss: 0.36454\n",
      "Epoch: 72/99 Iteration: 5820 Training loss: 0.34399\n",
      "Epoch: 72/99 Iteration: 5821 Training loss: 0.43046\n",
      "Epoch: 72/99 Iteration: 5822 Training loss: 0.27299\n",
      "Epoch: 72/99 Iteration: 5823 Training loss: 0.50099\n",
      "Epoch: 72/99 Iteration: 5824 Training loss: 0.38282\n",
      "Epoch: 72/99 Iteration: 5825 Training loss: 0.36485\n",
      "Epoch: 72/99 Iteration: 5826 Training loss: 0.41415\n",
      "Epoch: 72/99 Iteration: 5827 Training loss: 0.33597\n",
      "Epoch: 72/99 Iteration: 5828 Training loss: 0.45395\n",
      "Epoch: 72/99 Iteration: 5829 Training loss: 0.40276\n",
      "Epoch: 72/99 Iteration: 5830 Training loss: 0.63689\n",
      "Epoch: 72/99 Iteration: 5831 Training loss: 0.27496\n",
      "Epoch: 72/99 Iteration: 5832 Training loss: 0.54240\n",
      "Epoch: 72/99 Iteration: 5833 Training loss: 0.34002\n",
      "Epoch: 72/99 Iteration: 5834 Training loss: 0.37414\n",
      "Epoch: 72/99 Iteration: 5835 Training loss: 0.37263\n",
      "Epoch: 72/99 Iteration: 5836 Training loss: 0.33611\n",
      "Epoch: 72/99 Iteration: 5837 Training loss: 0.50989\n",
      "Epoch: 72/99 Iteration: 5838 Training loss: 0.41006\n",
      "Epoch: 72/99 Iteration: 5839 Training loss: 0.39921\n",
      "Epoch: 73/99 Iteration: 5840 Training loss: 0.39159\n",
      "Epoch: 73/99 Iteration: 5841 Training loss: 0.51489\n",
      "Epoch: 73/99 Iteration: 5842 Training loss: 0.37618\n",
      "Epoch: 73/99 Iteration: 5843 Training loss: 0.43384\n",
      "Epoch: 73/99 Iteration: 5844 Training loss: 0.36514\n",
      "Epoch: 73/99 Iteration: 5845 Training loss: 0.34974\n",
      "Epoch: 73/99 Iteration: 5846 Training loss: 0.28646\n",
      "Epoch: 73/99 Iteration: 5847 Training loss: 0.37111\n",
      "Epoch: 73/99 Iteration: 5848 Training loss: 0.45383\n",
      "Epoch: 73/99 Iteration: 5849 Training loss: 0.39883\n",
      "Epoch: 73/99 Iteration: 5850 Training loss: 0.24556\n",
      "***\n",
      "Epoch: 73/99 Iteration: 5850 Validation Acc: 0.8680\n",
      "***\n",
      "Epoch: 73/99 Iteration: 5851 Training loss: 0.22084\n",
      "Epoch: 73/99 Iteration: 5852 Training loss: 0.36669\n",
      "Epoch: 73/99 Iteration: 5853 Training loss: 0.41170\n",
      "Epoch: 73/99 Iteration: 5854 Training loss: 0.44305\n",
      "Epoch: 73/99 Iteration: 5855 Training loss: 0.29952\n",
      "Epoch: 73/99 Iteration: 5856 Training loss: 0.25161\n",
      "Epoch: 73/99 Iteration: 5857 Training loss: 0.27671\n",
      "Epoch: 73/99 Iteration: 5858 Training loss: 0.30775\n",
      "Epoch: 73/99 Iteration: 5859 Training loss: 0.33562\n",
      "Epoch: 73/99 Iteration: 5860 Training loss: 0.35275\n",
      "Epoch: 73/99 Iteration: 5861 Training loss: 0.34321\n",
      "Epoch: 73/99 Iteration: 5862 Training loss: 0.29207\n",
      "Epoch: 73/99 Iteration: 5863 Training loss: 0.38132\n",
      "Epoch: 73/99 Iteration: 5864 Training loss: 0.34081\n",
      "Epoch: 73/99 Iteration: 5865 Training loss: 0.29550\n",
      "Epoch: 73/99 Iteration: 5866 Training loss: 0.35765\n",
      "Epoch: 73/99 Iteration: 5867 Training loss: 0.38636\n",
      "Epoch: 73/99 Iteration: 5868 Training loss: 0.35986\n",
      "Epoch: 73/99 Iteration: 5869 Training loss: 0.45469\n",
      "Epoch: 73/99 Iteration: 5870 Training loss: 0.32193\n",
      "Epoch: 73/99 Iteration: 5871 Training loss: 0.23780\n",
      "Epoch: 73/99 Iteration: 5872 Training loss: 0.32662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/99 Iteration: 5873 Training loss: 0.47320\n",
      "Epoch: 73/99 Iteration: 5874 Training loss: 0.34500\n",
      "Epoch: 73/99 Iteration: 5875 Training loss: 0.26992\n",
      "Epoch: 73/99 Iteration: 5876 Training loss: 0.43331\n",
      "Epoch: 73/99 Iteration: 5877 Training loss: 0.28045\n",
      "Epoch: 73/99 Iteration: 5878 Training loss: 0.26296\n",
      "Epoch: 73/99 Iteration: 5879 Training loss: 0.41148\n",
      "Epoch: 73/99 Iteration: 5880 Training loss: 0.42855\n",
      "Epoch: 73/99 Iteration: 5881 Training loss: 0.32231\n",
      "Epoch: 73/99 Iteration: 5882 Training loss: 0.31505\n",
      "Epoch: 73/99 Iteration: 5883 Training loss: 0.31602\n",
      "Epoch: 73/99 Iteration: 5884 Training loss: 0.44442\n",
      "Epoch: 73/99 Iteration: 5885 Training loss: 0.27185\n",
      "Epoch: 73/99 Iteration: 5886 Training loss: 0.31108\n",
      "Epoch: 73/99 Iteration: 5887 Training loss: 0.37318\n",
      "Epoch: 73/99 Iteration: 5888 Training loss: 0.61850\n",
      "Epoch: 73/99 Iteration: 5889 Training loss: 0.22098\n",
      "Epoch: 73/99 Iteration: 5890 Training loss: 0.44587\n",
      "Epoch: 73/99 Iteration: 5891 Training loss: 0.43261\n",
      "Epoch: 73/99 Iteration: 5892 Training loss: 0.27486\n",
      "Epoch: 73/99 Iteration: 5893 Training loss: 0.40606\n",
      "Epoch: 73/99 Iteration: 5894 Training loss: 0.42002\n",
      "Epoch: 73/99 Iteration: 5895 Training loss: 0.39949\n",
      "Epoch: 73/99 Iteration: 5896 Training loss: 0.33284\n",
      "Epoch: 73/99 Iteration: 5897 Training loss: 0.30526\n",
      "Epoch: 73/99 Iteration: 5898 Training loss: 0.42219\n",
      "Epoch: 73/99 Iteration: 5899 Training loss: 0.45946\n",
      "Epoch: 73/99 Iteration: 5900 Training loss: 0.26556\n",
      "***\n",
      "Epoch: 73/99 Iteration: 5900 Validation Acc: 0.8740\n",
      "***\n",
      "Epoch: 73/99 Iteration: 5901 Training loss: 0.34964\n",
      "Epoch: 73/99 Iteration: 5902 Training loss: 0.32843\n",
      "Epoch: 73/99 Iteration: 5903 Training loss: 0.34602\n",
      "Epoch: 73/99 Iteration: 5904 Training loss: 0.46613\n",
      "Epoch: 73/99 Iteration: 5905 Training loss: 0.37578\n",
      "Epoch: 73/99 Iteration: 5906 Training loss: 0.31853\n",
      "Epoch: 73/99 Iteration: 5907 Training loss: 0.30074\n",
      "Epoch: 73/99 Iteration: 5908 Training loss: 0.26498\n",
      "Epoch: 73/99 Iteration: 5909 Training loss: 0.24491\n",
      "Epoch: 73/99 Iteration: 5910 Training loss: 0.37967\n",
      "Epoch: 73/99 Iteration: 5911 Training loss: 0.33421\n",
      "Epoch: 73/99 Iteration: 5912 Training loss: 0.31681\n",
      "Epoch: 73/99 Iteration: 5913 Training loss: 0.29379\n",
      "Epoch: 73/99 Iteration: 5914 Training loss: 0.23409\n",
      "Epoch: 73/99 Iteration: 5915 Training loss: 0.22774\n",
      "Epoch: 73/99 Iteration: 5916 Training loss: 0.27954\n",
      "Epoch: 73/99 Iteration: 5917 Training loss: 0.28140\n",
      "Epoch: 73/99 Iteration: 5918 Training loss: 0.39899\n",
      "Epoch: 73/99 Iteration: 5919 Training loss: 0.32055\n",
      "Epoch: 74/99 Iteration: 5920 Training loss: 0.21441\n",
      "Epoch: 74/99 Iteration: 5921 Training loss: 0.25211\n",
      "Epoch: 74/99 Iteration: 5922 Training loss: 0.37830\n",
      "Epoch: 74/99 Iteration: 5923 Training loss: 0.34550\n",
      "Epoch: 74/99 Iteration: 5924 Training loss: 0.30248\n",
      "Epoch: 74/99 Iteration: 5925 Training loss: 0.23158\n",
      "Epoch: 74/99 Iteration: 5926 Training loss: 0.24367\n",
      "Epoch: 74/99 Iteration: 5927 Training loss: 0.33074\n",
      "Epoch: 74/99 Iteration: 5928 Training loss: 0.29526\n",
      "Epoch: 74/99 Iteration: 5929 Training loss: 0.35306\n",
      "Epoch: 74/99 Iteration: 5930 Training loss: 0.25030\n",
      "Epoch: 74/99 Iteration: 5931 Training loss: 0.32859\n",
      "Epoch: 74/99 Iteration: 5932 Training loss: 0.26589\n",
      "Epoch: 74/99 Iteration: 5933 Training loss: 0.40334\n",
      "Epoch: 74/99 Iteration: 5934 Training loss: 0.41993\n",
      "Epoch: 74/99 Iteration: 5935 Training loss: 0.20892\n",
      "Epoch: 74/99 Iteration: 5936 Training loss: 0.21547\n",
      "Epoch: 74/99 Iteration: 5937 Training loss: 0.28185\n",
      "Epoch: 74/99 Iteration: 5938 Training loss: 0.33531\n",
      "Epoch: 74/99 Iteration: 5939 Training loss: 0.30918\n",
      "Epoch: 74/99 Iteration: 5940 Training loss: 0.24628\n",
      "Epoch: 74/99 Iteration: 5941 Training loss: 0.25316\n",
      "Epoch: 74/99 Iteration: 5942 Training loss: 0.33382\n",
      "Epoch: 74/99 Iteration: 5943 Training loss: 0.37820\n",
      "Epoch: 74/99 Iteration: 5944 Training loss: 0.25803\n",
      "Epoch: 74/99 Iteration: 5945 Training loss: 0.36811\n",
      "Epoch: 74/99 Iteration: 5946 Training loss: 0.29022\n",
      "Epoch: 74/99 Iteration: 5947 Training loss: 0.21096\n",
      "Epoch: 74/99 Iteration: 5948 Training loss: 0.40421\n",
      "Epoch: 74/99 Iteration: 5949 Training loss: 0.46204\n",
      "Epoch: 74/99 Iteration: 5950 Training loss: 0.29922\n",
      "***\n",
      "Epoch: 74/99 Iteration: 5950 Validation Acc: 0.8460\n",
      "***\n",
      "Epoch: 74/99 Iteration: 5951 Training loss: 0.25895\n",
      "Epoch: 74/99 Iteration: 5952 Training loss: 0.38647\n",
      "Epoch: 74/99 Iteration: 5953 Training loss: 0.35975\n",
      "Epoch: 74/99 Iteration: 5954 Training loss: 0.25280\n",
      "Epoch: 74/99 Iteration: 5955 Training loss: 0.33302\n",
      "Epoch: 74/99 Iteration: 5956 Training loss: 0.41639\n",
      "Epoch: 74/99 Iteration: 5957 Training loss: 0.34176\n",
      "Epoch: 74/99 Iteration: 5958 Training loss: 0.22070\n",
      "Epoch: 74/99 Iteration: 5959 Training loss: 0.23918\n",
      "Epoch: 74/99 Iteration: 5960 Training loss: 0.26845\n",
      "Epoch: 74/99 Iteration: 5961 Training loss: 0.25032\n",
      "Epoch: 74/99 Iteration: 5962 Training loss: 0.30910\n",
      "Epoch: 74/99 Iteration: 5963 Training loss: 0.21257\n",
      "Epoch: 74/99 Iteration: 5964 Training loss: 0.40806\n",
      "Epoch: 74/99 Iteration: 5965 Training loss: 0.19693\n",
      "Epoch: 74/99 Iteration: 5966 Training loss: 0.30233\n",
      "Epoch: 74/99 Iteration: 5967 Training loss: 0.23193\n",
      "Epoch: 74/99 Iteration: 5968 Training loss: 0.52077\n",
      "Epoch: 74/99 Iteration: 5969 Training loss: 0.25957\n",
      "Epoch: 74/99 Iteration: 5970 Training loss: 0.29443\n",
      "Epoch: 74/99 Iteration: 5971 Training loss: 0.22362\n",
      "Epoch: 74/99 Iteration: 5972 Training loss: 0.22749\n",
      "Epoch: 74/99 Iteration: 5973 Training loss: 0.38946\n",
      "Epoch: 74/99 Iteration: 5974 Training loss: 0.20707\n",
      "Epoch: 74/99 Iteration: 5975 Training loss: 0.34328\n",
      "Epoch: 74/99 Iteration: 5976 Training loss: 0.28897\n",
      "Epoch: 74/99 Iteration: 5977 Training loss: 0.40493\n",
      "Epoch: 74/99 Iteration: 5978 Training loss: 0.29216\n",
      "Epoch: 74/99 Iteration: 5979 Training loss: 0.30155\n",
      "Epoch: 74/99 Iteration: 5980 Training loss: 0.35863\n",
      "Epoch: 74/99 Iteration: 5981 Training loss: 0.31370\n",
      "Epoch: 74/99 Iteration: 5982 Training loss: 0.28969\n",
      "Epoch: 74/99 Iteration: 5983 Training loss: 0.36066\n",
      "Epoch: 74/99 Iteration: 5984 Training loss: 0.31140\n",
      "Epoch: 74/99 Iteration: 5985 Training loss: 0.33780\n",
      "Epoch: 74/99 Iteration: 5986 Training loss: 0.38860\n",
      "Epoch: 74/99 Iteration: 5987 Training loss: 0.46691\n",
      "Epoch: 74/99 Iteration: 5988 Training loss: 0.27619\n",
      "Epoch: 74/99 Iteration: 5989 Training loss: 0.27092\n",
      "Epoch: 74/99 Iteration: 5990 Training loss: 0.37814\n",
      "Epoch: 74/99 Iteration: 5991 Training loss: 0.34499\n",
      "Epoch: 74/99 Iteration: 5992 Training loss: 0.34970\n",
      "Epoch: 74/99 Iteration: 5993 Training loss: 0.36068\n",
      "Epoch: 74/99 Iteration: 5994 Training loss: 0.28795\n",
      "Epoch: 74/99 Iteration: 5995 Training loss: 0.34028\n",
      "Epoch: 74/99 Iteration: 5996 Training loss: 0.41302\n",
      "Epoch: 74/99 Iteration: 5997 Training loss: 0.36942\n",
      "Epoch: 74/99 Iteration: 5998 Training loss: 0.27026\n",
      "Epoch: 74/99 Iteration: 5999 Training loss: 0.37549\n",
      "Epoch: 75/99 Iteration: 6000 Training loss: 0.19447\n",
      "***\n",
      "Epoch: 75/99 Iteration: 6000 Validation Acc: 0.8500\n",
      "***\n",
      "Epoch: 75/99 Iteration: 6001 Training loss: 0.45918\n",
      "Epoch: 75/99 Iteration: 6002 Training loss: 0.45029\n",
      "Epoch: 75/99 Iteration: 6003 Training loss: 0.34874\n",
      "Epoch: 75/99 Iteration: 6004 Training loss: 0.37257\n",
      "Epoch: 75/99 Iteration: 6005 Training loss: 0.21702\n",
      "Epoch: 75/99 Iteration: 6006 Training loss: 0.38401\n",
      "Epoch: 75/99 Iteration: 6007 Training loss: 0.35591\n",
      "Epoch: 75/99 Iteration: 6008 Training loss: 0.52614\n",
      "Epoch: 75/99 Iteration: 6009 Training loss: 0.33411\n",
      "Epoch: 75/99 Iteration: 6010 Training loss: 0.33853\n",
      "Epoch: 75/99 Iteration: 6011 Training loss: 0.32947\n",
      "Epoch: 75/99 Iteration: 6012 Training loss: 0.34288\n",
      "Epoch: 75/99 Iteration: 6013 Training loss: 0.36232\n",
      "Epoch: 75/99 Iteration: 6014 Training loss: 0.45509\n",
      "Epoch: 75/99 Iteration: 6015 Training loss: 0.26063\n",
      "Epoch: 75/99 Iteration: 6016 Training loss: 0.25395\n",
      "Epoch: 75/99 Iteration: 6017 Training loss: 0.28027\n",
      "Epoch: 75/99 Iteration: 6018 Training loss: 0.50875\n",
      "Epoch: 75/99 Iteration: 6019 Training loss: 0.25798\n",
      "Epoch: 75/99 Iteration: 6020 Training loss: 0.37893\n",
      "Epoch: 75/99 Iteration: 6021 Training loss: 0.19930\n",
      "Epoch: 75/99 Iteration: 6022 Training loss: 0.41063\n",
      "Epoch: 75/99 Iteration: 6023 Training loss: 0.33415\n",
      "Epoch: 75/99 Iteration: 6024 Training loss: 0.31664\n",
      "Epoch: 75/99 Iteration: 6025 Training loss: 0.35179\n",
      "Epoch: 75/99 Iteration: 6026 Training loss: 0.27916\n",
      "Epoch: 75/99 Iteration: 6027 Training loss: 0.20467\n",
      "Epoch: 75/99 Iteration: 6028 Training loss: 0.42834\n",
      "Epoch: 75/99 Iteration: 6029 Training loss: 0.42473\n",
      "Epoch: 75/99 Iteration: 6030 Training loss: 0.33328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/99 Iteration: 6031 Training loss: 0.26142\n",
      "Epoch: 75/99 Iteration: 6032 Training loss: 0.43398\n",
      "Epoch: 75/99 Iteration: 6033 Training loss: 0.16813\n",
      "Epoch: 75/99 Iteration: 6034 Training loss: 0.27466\n",
      "Epoch: 75/99 Iteration: 6035 Training loss: 0.33066\n",
      "Epoch: 75/99 Iteration: 6036 Training loss: 0.53739\n",
      "Epoch: 75/99 Iteration: 6037 Training loss: 0.20675\n",
      "Epoch: 75/99 Iteration: 6038 Training loss: 0.43836\n",
      "Epoch: 75/99 Iteration: 6039 Training loss: 0.28774\n",
      "Epoch: 75/99 Iteration: 6040 Training loss: 0.32980\n",
      "Epoch: 75/99 Iteration: 6041 Training loss: 0.29648\n",
      "Epoch: 75/99 Iteration: 6042 Training loss: 0.42924\n",
      "Epoch: 75/99 Iteration: 6043 Training loss: 0.20280\n",
      "Epoch: 75/99 Iteration: 6044 Training loss: 0.43189\n",
      "Epoch: 75/99 Iteration: 6045 Training loss: 0.18702\n",
      "Epoch: 75/99 Iteration: 6046 Training loss: 0.39575\n",
      "Epoch: 75/99 Iteration: 6047 Training loss: 0.35526\n",
      "Epoch: 75/99 Iteration: 6048 Training loss: 0.45111\n",
      "Epoch: 75/99 Iteration: 6049 Training loss: 0.20145\n",
      "Epoch: 75/99 Iteration: 6050 Training loss: 0.39188\n",
      "***\n",
      "Epoch: 75/99 Iteration: 6050 Validation Acc: 0.8690\n",
      "***\n",
      "Epoch: 75/99 Iteration: 6051 Training loss: 0.32255\n",
      "Epoch: 75/99 Iteration: 6052 Training loss: 0.36831\n",
      "Epoch: 75/99 Iteration: 6053 Training loss: 0.33315\n",
      "Epoch: 75/99 Iteration: 6054 Training loss: 0.38427\n",
      "Epoch: 75/99 Iteration: 6055 Training loss: 0.30054\n",
      "Epoch: 75/99 Iteration: 6056 Training loss: 0.22313\n",
      "Epoch: 75/99 Iteration: 6057 Training loss: 0.43315\n",
      "Epoch: 75/99 Iteration: 6058 Training loss: 0.28414\n",
      "Epoch: 75/99 Iteration: 6059 Training loss: 0.40644\n",
      "Epoch: 75/99 Iteration: 6060 Training loss: 0.33181\n",
      "Epoch: 75/99 Iteration: 6061 Training loss: 0.31645\n",
      "Epoch: 75/99 Iteration: 6062 Training loss: 0.36420\n",
      "Epoch: 75/99 Iteration: 6063 Training loss: 0.19385\n",
      "Epoch: 75/99 Iteration: 6064 Training loss: 0.28241\n",
      "Epoch: 75/99 Iteration: 6065 Training loss: 0.51410\n",
      "Epoch: 75/99 Iteration: 6066 Training loss: 0.27565\n",
      "Epoch: 75/99 Iteration: 6067 Training loss: 0.53281\n",
      "Epoch: 75/99 Iteration: 6068 Training loss: 0.33184\n",
      "Epoch: 75/99 Iteration: 6069 Training loss: 0.32024\n",
      "Epoch: 75/99 Iteration: 6070 Training loss: 0.52324\n",
      "Epoch: 75/99 Iteration: 6071 Training loss: 0.31509\n",
      "Epoch: 75/99 Iteration: 6072 Training loss: 0.39059\n",
      "Epoch: 75/99 Iteration: 6073 Training loss: 0.33639\n",
      "Epoch: 75/99 Iteration: 6074 Training loss: 0.27801\n",
      "Epoch: 75/99 Iteration: 6075 Training loss: 0.32781\n",
      "Epoch: 75/99 Iteration: 6076 Training loss: 0.36443\n",
      "Epoch: 75/99 Iteration: 6077 Training loss: 0.41320\n",
      "Epoch: 75/99 Iteration: 6078 Training loss: 0.25684\n",
      "Epoch: 75/99 Iteration: 6079 Training loss: 0.24649\n",
      "Epoch: 76/99 Iteration: 6080 Training loss: 0.28629\n",
      "Epoch: 76/99 Iteration: 6081 Training loss: 0.38167\n",
      "Epoch: 76/99 Iteration: 6082 Training loss: 0.23302\n",
      "Epoch: 76/99 Iteration: 6083 Training loss: 0.32457\n",
      "Epoch: 76/99 Iteration: 6084 Training loss: 0.38069\n",
      "Epoch: 76/99 Iteration: 6085 Training loss: 0.43304\n",
      "Epoch: 76/99 Iteration: 6086 Training loss: 0.29079\n",
      "Epoch: 76/99 Iteration: 6087 Training loss: 0.44223\n",
      "Epoch: 76/99 Iteration: 6088 Training loss: 0.52374\n",
      "Epoch: 76/99 Iteration: 6089 Training loss: 0.33160\n",
      "Epoch: 76/99 Iteration: 6090 Training loss: 0.47489\n",
      "Epoch: 76/99 Iteration: 6091 Training loss: 0.35744\n",
      "Epoch: 76/99 Iteration: 6092 Training loss: 0.26507\n",
      "Epoch: 76/99 Iteration: 6093 Training loss: 0.44547\n",
      "Epoch: 76/99 Iteration: 6094 Training loss: 0.32680\n",
      "Epoch: 76/99 Iteration: 6095 Training loss: 0.23036\n",
      "Epoch: 76/99 Iteration: 6096 Training loss: 0.24745\n",
      "Epoch: 76/99 Iteration: 6097 Training loss: 0.38303\n",
      "Epoch: 76/99 Iteration: 6098 Training loss: 0.31746\n",
      "Epoch: 76/99 Iteration: 6099 Training loss: 0.26434\n",
      "Epoch: 76/99 Iteration: 6100 Training loss: 0.28095\n",
      "***\n",
      "Epoch: 76/99 Iteration: 6100 Validation Acc: 0.8610\n",
      "***\n",
      "Epoch: 76/99 Iteration: 6101 Training loss: 0.40697\n",
      "Epoch: 76/99 Iteration: 6102 Training loss: 0.20311\n",
      "Epoch: 76/99 Iteration: 6103 Training loss: 0.41775\n",
      "Epoch: 76/99 Iteration: 6104 Training loss: 0.22459\n",
      "Epoch: 76/99 Iteration: 6105 Training loss: 0.39107\n",
      "Epoch: 76/99 Iteration: 6106 Training loss: 0.29871\n",
      "Epoch: 76/99 Iteration: 6107 Training loss: 0.25684\n",
      "Epoch: 76/99 Iteration: 6108 Training loss: 0.39472\n",
      "Epoch: 76/99 Iteration: 6109 Training loss: 0.46049\n",
      "Epoch: 76/99 Iteration: 6110 Training loss: 0.35509\n",
      "Epoch: 76/99 Iteration: 6111 Training loss: 0.37168\n",
      "Epoch: 76/99 Iteration: 6112 Training loss: 0.35818\n",
      "Epoch: 76/99 Iteration: 6113 Training loss: 0.29404\n",
      "Epoch: 76/99 Iteration: 6114 Training loss: 0.22879\n",
      "Epoch: 76/99 Iteration: 6115 Training loss: 0.31217\n",
      "Epoch: 76/99 Iteration: 6116 Training loss: 0.50268\n",
      "Epoch: 76/99 Iteration: 6117 Training loss: 0.29350\n",
      "Epoch: 76/99 Iteration: 6118 Training loss: 0.23347\n",
      "Epoch: 76/99 Iteration: 6119 Training loss: 0.30295\n",
      "Epoch: 76/99 Iteration: 6120 Training loss: 0.35436\n",
      "Epoch: 76/99 Iteration: 6121 Training loss: 0.38198\n",
      "Epoch: 76/99 Iteration: 6122 Training loss: 0.24925\n",
      "Epoch: 76/99 Iteration: 6123 Training loss: 0.29594\n",
      "Epoch: 76/99 Iteration: 6124 Training loss: 0.40531\n",
      "Epoch: 76/99 Iteration: 6125 Training loss: 0.32513\n",
      "Epoch: 76/99 Iteration: 6126 Training loss: 0.52365\n",
      "Epoch: 76/99 Iteration: 6127 Training loss: 0.26685\n",
      "Epoch: 76/99 Iteration: 6128 Training loss: 0.40893\n",
      "Epoch: 76/99 Iteration: 6129 Training loss: 0.24965\n",
      "Epoch: 76/99 Iteration: 6130 Training loss: 0.32231\n",
      "Epoch: 76/99 Iteration: 6131 Training loss: 0.26065\n",
      "Epoch: 76/99 Iteration: 6132 Training loss: 0.33140\n",
      "Epoch: 76/99 Iteration: 6133 Training loss: 0.28276\n",
      "Epoch: 76/99 Iteration: 6134 Training loss: 0.49708\n",
      "Epoch: 76/99 Iteration: 6135 Training loss: 0.17874\n",
      "Epoch: 76/99 Iteration: 6136 Training loss: 0.34830\n",
      "Epoch: 76/99 Iteration: 6137 Training loss: 0.27126\n",
      "Epoch: 76/99 Iteration: 6138 Training loss: 0.23941\n",
      "Epoch: 76/99 Iteration: 6139 Training loss: 0.38301\n",
      "Epoch: 76/99 Iteration: 6140 Training loss: 0.22459\n",
      "Epoch: 76/99 Iteration: 6141 Training loss: 0.24914\n",
      "Epoch: 76/99 Iteration: 6142 Training loss: 0.31378\n",
      "Epoch: 76/99 Iteration: 6143 Training loss: 0.31076\n",
      "Epoch: 76/99 Iteration: 6144 Training loss: 0.29113\n",
      "Epoch: 76/99 Iteration: 6145 Training loss: 0.37094\n",
      "Epoch: 76/99 Iteration: 6146 Training loss: 0.33389\n",
      "Epoch: 76/99 Iteration: 6147 Training loss: 0.35917\n",
      "Epoch: 76/99 Iteration: 6148 Training loss: 0.34344\n",
      "Epoch: 76/99 Iteration: 6149 Training loss: 0.32863\n",
      "Epoch: 76/99 Iteration: 6150 Training loss: 0.46316\n",
      "***\n",
      "Epoch: 76/99 Iteration: 6150 Validation Acc: 0.8590\n",
      "***\n",
      "Epoch: 76/99 Iteration: 6151 Training loss: 0.20013\n",
      "Epoch: 76/99 Iteration: 6152 Training loss: 0.31322\n",
      "Epoch: 76/99 Iteration: 6153 Training loss: 0.24252\n",
      "Epoch: 76/99 Iteration: 6154 Training loss: 0.24385\n",
      "Epoch: 76/99 Iteration: 6155 Training loss: 0.37300\n",
      "Epoch: 76/99 Iteration: 6156 Training loss: 0.25832\n",
      "Epoch: 76/99 Iteration: 6157 Training loss: 0.45021\n",
      "Epoch: 76/99 Iteration: 6158 Training loss: 0.35657\n",
      "Epoch: 76/99 Iteration: 6159 Training loss: 0.30002\n",
      "Epoch: 77/99 Iteration: 6160 Training loss: 0.18390\n",
      "Epoch: 77/99 Iteration: 6161 Training loss: 0.48127\n",
      "Epoch: 77/99 Iteration: 6162 Training loss: 0.33700\n",
      "Epoch: 77/99 Iteration: 6163 Training loss: 0.21660\n",
      "Epoch: 77/99 Iteration: 6164 Training loss: 0.33104\n",
      "Epoch: 77/99 Iteration: 6165 Training loss: 0.30613\n",
      "Epoch: 77/99 Iteration: 6166 Training loss: 0.19763\n",
      "Epoch: 77/99 Iteration: 6167 Training loss: 0.28563\n",
      "Epoch: 77/99 Iteration: 6168 Training loss: 0.37061\n",
      "Epoch: 77/99 Iteration: 6169 Training loss: 0.18012\n",
      "Epoch: 77/99 Iteration: 6170 Training loss: 0.32464\n",
      "Epoch: 77/99 Iteration: 6171 Training loss: 0.32691\n",
      "Epoch: 77/99 Iteration: 6172 Training loss: 0.27405\n",
      "Epoch: 77/99 Iteration: 6173 Training loss: 0.43568\n",
      "Epoch: 77/99 Iteration: 6174 Training loss: 0.37821\n",
      "Epoch: 77/99 Iteration: 6175 Training loss: 0.28825\n",
      "Epoch: 77/99 Iteration: 6176 Training loss: 0.21741\n",
      "Epoch: 77/99 Iteration: 6177 Training loss: 0.24697\n",
      "Epoch: 77/99 Iteration: 6178 Training loss: 0.37706\n",
      "Epoch: 77/99 Iteration: 6179 Training loss: 0.27940\n",
      "Epoch: 77/99 Iteration: 6180 Training loss: 0.32881\n",
      "Epoch: 77/99 Iteration: 6181 Training loss: 0.32726\n",
      "Epoch: 77/99 Iteration: 6182 Training loss: 0.26987\n",
      "Epoch: 77/99 Iteration: 6183 Training loss: 0.32035\n",
      "Epoch: 77/99 Iteration: 6184 Training loss: 0.25236\n",
      "Epoch: 77/99 Iteration: 6185 Training loss: 0.36654\n",
      "Epoch: 77/99 Iteration: 6186 Training loss: 0.36498\n",
      "Epoch: 77/99 Iteration: 6187 Training loss: 0.25627\n",
      "Epoch: 77/99 Iteration: 6188 Training loss: 0.31597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/99 Iteration: 6189 Training loss: 0.42866\n",
      "Epoch: 77/99 Iteration: 6190 Training loss: 0.21425\n",
      "Epoch: 77/99 Iteration: 6191 Training loss: 0.21944\n",
      "Epoch: 77/99 Iteration: 6192 Training loss: 0.33338\n",
      "Epoch: 77/99 Iteration: 6193 Training loss: 0.36788\n",
      "Epoch: 77/99 Iteration: 6194 Training loss: 0.36328\n",
      "Epoch: 77/99 Iteration: 6195 Training loss: 0.25440\n",
      "Epoch: 77/99 Iteration: 6196 Training loss: 0.49483\n",
      "Epoch: 77/99 Iteration: 6197 Training loss: 0.26456\n",
      "Epoch: 77/99 Iteration: 6198 Training loss: 0.15327\n",
      "Epoch: 77/99 Iteration: 6199 Training loss: 0.32027\n",
      "Epoch: 77/99 Iteration: 6200 Training loss: 0.27183\n",
      "***\n",
      "Epoch: 77/99 Iteration: 6200 Validation Acc: 0.8590\n",
      "***\n",
      "Epoch: 77/99 Iteration: 6201 Training loss: 0.27477\n",
      "Epoch: 77/99 Iteration: 6202 Training loss: 0.30501\n",
      "Epoch: 77/99 Iteration: 6203 Training loss: 0.24817\n",
      "Epoch: 77/99 Iteration: 6204 Training loss: 0.40818\n",
      "Epoch: 77/99 Iteration: 6205 Training loss: 0.26312\n",
      "Epoch: 77/99 Iteration: 6206 Training loss: 0.29729\n",
      "Epoch: 77/99 Iteration: 6207 Training loss: 0.41217\n",
      "Epoch: 77/99 Iteration: 6208 Training loss: 0.59498\n",
      "Epoch: 77/99 Iteration: 6209 Training loss: 0.28255\n",
      "Epoch: 77/99 Iteration: 6210 Training loss: 0.23452\n",
      "Epoch: 77/99 Iteration: 6211 Training loss: 0.32940\n",
      "Epoch: 77/99 Iteration: 6212 Training loss: 0.32297\n",
      "Epoch: 77/99 Iteration: 6213 Training loss: 0.24677\n",
      "Epoch: 77/99 Iteration: 6214 Training loss: 0.33491\n",
      "Epoch: 77/99 Iteration: 6215 Training loss: 0.32131\n",
      "Epoch: 77/99 Iteration: 6216 Training loss: 0.35610\n",
      "Epoch: 77/99 Iteration: 6217 Training loss: 0.24237\n",
      "Epoch: 77/99 Iteration: 6218 Training loss: 0.52377\n",
      "Epoch: 77/99 Iteration: 6219 Training loss: 0.40518\n",
      "Epoch: 77/99 Iteration: 6220 Training loss: 0.44321\n",
      "Epoch: 77/99 Iteration: 6221 Training loss: 0.36504\n",
      "Epoch: 77/99 Iteration: 6222 Training loss: 0.42674\n",
      "Epoch: 77/99 Iteration: 6223 Training loss: 0.33119\n",
      "Epoch: 77/99 Iteration: 6224 Training loss: 0.24994\n",
      "Epoch: 77/99 Iteration: 6225 Training loss: 0.34460\n",
      "Epoch: 77/99 Iteration: 6226 Training loss: 0.40805\n",
      "Epoch: 77/99 Iteration: 6227 Training loss: 0.44562\n",
      "Epoch: 77/99 Iteration: 6228 Training loss: 0.34650\n",
      "Epoch: 77/99 Iteration: 6229 Training loss: 0.32963\n",
      "Epoch: 77/99 Iteration: 6230 Training loss: 0.50229\n",
      "Epoch: 77/99 Iteration: 6231 Training loss: 0.38783\n",
      "Epoch: 77/99 Iteration: 6232 Training loss: 0.35714\n",
      "Epoch: 77/99 Iteration: 6233 Training loss: 0.35743\n",
      "Epoch: 77/99 Iteration: 6234 Training loss: 0.40023\n",
      "Epoch: 77/99 Iteration: 6235 Training loss: 0.27441\n",
      "Epoch: 77/99 Iteration: 6236 Training loss: 0.35660\n",
      "Epoch: 77/99 Iteration: 6237 Training loss: 0.44226\n",
      "Epoch: 77/99 Iteration: 6238 Training loss: 0.39724\n",
      "Epoch: 77/99 Iteration: 6239 Training loss: 0.33756\n",
      "Epoch: 78/99 Iteration: 6240 Training loss: 0.26647\n",
      "Epoch: 78/99 Iteration: 6241 Training loss: 0.35667\n",
      "Epoch: 78/99 Iteration: 6242 Training loss: 0.46296\n",
      "Epoch: 78/99 Iteration: 6243 Training loss: 0.43673\n",
      "Epoch: 78/99 Iteration: 6244 Training loss: 0.23793\n",
      "Epoch: 78/99 Iteration: 6245 Training loss: 0.21831\n",
      "Epoch: 78/99 Iteration: 6246 Training loss: 0.22699\n",
      "Epoch: 78/99 Iteration: 6247 Training loss: 0.28269\n",
      "Epoch: 78/99 Iteration: 6248 Training loss: 0.39015\n",
      "Epoch: 78/99 Iteration: 6249 Training loss: 0.33360\n",
      "Epoch: 78/99 Iteration: 6250 Training loss: 0.34303\n",
      "***\n",
      "Epoch: 78/99 Iteration: 6250 Validation Acc: 0.8490\n",
      "***\n",
      "Epoch: 78/99 Iteration: 6251 Training loss: 0.33270\n",
      "Epoch: 78/99 Iteration: 6252 Training loss: 0.29994\n",
      "Epoch: 78/99 Iteration: 6253 Training loss: 0.39373\n",
      "Epoch: 78/99 Iteration: 6254 Training loss: 0.26902\n",
      "Epoch: 78/99 Iteration: 6255 Training loss: 0.48172\n",
      "Epoch: 78/99 Iteration: 6256 Training loss: 0.23288\n",
      "Epoch: 78/99 Iteration: 6257 Training loss: 0.36915\n",
      "Epoch: 78/99 Iteration: 6258 Training loss: 0.32063\n",
      "Epoch: 78/99 Iteration: 6259 Training loss: 0.30612\n",
      "Epoch: 78/99 Iteration: 6260 Training loss: 0.39032\n",
      "Epoch: 78/99 Iteration: 6261 Training loss: 0.60622\n",
      "Epoch: 78/99 Iteration: 6262 Training loss: 0.28102\n",
      "Epoch: 78/99 Iteration: 6263 Training loss: 0.27604\n",
      "Epoch: 78/99 Iteration: 6264 Training loss: 0.33186\n",
      "Epoch: 78/99 Iteration: 6265 Training loss: 0.28339\n",
      "Epoch: 78/99 Iteration: 6266 Training loss: 0.42779\n",
      "Epoch: 78/99 Iteration: 6267 Training loss: 0.28019\n",
      "Epoch: 78/99 Iteration: 6268 Training loss: 0.32180\n",
      "Epoch: 78/99 Iteration: 6269 Training loss: 0.35778\n",
      "Epoch: 78/99 Iteration: 6270 Training loss: 0.21998\n",
      "Epoch: 78/99 Iteration: 6271 Training loss: 0.26188\n",
      "Epoch: 78/99 Iteration: 6272 Training loss: 0.32680\n",
      "Epoch: 78/99 Iteration: 6273 Training loss: 0.32444\n",
      "Epoch: 78/99 Iteration: 6274 Training loss: 0.42327\n",
      "Epoch: 78/99 Iteration: 6275 Training loss: 0.24485\n",
      "Epoch: 78/99 Iteration: 6276 Training loss: 0.52782\n",
      "Epoch: 78/99 Iteration: 6277 Training loss: 0.28578\n",
      "Epoch: 78/99 Iteration: 6278 Training loss: 0.21294\n",
      "Epoch: 78/99 Iteration: 6279 Training loss: 0.21924\n",
      "Epoch: 78/99 Iteration: 6280 Training loss: 0.27853\n",
      "Epoch: 78/99 Iteration: 6281 Training loss: 0.27855\n",
      "Epoch: 78/99 Iteration: 6282 Training loss: 0.22284\n",
      "Epoch: 78/99 Iteration: 6283 Training loss: 0.19311\n",
      "Epoch: 78/99 Iteration: 6284 Training loss: 0.38676\n",
      "Epoch: 78/99 Iteration: 6285 Training loss: 0.26032\n",
      "Epoch: 78/99 Iteration: 6286 Training loss: 0.38897\n",
      "Epoch: 78/99 Iteration: 6287 Training loss: 0.19714\n",
      "Epoch: 78/99 Iteration: 6288 Training loss: 0.51478\n",
      "Epoch: 78/99 Iteration: 6289 Training loss: 0.24616\n",
      "Epoch: 78/99 Iteration: 6290 Training loss: 0.35668\n",
      "Epoch: 78/99 Iteration: 6291 Training loss: 0.29245\n",
      "Epoch: 78/99 Iteration: 6292 Training loss: 0.37753\n",
      "Epoch: 78/99 Iteration: 6293 Training loss: 0.39633\n",
      "Epoch: 78/99 Iteration: 6294 Training loss: 0.38885\n",
      "Epoch: 78/99 Iteration: 6295 Training loss: 0.24019\n",
      "Epoch: 78/99 Iteration: 6296 Training loss: 0.28585\n",
      "Epoch: 78/99 Iteration: 6297 Training loss: 0.30843\n",
      "Epoch: 78/99 Iteration: 6298 Training loss: 0.37079\n",
      "Epoch: 78/99 Iteration: 6299 Training loss: 0.26143\n",
      "Epoch: 78/99 Iteration: 6300 Training loss: 0.30535\n",
      "***\n",
      "Epoch: 78/99 Iteration: 6300 Validation Acc: 0.8630\n",
      "***\n",
      "Epoch: 78/99 Iteration: 6301 Training loss: 0.36997\n",
      "Epoch: 78/99 Iteration: 6302 Training loss: 0.23854\n",
      "Epoch: 78/99 Iteration: 6303 Training loss: 0.41758\n",
      "Epoch: 78/99 Iteration: 6304 Training loss: 0.33507\n",
      "Epoch: 78/99 Iteration: 6305 Training loss: 0.45136\n",
      "Epoch: 78/99 Iteration: 6306 Training loss: 0.44926\n",
      "Epoch: 78/99 Iteration: 6307 Training loss: 0.40145\n",
      "Epoch: 78/99 Iteration: 6308 Training loss: 0.27570\n",
      "Epoch: 78/99 Iteration: 6309 Training loss: 0.36164\n",
      "Epoch: 78/99 Iteration: 6310 Training loss: 0.43421\n",
      "Epoch: 78/99 Iteration: 6311 Training loss: 0.28691\n",
      "Epoch: 78/99 Iteration: 6312 Training loss: 0.31992\n",
      "Epoch: 78/99 Iteration: 6313 Training loss: 0.25350\n",
      "Epoch: 78/99 Iteration: 6314 Training loss: 0.25073\n",
      "Epoch: 78/99 Iteration: 6315 Training loss: 0.27173\n",
      "Epoch: 78/99 Iteration: 6316 Training loss: 0.22555\n",
      "Epoch: 78/99 Iteration: 6317 Training loss: 0.36236\n",
      "Epoch: 78/99 Iteration: 6318 Training loss: 0.17600\n",
      "Epoch: 78/99 Iteration: 6319 Training loss: 0.21304\n",
      "Epoch: 79/99 Iteration: 6320 Training loss: 0.24380\n",
      "Epoch: 79/99 Iteration: 6321 Training loss: 0.63229\n",
      "Epoch: 79/99 Iteration: 6322 Training loss: 0.32921\n",
      "Epoch: 79/99 Iteration: 6323 Training loss: 0.26164\n",
      "Epoch: 79/99 Iteration: 6324 Training loss: 0.36135\n",
      "Epoch: 79/99 Iteration: 6325 Training loss: 0.27650\n",
      "Epoch: 79/99 Iteration: 6326 Training loss: 0.20226\n",
      "Epoch: 79/99 Iteration: 6327 Training loss: 0.38851\n",
      "Epoch: 79/99 Iteration: 6328 Training loss: 0.38700\n",
      "Epoch: 79/99 Iteration: 6329 Training loss: 0.24028\n",
      "Epoch: 79/99 Iteration: 6330 Training loss: 0.24070\n",
      "Epoch: 79/99 Iteration: 6331 Training loss: 0.24810\n",
      "Epoch: 79/99 Iteration: 6332 Training loss: 0.26980\n",
      "Epoch: 79/99 Iteration: 6333 Training loss: 0.42639\n",
      "Epoch: 79/99 Iteration: 6334 Training loss: 0.47063\n",
      "Epoch: 79/99 Iteration: 6335 Training loss: 0.45263\n",
      "Epoch: 79/99 Iteration: 6336 Training loss: 0.24284\n",
      "Epoch: 79/99 Iteration: 6337 Training loss: 0.30577\n",
      "Epoch: 79/99 Iteration: 6338 Training loss: 0.35400\n",
      "Epoch: 79/99 Iteration: 6339 Training loss: 0.34329\n",
      "Epoch: 79/99 Iteration: 6340 Training loss: 0.35860\n",
      "Epoch: 79/99 Iteration: 6341 Training loss: 0.27686\n",
      "Epoch: 79/99 Iteration: 6342 Training loss: 0.38073\n",
      "Epoch: 79/99 Iteration: 6343 Training loss: 0.42732\n",
      "Epoch: 79/99 Iteration: 6344 Training loss: 0.31134\n",
      "Epoch: 79/99 Iteration: 6345 Training loss: 0.43078\n",
      "Epoch: 79/99 Iteration: 6346 Training loss: 0.42629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/99 Iteration: 6347 Training loss: 0.36018\n",
      "Epoch: 79/99 Iteration: 6348 Training loss: 0.32301\n",
      "Epoch: 79/99 Iteration: 6349 Training loss: 0.32625\n",
      "Epoch: 79/99 Iteration: 6350 Training loss: 0.23923\n",
      "***\n",
      "Epoch: 79/99 Iteration: 6350 Validation Acc: 0.8590\n",
      "***\n",
      "Epoch: 79/99 Iteration: 6351 Training loss: 0.22939\n",
      "Epoch: 79/99 Iteration: 6352 Training loss: 0.34641\n",
      "Epoch: 79/99 Iteration: 6353 Training loss: 0.19676\n",
      "Epoch: 79/99 Iteration: 6354 Training loss: 0.28567\n",
      "Epoch: 79/99 Iteration: 6355 Training loss: 0.33214\n",
      "Epoch: 79/99 Iteration: 6356 Training loss: 0.59579\n",
      "Epoch: 79/99 Iteration: 6357 Training loss: 0.38896\n",
      "Epoch: 79/99 Iteration: 6358 Training loss: 0.35269\n",
      "Epoch: 79/99 Iteration: 6359 Training loss: 0.35118\n",
      "Epoch: 79/99 Iteration: 6360 Training loss: 0.49361\n",
      "Epoch: 79/99 Iteration: 6361 Training loss: 0.37604\n",
      "Epoch: 79/99 Iteration: 6362 Training loss: 0.39766\n",
      "Epoch: 79/99 Iteration: 6363 Training loss: 0.32024\n",
      "Epoch: 79/99 Iteration: 6364 Training loss: 0.28419\n",
      "Epoch: 79/99 Iteration: 6365 Training loss: 0.36274\n",
      "Epoch: 79/99 Iteration: 6366 Training loss: 0.34614\n",
      "Epoch: 79/99 Iteration: 6367 Training loss: 0.27877\n",
      "Epoch: 79/99 Iteration: 6368 Training loss: 0.44837\n",
      "Epoch: 79/99 Iteration: 6369 Training loss: 0.16361\n",
      "Epoch: 79/99 Iteration: 6370 Training loss: 0.20813\n",
      "Epoch: 79/99 Iteration: 6371 Training loss: 0.32740\n",
      "Epoch: 79/99 Iteration: 6372 Training loss: 0.28397\n",
      "Epoch: 79/99 Iteration: 6373 Training loss: 0.42641\n",
      "Epoch: 79/99 Iteration: 6374 Training loss: 0.30383\n",
      "Epoch: 79/99 Iteration: 6375 Training loss: 0.26739\n",
      "Epoch: 79/99 Iteration: 6376 Training loss: 0.30714\n",
      "Epoch: 79/99 Iteration: 6377 Training loss: 0.27233\n",
      "Epoch: 79/99 Iteration: 6378 Training loss: 0.27160\n",
      "Epoch: 79/99 Iteration: 6379 Training loss: 0.43651\n",
      "Epoch: 79/99 Iteration: 6380 Training loss: 0.44426\n",
      "Epoch: 79/99 Iteration: 6381 Training loss: 0.35239\n",
      "Epoch: 79/99 Iteration: 6382 Training loss: 0.32918\n",
      "Epoch: 79/99 Iteration: 6383 Training loss: 0.36236\n",
      "Epoch: 79/99 Iteration: 6384 Training loss: 0.30022\n",
      "Epoch: 79/99 Iteration: 6385 Training loss: 0.45127\n",
      "Epoch: 79/99 Iteration: 6386 Training loss: 0.40327\n",
      "Epoch: 79/99 Iteration: 6387 Training loss: 0.40405\n",
      "Epoch: 79/99 Iteration: 6388 Training loss: 0.26251\n",
      "Epoch: 79/99 Iteration: 6389 Training loss: 0.41380\n",
      "Epoch: 79/99 Iteration: 6390 Training loss: 0.29466\n",
      "Epoch: 79/99 Iteration: 6391 Training loss: 0.30860\n",
      "Epoch: 79/99 Iteration: 6392 Training loss: 0.37271\n",
      "Epoch: 79/99 Iteration: 6393 Training loss: 0.24120\n",
      "Epoch: 79/99 Iteration: 6394 Training loss: 0.29939\n",
      "Epoch: 79/99 Iteration: 6395 Training loss: 0.25275\n",
      "Epoch: 79/99 Iteration: 6396 Training loss: 0.33217\n",
      "Epoch: 79/99 Iteration: 6397 Training loss: 0.51608\n",
      "Epoch: 79/99 Iteration: 6398 Training loss: 0.40856\n",
      "Epoch: 79/99 Iteration: 6399 Training loss: 0.30152\n",
      "Epoch: 80/99 Iteration: 6400 Training loss: 0.20253\n",
      "***\n",
      "Epoch: 80/99 Iteration: 6400 Validation Acc: 0.8510\n",
      "***\n",
      "Epoch: 80/99 Iteration: 6401 Training loss: 0.50535\n",
      "Epoch: 80/99 Iteration: 6402 Training loss: 0.36393\n",
      "Epoch: 80/99 Iteration: 6403 Training loss: 0.31622\n",
      "Epoch: 80/99 Iteration: 6404 Training loss: 0.33557\n",
      "Epoch: 80/99 Iteration: 6405 Training loss: 0.32754\n",
      "Epoch: 80/99 Iteration: 6406 Training loss: 0.46120\n",
      "Epoch: 80/99 Iteration: 6407 Training loss: 0.34864\n",
      "Epoch: 80/99 Iteration: 6408 Training loss: 0.42666\n",
      "Epoch: 80/99 Iteration: 6409 Training loss: 0.27683\n",
      "Epoch: 80/99 Iteration: 6410 Training loss: 0.26851\n",
      "Epoch: 80/99 Iteration: 6411 Training loss: 0.37609\n",
      "Epoch: 80/99 Iteration: 6412 Training loss: 0.22143\n",
      "Epoch: 80/99 Iteration: 6413 Training loss: 0.41269\n",
      "Epoch: 80/99 Iteration: 6414 Training loss: 0.42271\n",
      "Epoch: 80/99 Iteration: 6415 Training loss: 0.24429\n",
      "Epoch: 80/99 Iteration: 6416 Training loss: 0.30715\n",
      "Epoch: 80/99 Iteration: 6417 Training loss: 0.33360\n",
      "Epoch: 80/99 Iteration: 6418 Training loss: 0.36972\n",
      "Epoch: 80/99 Iteration: 6419 Training loss: 0.24887\n",
      "Epoch: 80/99 Iteration: 6420 Training loss: 0.36961\n",
      "Epoch: 80/99 Iteration: 6421 Training loss: 0.31826\n",
      "Epoch: 80/99 Iteration: 6422 Training loss: 0.27827\n",
      "Epoch: 80/99 Iteration: 6423 Training loss: 0.37123\n",
      "Epoch: 80/99 Iteration: 6424 Training loss: 0.27987\n",
      "Epoch: 80/99 Iteration: 6425 Training loss: 0.38210\n",
      "Epoch: 80/99 Iteration: 6426 Training loss: 0.41097\n",
      "Epoch: 80/99 Iteration: 6427 Training loss: 0.33397\n",
      "Epoch: 80/99 Iteration: 6428 Training loss: 0.52623\n",
      "Epoch: 80/99 Iteration: 6429 Training loss: 0.42350\n",
      "Epoch: 80/99 Iteration: 6430 Training loss: 0.35925\n",
      "Epoch: 80/99 Iteration: 6431 Training loss: 0.25850\n",
      "Epoch: 80/99 Iteration: 6432 Training loss: 0.51609\n",
      "Epoch: 80/99 Iteration: 6433 Training loss: 0.29726\n",
      "Epoch: 80/99 Iteration: 6434 Training loss: 0.38335\n",
      "Epoch: 80/99 Iteration: 6435 Training loss: 0.37399\n",
      "Epoch: 80/99 Iteration: 6436 Training loss: 0.62804\n",
      "Epoch: 80/99 Iteration: 6437 Training loss: 0.41971\n",
      "Epoch: 80/99 Iteration: 6438 Training loss: 0.27823\n",
      "Epoch: 80/99 Iteration: 6439 Training loss: 0.28701\n",
      "Epoch: 80/99 Iteration: 6440 Training loss: 0.54701\n",
      "Epoch: 80/99 Iteration: 6441 Training loss: 0.45044\n",
      "Epoch: 80/99 Iteration: 6442 Training loss: 0.35355\n",
      "Epoch: 80/99 Iteration: 6443 Training loss: 0.33622\n",
      "Epoch: 80/99 Iteration: 6444 Training loss: 0.40570\n",
      "Epoch: 80/99 Iteration: 6445 Training loss: 0.34858\n",
      "Epoch: 80/99 Iteration: 6446 Training loss: 0.28143\n",
      "Epoch: 80/99 Iteration: 6447 Training loss: 0.26557\n",
      "Epoch: 80/99 Iteration: 6448 Training loss: 0.36205\n",
      "Epoch: 80/99 Iteration: 6449 Training loss: 0.24819\n",
      "Epoch: 80/99 Iteration: 6450 Training loss: 0.37952\n",
      "***\n",
      "Epoch: 80/99 Iteration: 6450 Validation Acc: 0.8650\n",
      "***\n",
      "Epoch: 80/99 Iteration: 6451 Training loss: 0.60026\n",
      "Epoch: 80/99 Iteration: 6452 Training loss: 0.34851\n",
      "Epoch: 80/99 Iteration: 6453 Training loss: 0.34915\n",
      "Epoch: 80/99 Iteration: 6454 Training loss: 0.29078\n",
      "Epoch: 80/99 Iteration: 6455 Training loss: 0.34635\n",
      "Epoch: 80/99 Iteration: 6456 Training loss: 0.40248\n",
      "Epoch: 80/99 Iteration: 6457 Training loss: 0.35459\n",
      "Epoch: 80/99 Iteration: 6458 Training loss: 0.30589\n",
      "Epoch: 80/99 Iteration: 6459 Training loss: 0.41967\n",
      "Epoch: 80/99 Iteration: 6460 Training loss: 0.21923\n",
      "Epoch: 80/99 Iteration: 6461 Training loss: 0.34813\n",
      "Epoch: 80/99 Iteration: 6462 Training loss: 0.31892\n",
      "Epoch: 80/99 Iteration: 6463 Training loss: 0.46638\n",
      "Epoch: 80/99 Iteration: 6464 Training loss: 0.21512\n",
      "Epoch: 80/99 Iteration: 6465 Training loss: 0.40371\n",
      "Epoch: 80/99 Iteration: 6466 Training loss: 0.44104\n",
      "Epoch: 80/99 Iteration: 6467 Training loss: 0.33115\n",
      "Epoch: 80/99 Iteration: 6468 Training loss: 0.32164\n",
      "Epoch: 80/99 Iteration: 6469 Training loss: 0.33710\n",
      "Epoch: 80/99 Iteration: 6470 Training loss: 0.46982\n",
      "Epoch: 80/99 Iteration: 6471 Training loss: 0.34301\n",
      "Epoch: 80/99 Iteration: 6472 Training loss: 0.36508\n",
      "Epoch: 80/99 Iteration: 6473 Training loss: 0.49581\n",
      "Epoch: 80/99 Iteration: 6474 Training loss: 0.42808\n",
      "Epoch: 80/99 Iteration: 6475 Training loss: 0.32599\n",
      "Epoch: 80/99 Iteration: 6476 Training loss: 0.44807\n",
      "Epoch: 80/99 Iteration: 6477 Training loss: 0.48404\n",
      "Epoch: 80/99 Iteration: 6478 Training loss: 0.39830\n",
      "Epoch: 80/99 Iteration: 6479 Training loss: 0.24859\n",
      "Epoch: 81/99 Iteration: 6480 Training loss: 0.30806\n",
      "Epoch: 81/99 Iteration: 6481 Training loss: 0.41199\n",
      "Epoch: 81/99 Iteration: 6482 Training loss: 0.35052\n",
      "Epoch: 81/99 Iteration: 6483 Training loss: 0.35813\n",
      "Epoch: 81/99 Iteration: 6484 Training loss: 0.36587\n",
      "Epoch: 81/99 Iteration: 6485 Training loss: 0.32604\n",
      "Epoch: 81/99 Iteration: 6486 Training loss: 0.34562\n",
      "Epoch: 81/99 Iteration: 6487 Training loss: 0.30296\n",
      "Epoch: 81/99 Iteration: 6488 Training loss: 0.51932\n",
      "Epoch: 81/99 Iteration: 6489 Training loss: 0.32987\n",
      "Epoch: 81/99 Iteration: 6490 Training loss: 0.29839\n",
      "Epoch: 81/99 Iteration: 6491 Training loss: 0.41650\n",
      "Epoch: 81/99 Iteration: 6492 Training loss: 0.37732\n",
      "Epoch: 81/99 Iteration: 6493 Training loss: 0.38100\n",
      "Epoch: 81/99 Iteration: 6494 Training loss: 0.35593\n",
      "Epoch: 81/99 Iteration: 6495 Training loss: 0.32546\n",
      "Epoch: 81/99 Iteration: 6496 Training loss: 0.26715\n",
      "Epoch: 81/99 Iteration: 6497 Training loss: 0.26845\n",
      "Epoch: 81/99 Iteration: 6498 Training loss: 0.25278\n",
      "Epoch: 81/99 Iteration: 6499 Training loss: 0.38428\n",
      "Epoch: 81/99 Iteration: 6500 Training loss: 0.36233\n",
      "***\n",
      "Epoch: 81/99 Iteration: 6500 Validation Acc: 0.8620\n",
      "***\n",
      "Epoch: 81/99 Iteration: 6501 Training loss: 0.28438\n",
      "Epoch: 81/99 Iteration: 6502 Training loss: 0.27560\n",
      "Epoch: 81/99 Iteration: 6503 Training loss: 0.21882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/99 Iteration: 6504 Training loss: 0.31946\n",
      "Epoch: 81/99 Iteration: 6505 Training loss: 0.29316\n",
      "Epoch: 81/99 Iteration: 6506 Training loss: 0.39968\n",
      "Epoch: 81/99 Iteration: 6507 Training loss: 0.23922\n",
      "Epoch: 81/99 Iteration: 6508 Training loss: 0.35146\n",
      "Epoch: 81/99 Iteration: 6509 Training loss: 0.39699\n",
      "Epoch: 81/99 Iteration: 6510 Training loss: 0.35543\n",
      "Epoch: 81/99 Iteration: 6511 Training loss: 0.29458\n",
      "Epoch: 81/99 Iteration: 6512 Training loss: 0.26949\n",
      "Epoch: 81/99 Iteration: 6513 Training loss: 0.28351\n",
      "Epoch: 81/99 Iteration: 6514 Training loss: 0.34487\n",
      "Epoch: 81/99 Iteration: 6515 Training loss: 0.30295\n",
      "Epoch: 81/99 Iteration: 6516 Training loss: 0.43633\n",
      "Epoch: 81/99 Iteration: 6517 Training loss: 0.32984\n",
      "Epoch: 81/99 Iteration: 6518 Training loss: 0.31936\n",
      "Epoch: 81/99 Iteration: 6519 Training loss: 0.27306\n",
      "Epoch: 81/99 Iteration: 6520 Training loss: 0.29792\n",
      "Epoch: 81/99 Iteration: 6521 Training loss: 0.29927\n",
      "Epoch: 81/99 Iteration: 6522 Training loss: 0.36045\n",
      "Epoch: 81/99 Iteration: 6523 Training loss: 0.31969\n",
      "Epoch: 81/99 Iteration: 6524 Training loss: 0.28329\n",
      "Epoch: 81/99 Iteration: 6525 Training loss: 0.35337\n",
      "Epoch: 81/99 Iteration: 6526 Training loss: 0.28779\n",
      "Epoch: 81/99 Iteration: 6527 Training loss: 0.27130\n",
      "Epoch: 81/99 Iteration: 6528 Training loss: 0.43489\n",
      "Epoch: 81/99 Iteration: 6529 Training loss: 0.19366\n",
      "Epoch: 81/99 Iteration: 6530 Training loss: 0.31341\n",
      "Epoch: 81/99 Iteration: 6531 Training loss: 0.39185\n",
      "Epoch: 81/99 Iteration: 6532 Training loss: 0.25805\n",
      "Epoch: 81/99 Iteration: 6533 Training loss: 0.30418\n",
      "Epoch: 81/99 Iteration: 6534 Training loss: 0.34407\n",
      "Epoch: 81/99 Iteration: 6535 Training loss: 0.26153\n",
      "Epoch: 81/99 Iteration: 6536 Training loss: 0.35302\n",
      "Epoch: 81/99 Iteration: 6537 Training loss: 0.34488\n",
      "Epoch: 81/99 Iteration: 6538 Training loss: 0.38219\n",
      "Epoch: 81/99 Iteration: 6539 Training loss: 0.41786\n",
      "Epoch: 81/99 Iteration: 6540 Training loss: 0.28821\n",
      "Epoch: 81/99 Iteration: 6541 Training loss: 0.45396\n",
      "Epoch: 81/99 Iteration: 6542 Training loss: 0.30743\n",
      "Epoch: 81/99 Iteration: 6543 Training loss: 0.30475\n",
      "Epoch: 81/99 Iteration: 6544 Training loss: 0.32176\n",
      "Epoch: 81/99 Iteration: 6545 Training loss: 0.42408\n",
      "Epoch: 81/99 Iteration: 6546 Training loss: 0.24570\n",
      "Epoch: 81/99 Iteration: 6547 Training loss: 0.25949\n",
      "Epoch: 81/99 Iteration: 6548 Training loss: 0.25367\n",
      "Epoch: 81/99 Iteration: 6549 Training loss: 0.26938\n",
      "Epoch: 81/99 Iteration: 6550 Training loss: 0.33974\n",
      "***\n",
      "Epoch: 81/99 Iteration: 6550 Validation Acc: 0.8530\n",
      "***\n",
      "Epoch: 81/99 Iteration: 6551 Training loss: 0.28484\n",
      "Epoch: 81/99 Iteration: 6552 Training loss: 0.34208\n",
      "Epoch: 81/99 Iteration: 6553 Training loss: 0.35610\n",
      "Epoch: 81/99 Iteration: 6554 Training loss: 0.33955\n",
      "Epoch: 81/99 Iteration: 6555 Training loss: 0.26734\n",
      "Epoch: 81/99 Iteration: 6556 Training loss: 0.33141\n",
      "Epoch: 81/99 Iteration: 6557 Training loss: 0.37103\n",
      "Epoch: 81/99 Iteration: 6558 Training loss: 0.39440\n",
      "Epoch: 81/99 Iteration: 6559 Training loss: 0.22157\n",
      "Epoch: 82/99 Iteration: 6560 Training loss: 0.28619\n",
      "Epoch: 82/99 Iteration: 6561 Training loss: 0.30997\n",
      "Epoch: 82/99 Iteration: 6562 Training loss: 0.46638\n",
      "Epoch: 82/99 Iteration: 6563 Training loss: 0.39777\n",
      "Epoch: 82/99 Iteration: 6564 Training loss: 0.25987\n",
      "Epoch: 82/99 Iteration: 6565 Training loss: 0.26856\n",
      "Epoch: 82/99 Iteration: 6566 Training loss: 0.25436\n",
      "Epoch: 82/99 Iteration: 6567 Training loss: 0.20999\n",
      "Epoch: 82/99 Iteration: 6568 Training loss: 0.37522\n",
      "Epoch: 82/99 Iteration: 6569 Training loss: 0.26786\n",
      "Epoch: 82/99 Iteration: 6570 Training loss: 0.27895\n",
      "Epoch: 82/99 Iteration: 6571 Training loss: 0.31986\n",
      "Epoch: 82/99 Iteration: 6572 Training loss: 0.25299\n",
      "Epoch: 82/99 Iteration: 6573 Training loss: 0.30419\n",
      "Epoch: 82/99 Iteration: 6574 Training loss: 0.46872\n",
      "Epoch: 82/99 Iteration: 6575 Training loss: 0.25539\n",
      "Epoch: 82/99 Iteration: 6576 Training loss: 0.29112\n",
      "Epoch: 82/99 Iteration: 6577 Training loss: 0.21023\n",
      "Epoch: 82/99 Iteration: 6578 Training loss: 0.27808\n",
      "Epoch: 82/99 Iteration: 6579 Training loss: 0.23767\n",
      "Epoch: 82/99 Iteration: 6580 Training loss: 0.36162\n",
      "Epoch: 82/99 Iteration: 6581 Training loss: 0.29745\n",
      "Epoch: 82/99 Iteration: 6582 Training loss: 0.32820\n",
      "Epoch: 82/99 Iteration: 6583 Training loss: 0.31915\n",
      "Epoch: 82/99 Iteration: 6584 Training loss: 0.30447\n",
      "Epoch: 82/99 Iteration: 6585 Training loss: 0.29653\n",
      "Epoch: 82/99 Iteration: 6586 Training loss: 0.23690\n",
      "Epoch: 82/99 Iteration: 6587 Training loss: 0.36608\n",
      "Epoch: 82/99 Iteration: 6588 Training loss: 0.40286\n",
      "Epoch: 82/99 Iteration: 6589 Training loss: 0.41919\n",
      "Epoch: 82/99 Iteration: 6590 Training loss: 0.27444\n",
      "Epoch: 82/99 Iteration: 6591 Training loss: 0.25773\n",
      "Epoch: 82/99 Iteration: 6592 Training loss: 0.29916\n",
      "Epoch: 82/99 Iteration: 6593 Training loss: 0.23813\n",
      "Epoch: 82/99 Iteration: 6594 Training loss: 0.31565\n",
      "Epoch: 82/99 Iteration: 6595 Training loss: 0.34572\n",
      "Epoch: 82/99 Iteration: 6596 Training loss: 0.47849\n",
      "Epoch: 82/99 Iteration: 6597 Training loss: 0.30044\n",
      "Epoch: 82/99 Iteration: 6598 Training loss: 0.26020\n",
      "Epoch: 82/99 Iteration: 6599 Training loss: 0.38890\n",
      "Epoch: 82/99 Iteration: 6600 Training loss: 0.30106\n",
      "***\n",
      "Epoch: 82/99 Iteration: 6600 Validation Acc: 0.8530\n",
      "***\n",
      "Epoch: 82/99 Iteration: 6601 Training loss: 0.28468\n",
      "Epoch: 82/99 Iteration: 6602 Training loss: 0.29311\n",
      "Epoch: 82/99 Iteration: 6603 Training loss: 0.31932\n",
      "Epoch: 82/99 Iteration: 6604 Training loss: 0.29154\n",
      "Epoch: 82/99 Iteration: 6605 Training loss: 0.25157\n",
      "Epoch: 82/99 Iteration: 6606 Training loss: 0.27546\n",
      "Epoch: 82/99 Iteration: 6607 Training loss: 0.36799\n",
      "Epoch: 82/99 Iteration: 6608 Training loss: 0.45291\n",
      "Epoch: 82/99 Iteration: 6609 Training loss: 0.21451\n",
      "Epoch: 82/99 Iteration: 6610 Training loss: 0.32684\n",
      "Epoch: 82/99 Iteration: 6611 Training loss: 0.23753\n",
      "Epoch: 82/99 Iteration: 6612 Training loss: 0.22521\n",
      "Epoch: 82/99 Iteration: 6613 Training loss: 0.23828\n",
      "Epoch: 82/99 Iteration: 6614 Training loss: 0.36683\n",
      "Epoch: 82/99 Iteration: 6615 Training loss: 0.28381\n",
      "Epoch: 82/99 Iteration: 6616 Training loss: 0.28766\n",
      "Epoch: 82/99 Iteration: 6617 Training loss: 0.28103\n",
      "Epoch: 82/99 Iteration: 6618 Training loss: 0.28028\n",
      "Epoch: 82/99 Iteration: 6619 Training loss: 0.30871\n",
      "Epoch: 82/99 Iteration: 6620 Training loss: 0.23739\n",
      "Epoch: 82/99 Iteration: 6621 Training loss: 0.36593\n",
      "Epoch: 82/99 Iteration: 6622 Training loss: 0.20778\n",
      "Epoch: 82/99 Iteration: 6623 Training loss: 0.22791\n",
      "Epoch: 82/99 Iteration: 6624 Training loss: 0.24797\n",
      "Epoch: 82/99 Iteration: 6625 Training loss: 0.30780\n",
      "Epoch: 82/99 Iteration: 6626 Training loss: 0.26831\n",
      "Epoch: 82/99 Iteration: 6627 Training loss: 0.28120\n",
      "Epoch: 82/99 Iteration: 6628 Training loss: 0.21457\n",
      "Epoch: 82/99 Iteration: 6629 Training loss: 0.27889\n",
      "Epoch: 82/99 Iteration: 6630 Training loss: 0.38654\n",
      "Epoch: 82/99 Iteration: 6631 Training loss: 0.16274\n",
      "Epoch: 82/99 Iteration: 6632 Training loss: 0.26213\n",
      "Epoch: 82/99 Iteration: 6633 Training loss: 0.23056\n",
      "Epoch: 82/99 Iteration: 6634 Training loss: 0.31587\n",
      "Epoch: 82/99 Iteration: 6635 Training loss: 0.24597\n",
      "Epoch: 82/99 Iteration: 6636 Training loss: 0.20010\n",
      "Epoch: 82/99 Iteration: 6637 Training loss: 0.32180\n",
      "Epoch: 82/99 Iteration: 6638 Training loss: 0.27111\n",
      "Epoch: 82/99 Iteration: 6639 Training loss: 0.21533\n",
      "Epoch: 83/99 Iteration: 6640 Training loss: 0.22094\n",
      "Epoch: 83/99 Iteration: 6641 Training loss: 0.31638\n",
      "Epoch: 83/99 Iteration: 6642 Training loss: 0.24931\n",
      "Epoch: 83/99 Iteration: 6643 Training loss: 0.27584\n",
      "Epoch: 83/99 Iteration: 6644 Training loss: 0.18952\n",
      "Epoch: 83/99 Iteration: 6645 Training loss: 0.17668\n",
      "Epoch: 83/99 Iteration: 6646 Training loss: 0.23809\n",
      "Epoch: 83/99 Iteration: 6647 Training loss: 0.32455\n",
      "Epoch: 83/99 Iteration: 6648 Training loss: 0.37272\n",
      "Epoch: 83/99 Iteration: 6649 Training loss: 0.22265\n",
      "Epoch: 83/99 Iteration: 6650 Training loss: 0.14723\n",
      "***\n",
      "Epoch: 83/99 Iteration: 6650 Validation Acc: 0.8680\n",
      "***\n",
      "Epoch: 83/99 Iteration: 6651 Training loss: 0.23790\n",
      "Epoch: 83/99 Iteration: 6652 Training loss: 0.18242\n",
      "Epoch: 83/99 Iteration: 6653 Training loss: 0.42478\n",
      "Epoch: 83/99 Iteration: 6654 Training loss: 0.33354\n",
      "Epoch: 83/99 Iteration: 6655 Training loss: 0.21966\n",
      "Epoch: 83/99 Iteration: 6656 Training loss: 0.20480\n",
      "Epoch: 83/99 Iteration: 6657 Training loss: 0.13237\n",
      "Epoch: 83/99 Iteration: 6658 Training loss: 0.30773\n",
      "Epoch: 83/99 Iteration: 6659 Training loss: 0.20125\n",
      "Epoch: 83/99 Iteration: 6660 Training loss: 0.29950\n",
      "Epoch: 83/99 Iteration: 6661 Training loss: 0.27036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/99 Iteration: 6662 Training loss: 0.23359\n",
      "Epoch: 83/99 Iteration: 6663 Training loss: 0.31989\n",
      "Epoch: 83/99 Iteration: 6664 Training loss: 0.19911\n",
      "Epoch: 83/99 Iteration: 6665 Training loss: 0.20328\n",
      "Epoch: 83/99 Iteration: 6666 Training loss: 0.24931\n",
      "Epoch: 83/99 Iteration: 6667 Training loss: 0.22813\n",
      "Epoch: 83/99 Iteration: 6668 Training loss: 0.20363\n",
      "Epoch: 83/99 Iteration: 6669 Training loss: 0.46053\n",
      "Epoch: 83/99 Iteration: 6670 Training loss: 0.16140\n",
      "Epoch: 83/99 Iteration: 6671 Training loss: 0.22818\n",
      "Epoch: 83/99 Iteration: 6672 Training loss: 0.23755\n",
      "Epoch: 83/99 Iteration: 6673 Training loss: 0.19121\n",
      "Epoch: 83/99 Iteration: 6674 Training loss: 0.23506\n",
      "Epoch: 83/99 Iteration: 6675 Training loss: 0.20164\n",
      "Epoch: 83/99 Iteration: 6676 Training loss: 0.43181\n",
      "Epoch: 83/99 Iteration: 6677 Training loss: 0.23550\n",
      "Epoch: 83/99 Iteration: 6678 Training loss: 0.19024\n",
      "Epoch: 83/99 Iteration: 6679 Training loss: 0.29030\n",
      "Epoch: 83/99 Iteration: 6680 Training loss: 0.28782\n",
      "Epoch: 83/99 Iteration: 6681 Training loss: 0.20621\n",
      "Epoch: 83/99 Iteration: 6682 Training loss: 0.31492\n",
      "Epoch: 83/99 Iteration: 6683 Training loss: 0.23312\n",
      "Epoch: 83/99 Iteration: 6684 Training loss: 0.23864\n",
      "Epoch: 83/99 Iteration: 6685 Training loss: 0.15002\n",
      "Epoch: 83/99 Iteration: 6686 Training loss: 0.36556\n",
      "Epoch: 83/99 Iteration: 6687 Training loss: 0.22447\n",
      "Epoch: 83/99 Iteration: 6688 Training loss: 0.38668\n",
      "Epoch: 83/99 Iteration: 6689 Training loss: 0.31612\n",
      "Epoch: 83/99 Iteration: 6690 Training loss: 0.20514\n",
      "Epoch: 83/99 Iteration: 6691 Training loss: 0.23609\n",
      "Epoch: 83/99 Iteration: 6692 Training loss: 0.22069\n",
      "Epoch: 83/99 Iteration: 6693 Training loss: 0.28065\n",
      "Epoch: 83/99 Iteration: 6694 Training loss: 0.36077\n",
      "Epoch: 83/99 Iteration: 6695 Training loss: 0.21573\n",
      "Epoch: 83/99 Iteration: 6696 Training loss: 0.19211\n",
      "Epoch: 83/99 Iteration: 6697 Training loss: 0.21549\n",
      "Epoch: 83/99 Iteration: 6698 Training loss: 0.29979\n",
      "Epoch: 83/99 Iteration: 6699 Training loss: 0.34988\n",
      "Epoch: 83/99 Iteration: 6700 Training loss: 0.18996\n",
      "***\n",
      "Epoch: 83/99 Iteration: 6700 Validation Acc: 0.8580\n",
      "***\n",
      "Epoch: 83/99 Iteration: 6701 Training loss: 0.30144\n",
      "Epoch: 83/99 Iteration: 6702 Training loss: 0.21619\n",
      "Epoch: 83/99 Iteration: 6703 Training loss: 0.21744\n",
      "Epoch: 83/99 Iteration: 6704 Training loss: 0.18412\n",
      "Epoch: 83/99 Iteration: 6705 Training loss: 0.27649\n",
      "Epoch: 83/99 Iteration: 6706 Training loss: 0.28610\n",
      "Epoch: 83/99 Iteration: 6707 Training loss: 0.49497\n",
      "Epoch: 83/99 Iteration: 6708 Training loss: 0.22927\n",
      "Epoch: 83/99 Iteration: 6709 Training loss: 0.26513\n",
      "Epoch: 83/99 Iteration: 6710 Training loss: 0.38449\n",
      "Epoch: 83/99 Iteration: 6711 Training loss: 0.29848\n",
      "Epoch: 83/99 Iteration: 6712 Training loss: 0.46028\n",
      "Epoch: 83/99 Iteration: 6713 Training loss: 0.19661\n",
      "Epoch: 83/99 Iteration: 6714 Training loss: 0.26137\n",
      "Epoch: 83/99 Iteration: 6715 Training loss: 0.25777\n",
      "Epoch: 83/99 Iteration: 6716 Training loss: 0.26281\n",
      "Epoch: 83/99 Iteration: 6717 Training loss: 0.27779\n",
      "Epoch: 83/99 Iteration: 6718 Training loss: 0.37366\n",
      "Epoch: 83/99 Iteration: 6719 Training loss: 0.23119\n",
      "Epoch: 84/99 Iteration: 6720 Training loss: 0.20660\n",
      "Epoch: 84/99 Iteration: 6721 Training loss: 0.30363\n",
      "Epoch: 84/99 Iteration: 6722 Training loss: 0.26344\n",
      "Epoch: 84/99 Iteration: 6723 Training loss: 0.30910\n",
      "Epoch: 84/99 Iteration: 6724 Training loss: 0.15173\n",
      "Epoch: 84/99 Iteration: 6725 Training loss: 0.25979\n",
      "Epoch: 84/99 Iteration: 6726 Training loss: 0.21631\n",
      "Epoch: 84/99 Iteration: 6727 Training loss: 0.28198\n",
      "Epoch: 84/99 Iteration: 6728 Training loss: 0.46937\n",
      "Epoch: 84/99 Iteration: 6729 Training loss: 0.18133\n",
      "Epoch: 84/99 Iteration: 6730 Training loss: 0.22035\n",
      "Epoch: 84/99 Iteration: 6731 Training loss: 0.16700\n",
      "Epoch: 84/99 Iteration: 6732 Training loss: 0.34976\n",
      "Epoch: 84/99 Iteration: 6733 Training loss: 0.30411\n",
      "Epoch: 84/99 Iteration: 6734 Training loss: 0.26759\n",
      "Epoch: 84/99 Iteration: 6735 Training loss: 0.24124\n",
      "Epoch: 84/99 Iteration: 6736 Training loss: 0.10933\n",
      "Epoch: 84/99 Iteration: 6737 Training loss: 0.31120\n",
      "Epoch: 84/99 Iteration: 6738 Training loss: 0.22551\n",
      "Epoch: 84/99 Iteration: 6739 Training loss: 0.16168\n",
      "Epoch: 84/99 Iteration: 6740 Training loss: 0.33923\n",
      "Epoch: 84/99 Iteration: 6741 Training loss: 0.20539\n",
      "Epoch: 84/99 Iteration: 6742 Training loss: 0.26827\n",
      "Epoch: 84/99 Iteration: 6743 Training loss: 0.31688\n",
      "Epoch: 84/99 Iteration: 6744 Training loss: 0.23096\n",
      "Epoch: 84/99 Iteration: 6745 Training loss: 0.43181\n",
      "Epoch: 84/99 Iteration: 6746 Training loss: 0.27695\n",
      "Epoch: 84/99 Iteration: 6747 Training loss: 0.26801\n",
      "Epoch: 84/99 Iteration: 6748 Training loss: 0.39031\n",
      "Epoch: 84/99 Iteration: 6749 Training loss: 0.32952\n",
      "Epoch: 84/99 Iteration: 6750 Training loss: 0.17198\n",
      "***\n",
      "Epoch: 84/99 Iteration: 6750 Validation Acc: 0.8780\n",
      "***\n",
      "Epoch: 84/99 Iteration: 6751 Training loss: 0.25815\n",
      "Epoch: 84/99 Iteration: 6752 Training loss: 0.23204\n",
      "Epoch: 84/99 Iteration: 6753 Training loss: 0.20829\n",
      "Epoch: 84/99 Iteration: 6754 Training loss: 0.30723\n",
      "Epoch: 84/99 Iteration: 6755 Training loss: 0.26626\n",
      "Epoch: 84/99 Iteration: 6756 Training loss: 0.35572\n",
      "Epoch: 84/99 Iteration: 6757 Training loss: 0.11516\n",
      "Epoch: 84/99 Iteration: 6758 Training loss: 0.27286\n",
      "Epoch: 84/99 Iteration: 6759 Training loss: 0.20961\n",
      "Epoch: 84/99 Iteration: 6760 Training loss: 0.25885\n",
      "Epoch: 84/99 Iteration: 6761 Training loss: 0.26309\n",
      "Epoch: 84/99 Iteration: 6762 Training loss: 0.28494\n",
      "Epoch: 84/99 Iteration: 6763 Training loss: 0.23281\n",
      "Epoch: 84/99 Iteration: 6764 Training loss: 0.25272\n",
      "Epoch: 84/99 Iteration: 6765 Training loss: 0.21606\n",
      "Epoch: 84/99 Iteration: 6766 Training loss: 0.44807\n",
      "Epoch: 84/99 Iteration: 6767 Training loss: 0.32614\n",
      "Epoch: 84/99 Iteration: 6768 Training loss: 0.30572\n",
      "Epoch: 84/99 Iteration: 6769 Training loss: 0.27028\n",
      "Epoch: 84/99 Iteration: 6770 Training loss: 0.29984\n",
      "Epoch: 84/99 Iteration: 6771 Training loss: 0.32131\n",
      "Epoch: 84/99 Iteration: 6772 Training loss: 0.22324\n",
      "Epoch: 84/99 Iteration: 6773 Training loss: 0.25801\n",
      "Epoch: 84/99 Iteration: 6774 Training loss: 0.30039\n",
      "Epoch: 84/99 Iteration: 6775 Training loss: 0.28064\n",
      "Epoch: 84/99 Iteration: 6776 Training loss: 0.33165\n",
      "Epoch: 84/99 Iteration: 6777 Training loss: 0.23113\n",
      "Epoch: 84/99 Iteration: 6778 Training loss: 0.24328\n",
      "Epoch: 84/99 Iteration: 6779 Training loss: 0.26262\n",
      "Epoch: 84/99 Iteration: 6780 Training loss: 0.17899\n",
      "Epoch: 84/99 Iteration: 6781 Training loss: 0.34541\n",
      "Epoch: 84/99 Iteration: 6782 Training loss: 0.29721\n",
      "Epoch: 84/99 Iteration: 6783 Training loss: 0.23863\n",
      "Epoch: 84/99 Iteration: 6784 Training loss: 0.19654\n",
      "Epoch: 84/99 Iteration: 6785 Training loss: 0.30474\n",
      "Epoch: 84/99 Iteration: 6786 Training loss: 0.28177\n",
      "Epoch: 84/99 Iteration: 6787 Training loss: 0.21071\n",
      "Epoch: 84/99 Iteration: 6788 Training loss: 0.18906\n",
      "Epoch: 84/99 Iteration: 6789 Training loss: 0.26083\n",
      "Epoch: 84/99 Iteration: 6790 Training loss: 0.48750\n",
      "Epoch: 84/99 Iteration: 6791 Training loss: 0.24235\n",
      "Epoch: 84/99 Iteration: 6792 Training loss: 0.28111\n",
      "Epoch: 84/99 Iteration: 6793 Training loss: 0.15238\n",
      "Epoch: 84/99 Iteration: 6794 Training loss: 0.29605\n",
      "Epoch: 84/99 Iteration: 6795 Training loss: 0.20442\n",
      "Epoch: 84/99 Iteration: 6796 Training loss: 0.28563\n",
      "Epoch: 84/99 Iteration: 6797 Training loss: 0.37468\n",
      "Epoch: 84/99 Iteration: 6798 Training loss: 0.26075\n",
      "Epoch: 84/99 Iteration: 6799 Training loss: 0.21283\n",
      "Epoch: 85/99 Iteration: 6800 Training loss: 0.16150\n",
      "***\n",
      "Epoch: 85/99 Iteration: 6800 Validation Acc: 0.8800\n",
      "***\n",
      "Epoch: 85/99 Iteration: 6801 Training loss: 0.24168\n",
      "Epoch: 85/99 Iteration: 6802 Training loss: 0.21950\n",
      "Epoch: 85/99 Iteration: 6803 Training loss: 0.30309\n",
      "Epoch: 85/99 Iteration: 6804 Training loss: 0.26740\n",
      "Epoch: 85/99 Iteration: 6805 Training loss: 0.21999\n",
      "Epoch: 85/99 Iteration: 6806 Training loss: 0.20944\n",
      "Epoch: 85/99 Iteration: 6807 Training loss: 0.33680\n",
      "Epoch: 85/99 Iteration: 6808 Training loss: 0.34988\n",
      "Epoch: 85/99 Iteration: 6809 Training loss: 0.27273\n",
      "Epoch: 85/99 Iteration: 6810 Training loss: 0.30024\n",
      "Epoch: 85/99 Iteration: 6811 Training loss: 0.28698\n",
      "Epoch: 85/99 Iteration: 6812 Training loss: 0.44990\n",
      "Epoch: 85/99 Iteration: 6813 Training loss: 0.34849\n",
      "Epoch: 85/99 Iteration: 6814 Training loss: 0.21763\n",
      "Epoch: 85/99 Iteration: 6815 Training loss: 0.24825\n",
      "Epoch: 85/99 Iteration: 6816 Training loss: 0.21646\n",
      "Epoch: 85/99 Iteration: 6817 Training loss: 0.12926\n",
      "Epoch: 85/99 Iteration: 6818 Training loss: 0.31972\n",
      "Epoch: 85/99 Iteration: 6819 Training loss: 0.25713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/99 Iteration: 6820 Training loss: 0.35709\n",
      "Epoch: 85/99 Iteration: 6821 Training loss: 0.18707\n",
      "Epoch: 85/99 Iteration: 6822 Training loss: 0.35651\n",
      "Epoch: 85/99 Iteration: 6823 Training loss: 0.34667\n",
      "Epoch: 85/99 Iteration: 6824 Training loss: 0.23096\n",
      "Epoch: 85/99 Iteration: 6825 Training loss: 0.27846\n",
      "Epoch: 85/99 Iteration: 6826 Training loss: 0.32970\n",
      "Epoch: 85/99 Iteration: 6827 Training loss: 0.22747\n",
      "Epoch: 85/99 Iteration: 6828 Training loss: 0.46764\n",
      "Epoch: 85/99 Iteration: 6829 Training loss: 0.37522\n",
      "Epoch: 85/99 Iteration: 6830 Training loss: 0.32474\n",
      "Epoch: 85/99 Iteration: 6831 Training loss: 0.17873\n",
      "Epoch: 85/99 Iteration: 6832 Training loss: 0.30070\n",
      "Epoch: 85/99 Iteration: 6833 Training loss: 0.26208\n",
      "Epoch: 85/99 Iteration: 6834 Training loss: 0.26572\n",
      "Epoch: 85/99 Iteration: 6835 Training loss: 0.26949\n",
      "Epoch: 85/99 Iteration: 6836 Training loss: 0.38604\n",
      "Epoch: 85/99 Iteration: 6837 Training loss: 0.21876\n",
      "Epoch: 85/99 Iteration: 6838 Training loss: 0.21778\n",
      "Epoch: 85/99 Iteration: 6839 Training loss: 0.40375\n",
      "Epoch: 85/99 Iteration: 6840 Training loss: 0.36074\n",
      "Epoch: 85/99 Iteration: 6841 Training loss: 0.30535\n",
      "Epoch: 85/99 Iteration: 6842 Training loss: 0.25867\n",
      "Epoch: 85/99 Iteration: 6843 Training loss: 0.17875\n",
      "Epoch: 85/99 Iteration: 6844 Training loss: 0.25221\n",
      "Epoch: 85/99 Iteration: 6845 Training loss: 0.22029\n",
      "Epoch: 85/99 Iteration: 6846 Training loss: 0.31687\n",
      "Epoch: 85/99 Iteration: 6847 Training loss: 0.21923\n",
      "Epoch: 85/99 Iteration: 6848 Training loss: 0.26741\n",
      "Epoch: 85/99 Iteration: 6849 Training loss: 0.21442\n",
      "Epoch: 85/99 Iteration: 6850 Training loss: 0.18753\n",
      "***\n",
      "Epoch: 85/99 Iteration: 6850 Validation Acc: 0.8660\n",
      "***\n",
      "Epoch: 85/99 Iteration: 6851 Training loss: 0.25096\n",
      "Epoch: 85/99 Iteration: 6852 Training loss: 0.33765\n",
      "Epoch: 85/99 Iteration: 6853 Training loss: 0.36568\n",
      "Epoch: 85/99 Iteration: 6854 Training loss: 0.21935\n",
      "Epoch: 85/99 Iteration: 6855 Training loss: 0.23792\n",
      "Epoch: 85/99 Iteration: 6856 Training loss: 0.20306\n",
      "Epoch: 85/99 Iteration: 6857 Training loss: 0.44434\n",
      "Epoch: 85/99 Iteration: 6858 Training loss: 0.31032\n",
      "Epoch: 85/99 Iteration: 6859 Training loss: 0.30162\n",
      "Epoch: 85/99 Iteration: 6860 Training loss: 0.21554\n",
      "Epoch: 85/99 Iteration: 6861 Training loss: 0.29209\n",
      "Epoch: 85/99 Iteration: 6862 Training loss: 0.30151\n",
      "Epoch: 85/99 Iteration: 6863 Training loss: 0.42072\n",
      "Epoch: 85/99 Iteration: 6864 Training loss: 0.20974\n",
      "Epoch: 85/99 Iteration: 6865 Training loss: 0.29175\n",
      "Epoch: 85/99 Iteration: 6866 Training loss: 0.36503\n",
      "Epoch: 85/99 Iteration: 6867 Training loss: 0.41266\n",
      "Epoch: 85/99 Iteration: 6868 Training loss: 0.28838\n",
      "Epoch: 85/99 Iteration: 6869 Training loss: 0.22457\n",
      "Epoch: 85/99 Iteration: 6870 Training loss: 0.37244\n",
      "Epoch: 85/99 Iteration: 6871 Training loss: 0.25841\n",
      "Epoch: 85/99 Iteration: 6872 Training loss: 0.28721\n",
      "Epoch: 85/99 Iteration: 6873 Training loss: 0.18327\n",
      "Epoch: 85/99 Iteration: 6874 Training loss: 0.28660\n",
      "Epoch: 85/99 Iteration: 6875 Training loss: 0.25082\n",
      "Epoch: 85/99 Iteration: 6876 Training loss: 0.37735\n",
      "Epoch: 85/99 Iteration: 6877 Training loss: 0.30054\n",
      "Epoch: 85/99 Iteration: 6878 Training loss: 0.22032\n",
      "Epoch: 85/99 Iteration: 6879 Training loss: 0.32789\n",
      "Epoch: 86/99 Iteration: 6880 Training loss: 0.32059\n",
      "Epoch: 86/99 Iteration: 6881 Training loss: 0.44150\n",
      "Epoch: 86/99 Iteration: 6882 Training loss: 0.16458\n",
      "Epoch: 86/99 Iteration: 6883 Training loss: 0.20741\n",
      "Epoch: 86/99 Iteration: 6884 Training loss: 0.29839\n",
      "Epoch: 86/99 Iteration: 6885 Training loss: 0.23854\n",
      "Epoch: 86/99 Iteration: 6886 Training loss: 0.24122\n",
      "Epoch: 86/99 Iteration: 6887 Training loss: 0.34551\n",
      "Epoch: 86/99 Iteration: 6888 Training loss: 0.40822\n",
      "Epoch: 86/99 Iteration: 6889 Training loss: 0.19972\n",
      "Epoch: 86/99 Iteration: 6890 Training loss: 0.29165\n",
      "Epoch: 86/99 Iteration: 6891 Training loss: 0.28140\n",
      "Epoch: 86/99 Iteration: 6892 Training loss: 0.25338\n",
      "Epoch: 86/99 Iteration: 6893 Training loss: 0.29833\n",
      "Epoch: 86/99 Iteration: 6894 Training loss: 0.23518\n",
      "Epoch: 86/99 Iteration: 6895 Training loss: 0.19699\n",
      "Epoch: 86/99 Iteration: 6896 Training loss: 0.17602\n",
      "Epoch: 86/99 Iteration: 6897 Training loss: 0.26289\n",
      "Epoch: 86/99 Iteration: 6898 Training loss: 0.20327\n",
      "Epoch: 86/99 Iteration: 6899 Training loss: 0.33371\n",
      "Epoch: 86/99 Iteration: 6900 Training loss: 0.12786\n",
      "***\n",
      "Epoch: 86/99 Iteration: 6900 Validation Acc: 0.8720\n",
      "***\n",
      "Epoch: 86/99 Iteration: 6901 Training loss: 0.27089\n",
      "Epoch: 86/99 Iteration: 6902 Training loss: 0.23596\n",
      "Epoch: 86/99 Iteration: 6903 Training loss: 0.26855\n",
      "Epoch: 86/99 Iteration: 6904 Training loss: 0.27125\n",
      "Epoch: 86/99 Iteration: 6905 Training loss: 0.26848\n",
      "Epoch: 86/99 Iteration: 6906 Training loss: 0.25993\n",
      "Epoch: 86/99 Iteration: 6907 Training loss: 0.21973\n",
      "Epoch: 86/99 Iteration: 6908 Training loss: 0.29481\n",
      "Epoch: 86/99 Iteration: 6909 Training loss: 0.46526\n",
      "Epoch: 86/99 Iteration: 6910 Training loss: 0.13334\n",
      "Epoch: 86/99 Iteration: 6911 Training loss: 0.36223\n",
      "Epoch: 86/99 Iteration: 6912 Training loss: 0.31686\n",
      "Epoch: 86/99 Iteration: 6913 Training loss: 0.41549\n",
      "Epoch: 86/99 Iteration: 6914 Training loss: 0.16788\n",
      "Epoch: 86/99 Iteration: 6915 Training loss: 0.27580\n",
      "Epoch: 86/99 Iteration: 6916 Training loss: 0.41199\n",
      "Epoch: 86/99 Iteration: 6917 Training loss: 0.43826\n",
      "Epoch: 86/99 Iteration: 6918 Training loss: 0.28714\n",
      "Epoch: 86/99 Iteration: 6919 Training loss: 0.25803\n",
      "Epoch: 86/99 Iteration: 6920 Training loss: 0.25726\n",
      "Epoch: 86/99 Iteration: 6921 Training loss: 0.32895\n",
      "Epoch: 86/99 Iteration: 6922 Training loss: 0.22855\n",
      "Epoch: 86/99 Iteration: 6923 Training loss: 0.19293\n",
      "Epoch: 86/99 Iteration: 6924 Training loss: 0.16911\n",
      "Epoch: 86/99 Iteration: 6925 Training loss: 0.27458\n",
      "Epoch: 86/99 Iteration: 6926 Training loss: 0.39508\n",
      "Epoch: 86/99 Iteration: 6927 Training loss: 0.31784\n",
      "Epoch: 86/99 Iteration: 6928 Training loss: 0.35709\n",
      "Epoch: 86/99 Iteration: 6929 Training loss: 0.13449\n",
      "Epoch: 86/99 Iteration: 6930 Training loss: 0.20465\n",
      "Epoch: 86/99 Iteration: 6931 Training loss: 0.31360\n",
      "Epoch: 86/99 Iteration: 6932 Training loss: 0.24088\n",
      "Epoch: 86/99 Iteration: 6933 Training loss: 0.19141\n",
      "Epoch: 86/99 Iteration: 6934 Training loss: 0.27933\n",
      "Epoch: 86/99 Iteration: 6935 Training loss: 0.25599\n",
      "Epoch: 86/99 Iteration: 6936 Training loss: 0.28501\n",
      "Epoch: 86/99 Iteration: 6937 Training loss: 0.25665\n",
      "Epoch: 86/99 Iteration: 6938 Training loss: 0.30812\n",
      "Epoch: 86/99 Iteration: 6939 Training loss: 0.37451\n",
      "Epoch: 86/99 Iteration: 6940 Training loss: 0.34833\n",
      "Epoch: 86/99 Iteration: 6941 Training loss: 0.23615\n",
      "Epoch: 86/99 Iteration: 6942 Training loss: 0.24776\n",
      "Epoch: 86/99 Iteration: 6943 Training loss: 0.33120\n",
      "Epoch: 86/99 Iteration: 6944 Training loss: 0.27611\n",
      "Epoch: 86/99 Iteration: 6945 Training loss: 0.37342\n",
      "Epoch: 86/99 Iteration: 6946 Training loss: 0.31003\n",
      "Epoch: 86/99 Iteration: 6947 Training loss: 0.39986\n",
      "Epoch: 86/99 Iteration: 6948 Training loss: 0.28643\n",
      "Epoch: 86/99 Iteration: 6949 Training loss: 0.24725\n",
      "Epoch: 86/99 Iteration: 6950 Training loss: 0.37555\n",
      "***\n",
      "Epoch: 86/99 Iteration: 6950 Validation Acc: 0.8680\n",
      "***\n",
      "Epoch: 86/99 Iteration: 6951 Training loss: 0.26554\n",
      "Epoch: 86/99 Iteration: 6952 Training loss: 0.31952\n",
      "Epoch: 86/99 Iteration: 6953 Training loss: 0.16296\n",
      "Epoch: 86/99 Iteration: 6954 Training loss: 0.33504\n",
      "Epoch: 86/99 Iteration: 6955 Training loss: 0.26251\n",
      "Epoch: 86/99 Iteration: 6956 Training loss: 0.29069\n",
      "Epoch: 86/99 Iteration: 6957 Training loss: 0.38258\n",
      "Epoch: 86/99 Iteration: 6958 Training loss: 0.33967\n",
      "Epoch: 86/99 Iteration: 6959 Training loss: 0.17707\n",
      "Epoch: 87/99 Iteration: 6960 Training loss: 0.17680\n",
      "Epoch: 87/99 Iteration: 6961 Training loss: 0.43498\n",
      "Epoch: 87/99 Iteration: 6962 Training loss: 0.26759\n",
      "Epoch: 87/99 Iteration: 6963 Training loss: 0.31861\n",
      "Epoch: 87/99 Iteration: 6964 Training loss: 0.34399\n",
      "Epoch: 87/99 Iteration: 6965 Training loss: 0.31639\n",
      "Epoch: 87/99 Iteration: 6966 Training loss: 0.24476\n",
      "Epoch: 87/99 Iteration: 6967 Training loss: 0.20143\n",
      "Epoch: 87/99 Iteration: 6968 Training loss: 0.40642\n",
      "Epoch: 87/99 Iteration: 6969 Training loss: 0.15317\n",
      "Epoch: 87/99 Iteration: 6970 Training loss: 0.29041\n",
      "Epoch: 87/99 Iteration: 6971 Training loss: 0.29827\n",
      "Epoch: 87/99 Iteration: 6972 Training loss: 0.29448\n",
      "Epoch: 87/99 Iteration: 6973 Training loss: 0.30248\n",
      "Epoch: 87/99 Iteration: 6974 Training loss: 0.40605\n",
      "Epoch: 87/99 Iteration: 6975 Training loss: 0.28012\n",
      "Epoch: 87/99 Iteration: 6976 Training loss: 0.19034\n",
      "Epoch: 87/99 Iteration: 6977 Training loss: 0.30129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/99 Iteration: 6978 Training loss: 0.26960\n",
      "Epoch: 87/99 Iteration: 6979 Training loss: 0.30598\n",
      "Epoch: 87/99 Iteration: 6980 Training loss: 0.23686\n",
      "Epoch: 87/99 Iteration: 6981 Training loss: 0.21896\n",
      "Epoch: 87/99 Iteration: 6982 Training loss: 0.33444\n",
      "Epoch: 87/99 Iteration: 6983 Training loss: 0.32777\n",
      "Epoch: 87/99 Iteration: 6984 Training loss: 0.18733\n",
      "Epoch: 87/99 Iteration: 6985 Training loss: 0.23611\n",
      "Epoch: 87/99 Iteration: 6986 Training loss: 0.28149\n",
      "Epoch: 87/99 Iteration: 6987 Training loss: 0.28420\n",
      "Epoch: 87/99 Iteration: 6988 Training loss: 0.51975\n",
      "Epoch: 87/99 Iteration: 6989 Training loss: 0.41077\n",
      "Epoch: 87/99 Iteration: 6990 Training loss: 0.15826\n",
      "Epoch: 87/99 Iteration: 6991 Training loss: 0.28453\n",
      "Epoch: 87/99 Iteration: 6992 Training loss: 0.30513\n",
      "Epoch: 87/99 Iteration: 6993 Training loss: 0.26433\n",
      "Epoch: 87/99 Iteration: 6994 Training loss: 0.38767\n",
      "Epoch: 87/99 Iteration: 6995 Training loss: 0.15816\n",
      "Epoch: 87/99 Iteration: 6996 Training loss: 0.39904\n",
      "Epoch: 87/99 Iteration: 6997 Training loss: 0.24597\n",
      "Epoch: 87/99 Iteration: 6998 Training loss: 0.28624\n",
      "Epoch: 87/99 Iteration: 6999 Training loss: 0.28686\n",
      "Epoch: 87/99 Iteration: 7000 Training loss: 0.17472\n",
      "***\n",
      "Epoch: 87/99 Iteration: 7000 Validation Acc: 0.8610\n",
      "***\n",
      "Epoch: 87/99 Iteration: 7001 Training loss: 0.30510\n",
      "Epoch: 87/99 Iteration: 7002 Training loss: 0.37971\n",
      "Epoch: 87/99 Iteration: 7003 Training loss: 0.24617\n",
      "Epoch: 87/99 Iteration: 7004 Training loss: 0.32848\n",
      "Epoch: 87/99 Iteration: 7005 Training loss: 0.22416\n",
      "Epoch: 87/99 Iteration: 7006 Training loss: 0.31939\n",
      "Epoch: 87/99 Iteration: 7007 Training loss: 0.43468\n",
      "Epoch: 87/99 Iteration: 7008 Training loss: 0.31005\n",
      "Epoch: 87/99 Iteration: 7009 Training loss: 0.21524\n",
      "Epoch: 87/99 Iteration: 7010 Training loss: 0.25826\n",
      "Epoch: 87/99 Iteration: 7011 Training loss: 0.34357\n",
      "Epoch: 87/99 Iteration: 7012 Training loss: 0.36032\n",
      "Epoch: 87/99 Iteration: 7013 Training loss: 0.36626\n",
      "Epoch: 87/99 Iteration: 7014 Training loss: 0.42147\n",
      "Epoch: 87/99 Iteration: 7015 Training loss: 0.30706\n",
      "Epoch: 87/99 Iteration: 7016 Training loss: 0.30496\n",
      "Epoch: 87/99 Iteration: 7017 Training loss: 0.38190\n",
      "Epoch: 87/99 Iteration: 7018 Training loss: 0.32205\n",
      "Epoch: 87/99 Iteration: 7019 Training loss: 0.33677\n",
      "Epoch: 87/99 Iteration: 7020 Training loss: 0.22228\n",
      "Epoch: 87/99 Iteration: 7021 Training loss: 0.39049\n",
      "Epoch: 87/99 Iteration: 7022 Training loss: 0.20277\n",
      "Epoch: 87/99 Iteration: 7023 Training loss: 0.36068\n",
      "Epoch: 87/99 Iteration: 7024 Training loss: 0.29611\n",
      "Epoch: 87/99 Iteration: 7025 Training loss: 0.24062\n",
      "Epoch: 87/99 Iteration: 7026 Training loss: 0.36093\n",
      "Epoch: 87/99 Iteration: 7027 Training loss: 0.35470\n",
      "Epoch: 87/99 Iteration: 7028 Training loss: 0.27471\n",
      "Epoch: 87/99 Iteration: 7029 Training loss: 0.43154\n",
      "Epoch: 87/99 Iteration: 7030 Training loss: 0.33385\n",
      "Epoch: 87/99 Iteration: 7031 Training loss: 0.22239\n",
      "Epoch: 87/99 Iteration: 7032 Training loss: 0.32894\n",
      "Epoch: 87/99 Iteration: 7033 Training loss: 0.18573\n",
      "Epoch: 87/99 Iteration: 7034 Training loss: 0.38556\n",
      "Epoch: 87/99 Iteration: 7035 Training loss: 0.21704\n",
      "Epoch: 87/99 Iteration: 7036 Training loss: 0.22786\n",
      "Epoch: 87/99 Iteration: 7037 Training loss: 0.40133\n",
      "Epoch: 87/99 Iteration: 7038 Training loss: 0.34045\n",
      "Epoch: 87/99 Iteration: 7039 Training loss: 0.37796\n",
      "Epoch: 88/99 Iteration: 7040 Training loss: 0.31176\n",
      "Epoch: 88/99 Iteration: 7041 Training loss: 0.52442\n",
      "Epoch: 88/99 Iteration: 7042 Training loss: 0.20999\n",
      "Epoch: 88/99 Iteration: 7043 Training loss: 0.35241\n",
      "Epoch: 88/99 Iteration: 7044 Training loss: 0.28906\n",
      "Epoch: 88/99 Iteration: 7045 Training loss: 0.36685\n",
      "Epoch: 88/99 Iteration: 7046 Training loss: 0.27685\n",
      "Epoch: 88/99 Iteration: 7047 Training loss: 0.24726\n",
      "Epoch: 88/99 Iteration: 7048 Training loss: 0.46644\n",
      "Epoch: 88/99 Iteration: 7049 Training loss: 0.22503\n",
      "Epoch: 88/99 Iteration: 7050 Training loss: 0.25603\n",
      "***\n",
      "Epoch: 88/99 Iteration: 7050 Validation Acc: 0.8580\n",
      "***\n",
      "Epoch: 88/99 Iteration: 7051 Training loss: 0.26741\n",
      "Epoch: 88/99 Iteration: 7052 Training loss: 0.22147\n",
      "Epoch: 88/99 Iteration: 7053 Training loss: 0.35528\n",
      "Epoch: 88/99 Iteration: 7054 Training loss: 0.41494\n",
      "Epoch: 88/99 Iteration: 7055 Training loss: 0.24611\n",
      "Epoch: 88/99 Iteration: 7056 Training loss: 0.19350\n",
      "Epoch: 88/99 Iteration: 7057 Training loss: 0.20394\n",
      "Epoch: 88/99 Iteration: 7058 Training loss: 0.35691\n",
      "Epoch: 88/99 Iteration: 7059 Training loss: 0.23216\n",
      "Epoch: 88/99 Iteration: 7060 Training loss: 0.28056\n",
      "Epoch: 88/99 Iteration: 7061 Training loss: 0.24686\n",
      "Epoch: 88/99 Iteration: 7062 Training loss: 0.23881\n",
      "Epoch: 88/99 Iteration: 7063 Training loss: 0.30502\n",
      "Epoch: 88/99 Iteration: 7064 Training loss: 0.33065\n",
      "Epoch: 88/99 Iteration: 7065 Training loss: 0.27711\n",
      "Epoch: 88/99 Iteration: 7066 Training loss: 0.24098\n",
      "Epoch: 88/99 Iteration: 7067 Training loss: 0.31254\n",
      "Epoch: 88/99 Iteration: 7068 Training loss: 0.30761\n",
      "Epoch: 88/99 Iteration: 7069 Training loss: 0.35127\n",
      "Epoch: 88/99 Iteration: 7070 Training loss: 0.27171\n",
      "Epoch: 88/99 Iteration: 7071 Training loss: 0.22219\n",
      "Epoch: 88/99 Iteration: 7072 Training loss: 0.26149\n",
      "Epoch: 88/99 Iteration: 7073 Training loss: 0.29819\n",
      "Epoch: 88/99 Iteration: 7074 Training loss: 0.26500\n",
      "Epoch: 88/99 Iteration: 7075 Training loss: 0.41599\n",
      "Epoch: 88/99 Iteration: 7076 Training loss: 0.40822\n",
      "Epoch: 88/99 Iteration: 7077 Training loss: 0.17890\n",
      "Epoch: 88/99 Iteration: 7078 Training loss: 0.33483\n",
      "Epoch: 88/99 Iteration: 7079 Training loss: 0.30117\n",
      "Epoch: 88/99 Iteration: 7080 Training loss: 0.33871\n",
      "Epoch: 88/99 Iteration: 7081 Training loss: 0.20946\n",
      "Epoch: 88/99 Iteration: 7082 Training loss: 0.27017\n",
      "Epoch: 88/99 Iteration: 7083 Training loss: 0.15787\n",
      "Epoch: 88/99 Iteration: 7084 Training loss: 0.36736\n",
      "Epoch: 88/99 Iteration: 7085 Training loss: 0.27221\n",
      "Epoch: 88/99 Iteration: 7086 Training loss: 0.34147\n",
      "Epoch: 88/99 Iteration: 7087 Training loss: 0.35599\n",
      "Epoch: 88/99 Iteration: 7088 Training loss: 0.23652\n",
      "Epoch: 88/99 Iteration: 7089 Training loss: 0.25814\n",
      "Epoch: 88/99 Iteration: 7090 Training loss: 0.25731\n",
      "Epoch: 88/99 Iteration: 7091 Training loss: 0.24229\n",
      "Epoch: 88/99 Iteration: 7092 Training loss: 0.32471\n",
      "Epoch: 88/99 Iteration: 7093 Training loss: 0.36157\n",
      "Epoch: 88/99 Iteration: 7094 Training loss: 0.37901\n",
      "Epoch: 88/99 Iteration: 7095 Training loss: 0.38888\n",
      "Epoch: 88/99 Iteration: 7096 Training loss: 0.26934\n",
      "Epoch: 88/99 Iteration: 7097 Training loss: 0.24296\n",
      "Epoch: 88/99 Iteration: 7098 Training loss: 0.30873\n",
      "Epoch: 88/99 Iteration: 7099 Training loss: 0.38996\n",
      "Epoch: 88/99 Iteration: 7100 Training loss: 0.47394\n",
      "***\n",
      "Epoch: 88/99 Iteration: 7100 Validation Acc: 0.8550\n",
      "***\n",
      "Epoch: 88/99 Iteration: 7101 Training loss: 0.32723\n",
      "Epoch: 88/99 Iteration: 7102 Training loss: 0.20380\n",
      "Epoch: 88/99 Iteration: 7103 Training loss: 0.31578\n",
      "Epoch: 88/99 Iteration: 7104 Training loss: 0.44397\n",
      "Epoch: 88/99 Iteration: 7105 Training loss: 0.34716\n",
      "Epoch: 88/99 Iteration: 7106 Training loss: 0.43024\n",
      "Epoch: 88/99 Iteration: 7107 Training loss: 0.28824\n",
      "Epoch: 88/99 Iteration: 7108 Training loss: 0.43577\n",
      "Epoch: 88/99 Iteration: 7109 Training loss: 0.35575\n",
      "Epoch: 88/99 Iteration: 7110 Training loss: 0.34215\n",
      "Epoch: 88/99 Iteration: 7111 Training loss: 0.29173\n",
      "Epoch: 88/99 Iteration: 7112 Training loss: 0.36059\n",
      "Epoch: 88/99 Iteration: 7113 Training loss: 0.43058\n",
      "Epoch: 88/99 Iteration: 7114 Training loss: 0.26762\n",
      "Epoch: 88/99 Iteration: 7115 Training loss: 0.23640\n",
      "Epoch: 88/99 Iteration: 7116 Training loss: 0.40894\n",
      "Epoch: 88/99 Iteration: 7117 Training loss: 0.55184\n",
      "Epoch: 88/99 Iteration: 7118 Training loss: 0.42538\n",
      "Epoch: 88/99 Iteration: 7119 Training loss: 0.29878\n",
      "Epoch: 89/99 Iteration: 7120 Training loss: 0.30438\n",
      "Epoch: 89/99 Iteration: 7121 Training loss: 0.46837\n",
      "Epoch: 89/99 Iteration: 7122 Training loss: 0.39942\n",
      "Epoch: 89/99 Iteration: 7123 Training loss: 0.35479\n",
      "Epoch: 89/99 Iteration: 7124 Training loss: 0.34053\n",
      "Epoch: 89/99 Iteration: 7125 Training loss: 0.35009\n",
      "Epoch: 89/99 Iteration: 7126 Training loss: 0.25389\n",
      "Epoch: 89/99 Iteration: 7127 Training loss: 0.24776\n",
      "Epoch: 89/99 Iteration: 7128 Training loss: 0.32746\n",
      "Epoch: 89/99 Iteration: 7129 Training loss: 0.21474\n",
      "Epoch: 89/99 Iteration: 7130 Training loss: 0.30448\n",
      "Epoch: 89/99 Iteration: 7131 Training loss: 0.22066\n",
      "Epoch: 89/99 Iteration: 7132 Training loss: 0.32174\n",
      "Epoch: 89/99 Iteration: 7133 Training loss: 0.38837\n",
      "Epoch: 89/99 Iteration: 7134 Training loss: 0.40757\n",
      "Epoch: 89/99 Iteration: 7135 Training loss: 0.37068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/99 Iteration: 7136 Training loss: 0.18830\n",
      "Epoch: 89/99 Iteration: 7137 Training loss: 0.22825\n",
      "Epoch: 89/99 Iteration: 7138 Training loss: 0.35060\n",
      "Epoch: 89/99 Iteration: 7139 Training loss: 0.24638\n",
      "Epoch: 89/99 Iteration: 7140 Training loss: 0.23855\n",
      "Epoch: 89/99 Iteration: 7141 Training loss: 0.31349\n",
      "Epoch: 89/99 Iteration: 7142 Training loss: 0.24955\n",
      "Epoch: 89/99 Iteration: 7143 Training loss: 0.37190\n",
      "Epoch: 89/99 Iteration: 7144 Training loss: 0.23752\n",
      "Epoch: 89/99 Iteration: 7145 Training loss: 0.29460\n",
      "Epoch: 89/99 Iteration: 7146 Training loss: 0.32696\n",
      "Epoch: 89/99 Iteration: 7147 Training loss: 0.34211\n",
      "Epoch: 89/99 Iteration: 7148 Training loss: 0.40753\n",
      "Epoch: 89/99 Iteration: 7149 Training loss: 0.46087\n",
      "Epoch: 89/99 Iteration: 7150 Training loss: 0.34513\n",
      "***\n",
      "Epoch: 89/99 Iteration: 7150 Validation Acc: 0.8610\n",
      "***\n",
      "Epoch: 89/99 Iteration: 7151 Training loss: 0.19855\n",
      "Epoch: 89/99 Iteration: 7152 Training loss: 0.30761\n",
      "Epoch: 89/99 Iteration: 7153 Training loss: 0.27274\n",
      "Epoch: 89/99 Iteration: 7154 Training loss: 0.22979\n",
      "Epoch: 89/99 Iteration: 7155 Training loss: 0.34439\n",
      "Epoch: 89/99 Iteration: 7156 Training loss: 0.37949\n",
      "Epoch: 89/99 Iteration: 7157 Training loss: 0.25607\n",
      "Epoch: 89/99 Iteration: 7158 Training loss: 0.23672\n",
      "Epoch: 89/99 Iteration: 7159 Training loss: 0.23058\n",
      "Epoch: 89/99 Iteration: 7160 Training loss: 0.27334\n",
      "Epoch: 89/99 Iteration: 7161 Training loss: 0.26913\n",
      "Epoch: 89/99 Iteration: 7162 Training loss: 0.29126\n",
      "Epoch: 89/99 Iteration: 7163 Training loss: 0.23167\n",
      "Epoch: 89/99 Iteration: 7164 Training loss: 0.22267\n",
      "Epoch: 89/99 Iteration: 7165 Training loss: 0.28170\n",
      "Epoch: 89/99 Iteration: 7166 Training loss: 0.50106\n",
      "Epoch: 89/99 Iteration: 7167 Training loss: 0.21310\n",
      "Epoch: 89/99 Iteration: 7168 Training loss: 0.35907\n",
      "Epoch: 89/99 Iteration: 7169 Training loss: 0.17308\n",
      "Epoch: 89/99 Iteration: 7170 Training loss: 0.30186\n",
      "Epoch: 89/99 Iteration: 7171 Training loss: 0.39961\n",
      "Epoch: 89/99 Iteration: 7172 Training loss: 0.30929\n",
      "Epoch: 89/99 Iteration: 7173 Training loss: 0.30402\n",
      "Epoch: 89/99 Iteration: 7174 Training loss: 0.63192\n",
      "Epoch: 89/99 Iteration: 7175 Training loss: 0.21397\n",
      "Epoch: 89/99 Iteration: 7176 Training loss: 0.30235\n",
      "Epoch: 89/99 Iteration: 7177 Training loss: 0.30812\n",
      "Epoch: 89/99 Iteration: 7178 Training loss: 0.41368\n",
      "Epoch: 89/99 Iteration: 7179 Training loss: 0.24409\n",
      "Epoch: 89/99 Iteration: 7180 Training loss: 0.34785\n",
      "Epoch: 89/99 Iteration: 7181 Training loss: 0.25791\n",
      "Epoch: 89/99 Iteration: 7182 Training loss: 0.31807\n",
      "Epoch: 89/99 Iteration: 7183 Training loss: 0.31237\n",
      "Epoch: 89/99 Iteration: 7184 Training loss: 0.25970\n",
      "Epoch: 89/99 Iteration: 7185 Training loss: 0.35327\n",
      "Epoch: 89/99 Iteration: 7186 Training loss: 0.32236\n",
      "Epoch: 89/99 Iteration: 7187 Training loss: 0.33972\n",
      "Epoch: 89/99 Iteration: 7188 Training loss: 0.35837\n",
      "Epoch: 89/99 Iteration: 7189 Training loss: 0.27026\n",
      "Epoch: 89/99 Iteration: 7190 Training loss: 0.32419\n",
      "Epoch: 89/99 Iteration: 7191 Training loss: 0.25799\n",
      "Epoch: 89/99 Iteration: 7192 Training loss: 0.26172\n",
      "Epoch: 89/99 Iteration: 7193 Training loss: 0.25589\n",
      "Epoch: 89/99 Iteration: 7194 Training loss: 0.24646\n",
      "Epoch: 89/99 Iteration: 7195 Training loss: 0.29348\n",
      "Epoch: 89/99 Iteration: 7196 Training loss: 0.28225\n",
      "Epoch: 89/99 Iteration: 7197 Training loss: 0.36136\n",
      "Epoch: 89/99 Iteration: 7198 Training loss: 0.27194\n",
      "Epoch: 89/99 Iteration: 7199 Training loss: 0.28087\n",
      "Epoch: 90/99 Iteration: 7200 Training loss: 0.32684\n",
      "***\n",
      "Epoch: 90/99 Iteration: 7200 Validation Acc: 0.8530\n",
      "***\n",
      "Epoch: 90/99 Iteration: 7201 Training loss: 0.32521\n",
      "Epoch: 90/99 Iteration: 7202 Training loss: 0.20237\n",
      "Epoch: 90/99 Iteration: 7203 Training loss: 0.25850\n",
      "Epoch: 90/99 Iteration: 7204 Training loss: 0.39971\n",
      "Epoch: 90/99 Iteration: 7205 Training loss: 0.30705\n",
      "Epoch: 90/99 Iteration: 7206 Training loss: 0.26374\n",
      "Epoch: 90/99 Iteration: 7207 Training loss: 0.30277\n",
      "Epoch: 90/99 Iteration: 7208 Training loss: 0.47619\n",
      "Epoch: 90/99 Iteration: 7209 Training loss: 0.29874\n",
      "Epoch: 90/99 Iteration: 7210 Training loss: 0.37046\n",
      "Epoch: 90/99 Iteration: 7211 Training loss: 0.30507\n",
      "Epoch: 90/99 Iteration: 7212 Training loss: 0.34882\n",
      "Epoch: 90/99 Iteration: 7213 Training loss: 0.39039\n",
      "Epoch: 90/99 Iteration: 7214 Training loss: 0.48587\n",
      "Epoch: 90/99 Iteration: 7215 Training loss: 0.38518\n",
      "Epoch: 90/99 Iteration: 7216 Training loss: 0.30385\n",
      "Epoch: 90/99 Iteration: 7217 Training loss: 0.26146\n",
      "Epoch: 90/99 Iteration: 7218 Training loss: 0.43184\n",
      "Epoch: 90/99 Iteration: 7219 Training loss: 0.47534\n",
      "Epoch: 90/99 Iteration: 7220 Training loss: 0.40120\n",
      "Epoch: 90/99 Iteration: 7221 Training loss: 0.37076\n",
      "Epoch: 90/99 Iteration: 7222 Training loss: 0.40308\n",
      "Epoch: 90/99 Iteration: 7223 Training loss: 0.39842\n",
      "Epoch: 90/99 Iteration: 7224 Training loss: 0.40332\n",
      "Epoch: 90/99 Iteration: 7225 Training loss: 0.39137\n",
      "Epoch: 90/99 Iteration: 7226 Training loss: 0.47336\n",
      "Epoch: 90/99 Iteration: 7227 Training loss: 0.25038\n",
      "Epoch: 90/99 Iteration: 7228 Training loss: 0.30309\n",
      "Epoch: 90/99 Iteration: 7229 Training loss: 0.42197\n",
      "Epoch: 90/99 Iteration: 7230 Training loss: 0.26599\n",
      "Epoch: 90/99 Iteration: 7231 Training loss: 0.35809\n",
      "Epoch: 90/99 Iteration: 7232 Training loss: 0.20818\n",
      "Epoch: 90/99 Iteration: 7233 Training loss: 0.32915\n",
      "Epoch: 90/99 Iteration: 7234 Training loss: 0.46643\n",
      "Epoch: 90/99 Iteration: 7235 Training loss: 0.39866\n",
      "Epoch: 90/99 Iteration: 7236 Training loss: 0.45186\n",
      "Epoch: 90/99 Iteration: 7237 Training loss: 0.14957\n",
      "Epoch: 90/99 Iteration: 7238 Training loss: 0.51912\n",
      "Epoch: 90/99 Iteration: 7239 Training loss: 0.27681\n",
      "Epoch: 90/99 Iteration: 7240 Training loss: 0.30293\n",
      "Epoch: 90/99 Iteration: 7241 Training loss: 0.32638\n",
      "Epoch: 90/99 Iteration: 7242 Training loss: 0.34359\n",
      "Epoch: 90/99 Iteration: 7243 Training loss: 0.26454\n",
      "Epoch: 90/99 Iteration: 7244 Training loss: 0.24249\n",
      "Epoch: 90/99 Iteration: 7245 Training loss: 0.33534\n",
      "Epoch: 90/99 Iteration: 7246 Training loss: 0.33102\n",
      "Epoch: 90/99 Iteration: 7247 Training loss: 0.29683\n",
      "Epoch: 90/99 Iteration: 7248 Training loss: 0.42435\n",
      "Epoch: 90/99 Iteration: 7249 Training loss: 0.39143\n",
      "Epoch: 90/99 Iteration: 7250 Training loss: 0.24442\n",
      "***\n",
      "Epoch: 90/99 Iteration: 7250 Validation Acc: 0.8580\n",
      "***\n",
      "Epoch: 90/99 Iteration: 7251 Training loss: 0.36739\n",
      "Epoch: 90/99 Iteration: 7252 Training loss: 0.23780\n",
      "Epoch: 90/99 Iteration: 7253 Training loss: 0.28321\n",
      "Epoch: 90/99 Iteration: 7254 Training loss: 0.32030\n",
      "Epoch: 90/99 Iteration: 7255 Training loss: 0.28288\n",
      "Epoch: 90/99 Iteration: 7256 Training loss: 0.22800\n",
      "Epoch: 90/99 Iteration: 7257 Training loss: 0.33541\n",
      "Epoch: 90/99 Iteration: 7258 Training loss: 0.31991\n",
      "Epoch: 90/99 Iteration: 7259 Training loss: 0.33793\n",
      "Epoch: 90/99 Iteration: 7260 Training loss: 0.23831\n",
      "Epoch: 90/99 Iteration: 7261 Training loss: 0.27827\n",
      "Epoch: 90/99 Iteration: 7262 Training loss: 0.21438\n",
      "Epoch: 90/99 Iteration: 7263 Training loss: 0.24849\n",
      "Epoch: 90/99 Iteration: 7264 Training loss: 0.31168\n",
      "Epoch: 90/99 Iteration: 7265 Training loss: 0.35978\n",
      "Epoch: 90/99 Iteration: 7266 Training loss: 0.35089\n",
      "Epoch: 90/99 Iteration: 7267 Training loss: 0.39994\n",
      "Epoch: 90/99 Iteration: 7268 Training loss: 0.33105\n",
      "Epoch: 90/99 Iteration: 7269 Training loss: 0.32454\n",
      "Epoch: 90/99 Iteration: 7270 Training loss: 0.43040\n",
      "Epoch: 90/99 Iteration: 7271 Training loss: 0.30356\n",
      "Epoch: 90/99 Iteration: 7272 Training loss: 0.33901\n",
      "Epoch: 90/99 Iteration: 7273 Training loss: 0.29207\n",
      "Epoch: 90/99 Iteration: 7274 Training loss: 0.29432\n",
      "Epoch: 90/99 Iteration: 7275 Training loss: 0.28941\n",
      "Epoch: 90/99 Iteration: 7276 Training loss: 0.28909\n",
      "Epoch: 90/99 Iteration: 7277 Training loss: 0.20431\n",
      "Epoch: 90/99 Iteration: 7278 Training loss: 0.34477\n",
      "Epoch: 90/99 Iteration: 7279 Training loss: 0.28826\n",
      "Epoch: 91/99 Iteration: 7280 Training loss: 0.28410\n",
      "Epoch: 91/99 Iteration: 7281 Training loss: 0.36820\n",
      "Epoch: 91/99 Iteration: 7282 Training loss: 0.26846\n",
      "Epoch: 91/99 Iteration: 7283 Training loss: 0.33655\n",
      "Epoch: 91/99 Iteration: 7284 Training loss: 0.15123\n",
      "Epoch: 91/99 Iteration: 7285 Training loss: 0.25192\n",
      "Epoch: 91/99 Iteration: 7286 Training loss: 0.36541\n",
      "Epoch: 91/99 Iteration: 7287 Training loss: 0.26916\n",
      "Epoch: 91/99 Iteration: 7288 Training loss: 0.32940\n",
      "Epoch: 91/99 Iteration: 7289 Training loss: 0.31414\n",
      "Epoch: 91/99 Iteration: 7290 Training loss: 0.31453\n",
      "Epoch: 91/99 Iteration: 7291 Training loss: 0.35699\n",
      "Epoch: 91/99 Iteration: 7292 Training loss: 0.22669\n",
      "Epoch: 91/99 Iteration: 7293 Training loss: 0.41415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/99 Iteration: 7294 Training loss: 0.29041\n",
      "Epoch: 91/99 Iteration: 7295 Training loss: 0.33352\n",
      "Epoch: 91/99 Iteration: 7296 Training loss: 0.20547\n",
      "Epoch: 91/99 Iteration: 7297 Training loss: 0.15831\n",
      "Epoch: 91/99 Iteration: 7298 Training loss: 0.32171\n",
      "Epoch: 91/99 Iteration: 7299 Training loss: 0.36833\n",
      "Epoch: 91/99 Iteration: 7300 Training loss: 0.35214\n",
      "***\n",
      "Epoch: 91/99 Iteration: 7300 Validation Acc: 0.8730\n",
      "***\n",
      "Epoch: 91/99 Iteration: 7301 Training loss: 0.39629\n",
      "Epoch: 91/99 Iteration: 7302 Training loss: 0.26800\n",
      "Epoch: 91/99 Iteration: 7303 Training loss: 0.40341\n",
      "Epoch: 91/99 Iteration: 7304 Training loss: 0.29077\n",
      "Epoch: 91/99 Iteration: 7305 Training loss: 0.51746\n",
      "Epoch: 91/99 Iteration: 7306 Training loss: 0.34160\n",
      "Epoch: 91/99 Iteration: 7307 Training loss: 0.24069\n",
      "Epoch: 91/99 Iteration: 7308 Training loss: 0.32513\n",
      "Epoch: 91/99 Iteration: 7309 Training loss: 0.45105\n",
      "Epoch: 91/99 Iteration: 7310 Training loss: 0.32487\n",
      "Epoch: 91/99 Iteration: 7311 Training loss: 0.30275\n",
      "Epoch: 91/99 Iteration: 7312 Training loss: 0.30086\n",
      "Epoch: 91/99 Iteration: 7313 Training loss: 0.21268\n",
      "Epoch: 91/99 Iteration: 7314 Training loss: 0.30250\n",
      "Epoch: 91/99 Iteration: 7315 Training loss: 0.30729\n",
      "Epoch: 91/99 Iteration: 7316 Training loss: 0.30280\n",
      "Epoch: 91/99 Iteration: 7317 Training loss: 0.30861\n",
      "Epoch: 91/99 Iteration: 7318 Training loss: 0.17021\n",
      "Epoch: 91/99 Iteration: 7319 Training loss: 0.32696\n",
      "Epoch: 91/99 Iteration: 7320 Training loss: 0.46640\n",
      "Epoch: 91/99 Iteration: 7321 Training loss: 0.22594\n",
      "Epoch: 91/99 Iteration: 7322 Training loss: 0.33232\n",
      "Epoch: 91/99 Iteration: 7323 Training loss: 0.39301\n",
      "Epoch: 91/99 Iteration: 7324 Training loss: 0.34354\n",
      "Epoch: 91/99 Iteration: 7325 Training loss: 0.34733\n",
      "Epoch: 91/99 Iteration: 7326 Training loss: 0.36446\n",
      "Epoch: 91/99 Iteration: 7327 Training loss: 0.45501\n",
      "Epoch: 91/99 Iteration: 7328 Training loss: 0.36791\n",
      "Epoch: 91/99 Iteration: 7329 Training loss: 0.19310\n",
      "Epoch: 91/99 Iteration: 7330 Training loss: 0.21604\n",
      "Epoch: 91/99 Iteration: 7331 Training loss: 0.24713\n",
      "Epoch: 91/99 Iteration: 7332 Training loss: 0.26548\n",
      "Epoch: 91/99 Iteration: 7333 Training loss: 0.30283\n",
      "Epoch: 91/99 Iteration: 7334 Training loss: 0.29142\n",
      "Epoch: 91/99 Iteration: 7335 Training loss: 0.21991\n",
      "Epoch: 91/99 Iteration: 7336 Training loss: 0.23367\n",
      "Epoch: 91/99 Iteration: 7337 Training loss: 0.24894\n",
      "Epoch: 91/99 Iteration: 7338 Training loss: 0.25082\n",
      "Epoch: 91/99 Iteration: 7339 Training loss: 0.29412\n",
      "Epoch: 91/99 Iteration: 7340 Training loss: 0.37163\n",
      "Epoch: 91/99 Iteration: 7341 Training loss: 0.25718\n",
      "Epoch: 91/99 Iteration: 7342 Training loss: 0.21143\n",
      "Epoch: 91/99 Iteration: 7343 Training loss: 0.30908\n",
      "Epoch: 91/99 Iteration: 7344 Training loss: 0.34590\n",
      "Epoch: 91/99 Iteration: 7345 Training loss: 0.41774\n",
      "Epoch: 91/99 Iteration: 7346 Training loss: 0.24891\n",
      "Epoch: 91/99 Iteration: 7347 Training loss: 0.34464\n",
      "Epoch: 91/99 Iteration: 7348 Training loss: 0.27603\n",
      "Epoch: 91/99 Iteration: 7349 Training loss: 0.29965\n",
      "Epoch: 91/99 Iteration: 7350 Training loss: 0.32432\n",
      "***\n",
      "Epoch: 91/99 Iteration: 7350 Validation Acc: 0.8670\n",
      "***\n",
      "Epoch: 91/99 Iteration: 7351 Training loss: 0.23110\n",
      "Epoch: 91/99 Iteration: 7352 Training loss: 0.32279\n",
      "Epoch: 91/99 Iteration: 7353 Training loss: 0.25458\n",
      "Epoch: 91/99 Iteration: 7354 Training loss: 0.35505\n",
      "Epoch: 91/99 Iteration: 7355 Training loss: 0.25508\n",
      "Epoch: 91/99 Iteration: 7356 Training loss: 0.28922\n",
      "Epoch: 91/99 Iteration: 7357 Training loss: 0.30072\n",
      "Epoch: 91/99 Iteration: 7358 Training loss: 0.28386\n",
      "Epoch: 91/99 Iteration: 7359 Training loss: 0.29539\n",
      "Epoch: 92/99 Iteration: 7360 Training loss: 0.20617\n",
      "Epoch: 92/99 Iteration: 7361 Training loss: 0.31115\n",
      "Epoch: 92/99 Iteration: 7362 Training loss: 0.27896\n",
      "Epoch: 92/99 Iteration: 7363 Training loss: 0.27588\n",
      "Epoch: 92/99 Iteration: 7364 Training loss: 0.22390\n",
      "Epoch: 92/99 Iteration: 7365 Training loss: 0.27454\n",
      "Epoch: 92/99 Iteration: 7366 Training loss: 0.36234\n",
      "Epoch: 92/99 Iteration: 7367 Training loss: 0.31626\n",
      "Epoch: 92/99 Iteration: 7368 Training loss: 0.27162\n",
      "Epoch: 92/99 Iteration: 7369 Training loss: 0.22573\n",
      "Epoch: 92/99 Iteration: 7370 Training loss: 0.25667\n",
      "Epoch: 92/99 Iteration: 7371 Training loss: 0.24433\n",
      "Epoch: 92/99 Iteration: 7372 Training loss: 0.26119\n",
      "Epoch: 92/99 Iteration: 7373 Training loss: 0.28350\n",
      "Epoch: 92/99 Iteration: 7374 Training loss: 0.24272\n",
      "Epoch: 92/99 Iteration: 7375 Training loss: 0.16949\n",
      "Epoch: 92/99 Iteration: 7376 Training loss: 0.28223\n",
      "Epoch: 92/99 Iteration: 7377 Training loss: 0.19252\n",
      "Epoch: 92/99 Iteration: 7378 Training loss: 0.28926\n",
      "Epoch: 92/99 Iteration: 7379 Training loss: 0.29064\n",
      "Epoch: 92/99 Iteration: 7380 Training loss: 0.29000\n",
      "Epoch: 92/99 Iteration: 7381 Training loss: 0.17420\n",
      "Epoch: 92/99 Iteration: 7382 Training loss: 0.23872\n",
      "Epoch: 92/99 Iteration: 7383 Training loss: 0.27445\n",
      "Epoch: 92/99 Iteration: 7384 Training loss: 0.27159\n",
      "Epoch: 92/99 Iteration: 7385 Training loss: 0.27510\n",
      "Epoch: 92/99 Iteration: 7386 Training loss: 0.17844\n",
      "Epoch: 92/99 Iteration: 7387 Training loss: 0.15977\n",
      "Epoch: 92/99 Iteration: 7388 Training loss: 0.27691\n",
      "Epoch: 92/99 Iteration: 7389 Training loss: 0.44246\n",
      "Epoch: 92/99 Iteration: 7390 Training loss: 0.21569\n",
      "Epoch: 92/99 Iteration: 7391 Training loss: 0.20240\n",
      "Epoch: 92/99 Iteration: 7392 Training loss: 0.19407\n",
      "Epoch: 92/99 Iteration: 7393 Training loss: 0.22290\n",
      "Epoch: 92/99 Iteration: 7394 Training loss: 0.33122\n",
      "Epoch: 92/99 Iteration: 7395 Training loss: 0.36367\n",
      "Epoch: 92/99 Iteration: 7396 Training loss: 0.28728\n",
      "Epoch: 92/99 Iteration: 7397 Training loss: 0.22765\n",
      "Epoch: 92/99 Iteration: 7398 Training loss: 0.28270\n",
      "Epoch: 92/99 Iteration: 7399 Training loss: 0.28832\n",
      "Epoch: 92/99 Iteration: 7400 Training loss: 0.26482\n",
      "***\n",
      "Epoch: 92/99 Iteration: 7400 Validation Acc: 0.8710\n",
      "***\n",
      "Epoch: 92/99 Iteration: 7401 Training loss: 0.20801\n",
      "Epoch: 92/99 Iteration: 7402 Training loss: 0.25815\n",
      "Epoch: 92/99 Iteration: 7403 Training loss: 0.20701\n",
      "Epoch: 92/99 Iteration: 7404 Training loss: 0.53705\n",
      "Epoch: 92/99 Iteration: 7405 Training loss: 0.25381\n",
      "Epoch: 92/99 Iteration: 7406 Training loss: 0.29248\n",
      "Epoch: 92/99 Iteration: 7407 Training loss: 0.32337\n",
      "Epoch: 92/99 Iteration: 7408 Training loss: 0.49980\n",
      "Epoch: 92/99 Iteration: 7409 Training loss: 0.18823\n",
      "Epoch: 92/99 Iteration: 7410 Training loss: 0.31468\n",
      "Epoch: 92/99 Iteration: 7411 Training loss: 0.33709\n",
      "Epoch: 92/99 Iteration: 7412 Training loss: 0.27201\n",
      "Epoch: 92/99 Iteration: 7413 Training loss: 0.33608\n",
      "Epoch: 92/99 Iteration: 7414 Training loss: 0.36084\n",
      "Epoch: 92/99 Iteration: 7415 Training loss: 0.22751\n",
      "Epoch: 92/99 Iteration: 7416 Training loss: 0.21356\n",
      "Epoch: 92/99 Iteration: 7417 Training loss: 0.29117\n",
      "Epoch: 92/99 Iteration: 7418 Training loss: 0.29275\n",
      "Epoch: 92/99 Iteration: 7419 Training loss: 0.34181\n",
      "Epoch: 92/99 Iteration: 7420 Training loss: 0.21460\n",
      "Epoch: 92/99 Iteration: 7421 Training loss: 0.31492\n",
      "Epoch: 92/99 Iteration: 7422 Training loss: 0.26054\n",
      "Epoch: 92/99 Iteration: 7423 Training loss: 0.28315\n",
      "Epoch: 92/99 Iteration: 7424 Training loss: 0.15841\n",
      "Epoch: 92/99 Iteration: 7425 Training loss: 0.34021\n",
      "Epoch: 92/99 Iteration: 7426 Training loss: 0.24229\n",
      "Epoch: 92/99 Iteration: 7427 Training loss: 0.39441\n",
      "Epoch: 92/99 Iteration: 7428 Training loss: 0.23339\n",
      "Epoch: 92/99 Iteration: 7429 Training loss: 0.31244\n",
      "Epoch: 92/99 Iteration: 7430 Training loss: 0.32848\n",
      "Epoch: 92/99 Iteration: 7431 Training loss: 0.29771\n",
      "Epoch: 92/99 Iteration: 7432 Training loss: 0.26656\n",
      "Epoch: 92/99 Iteration: 7433 Training loss: 0.36933\n",
      "Epoch: 92/99 Iteration: 7434 Training loss: 0.38663\n",
      "Epoch: 92/99 Iteration: 7435 Training loss: 0.26680\n",
      "Epoch: 92/99 Iteration: 7436 Training loss: 0.28211\n",
      "Epoch: 92/99 Iteration: 7437 Training loss: 0.35704\n",
      "Epoch: 92/99 Iteration: 7438 Training loss: 0.31669\n",
      "Epoch: 92/99 Iteration: 7439 Training loss: 0.24277\n",
      "Epoch: 93/99 Iteration: 7440 Training loss: 0.28359\n",
      "Epoch: 93/99 Iteration: 7441 Training loss: 0.43564\n",
      "Epoch: 93/99 Iteration: 7442 Training loss: 0.23673\n",
      "Epoch: 93/99 Iteration: 7443 Training loss: 0.29501\n",
      "Epoch: 93/99 Iteration: 7444 Training loss: 0.24075\n",
      "Epoch: 93/99 Iteration: 7445 Training loss: 0.22490\n",
      "Epoch: 93/99 Iteration: 7446 Training loss: 0.23920\n",
      "Epoch: 93/99 Iteration: 7447 Training loss: 0.25111\n",
      "Epoch: 93/99 Iteration: 7448 Training loss: 0.36898\n",
      "Epoch: 93/99 Iteration: 7449 Training loss: 0.19662\n",
      "Epoch: 93/99 Iteration: 7450 Training loss: 0.23322\n",
      "***\n",
      "Epoch: 93/99 Iteration: 7450 Validation Acc: 0.8800\n",
      "***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/99 Iteration: 7451 Training loss: 0.28084\n",
      "Epoch: 93/99 Iteration: 7452 Training loss: 0.20392\n",
      "Epoch: 93/99 Iteration: 7453 Training loss: 0.32150\n",
      "Epoch: 93/99 Iteration: 7454 Training loss: 0.27029\n",
      "Epoch: 93/99 Iteration: 7455 Training loss: 0.25856\n",
      "Epoch: 93/99 Iteration: 7456 Training loss: 0.19724\n",
      "Epoch: 93/99 Iteration: 7457 Training loss: 0.17674\n",
      "Epoch: 93/99 Iteration: 7458 Training loss: 0.31296\n",
      "Epoch: 93/99 Iteration: 7459 Training loss: 0.22862\n",
      "Epoch: 93/99 Iteration: 7460 Training loss: 0.29951\n",
      "Epoch: 93/99 Iteration: 7461 Training loss: 0.23083\n",
      "Epoch: 93/99 Iteration: 7462 Training loss: 0.25092\n",
      "Epoch: 93/99 Iteration: 7463 Training loss: 0.19829\n",
      "Epoch: 93/99 Iteration: 7464 Training loss: 0.33587\n",
      "Epoch: 93/99 Iteration: 7465 Training loss: 0.31181\n",
      "Epoch: 93/99 Iteration: 7466 Training loss: 0.35242\n",
      "Epoch: 93/99 Iteration: 7467 Training loss: 0.13293\n",
      "Epoch: 93/99 Iteration: 7468 Training loss: 0.39890\n",
      "Epoch: 93/99 Iteration: 7469 Training loss: 0.35843\n",
      "Epoch: 93/99 Iteration: 7470 Training loss: 0.18258\n",
      "Epoch: 93/99 Iteration: 7471 Training loss: 0.28752\n",
      "Epoch: 93/99 Iteration: 7472 Training loss: 0.21652\n",
      "Epoch: 93/99 Iteration: 7473 Training loss: 0.26864\n",
      "Epoch: 93/99 Iteration: 7474 Training loss: 0.31391\n",
      "Epoch: 93/99 Iteration: 7475 Training loss: 0.30818\n",
      "Epoch: 93/99 Iteration: 7476 Training loss: 0.42017\n",
      "Epoch: 93/99 Iteration: 7477 Training loss: 0.17580\n",
      "Epoch: 93/99 Iteration: 7478 Training loss: 0.19335\n",
      "Epoch: 93/99 Iteration: 7479 Training loss: 0.22052\n",
      "Epoch: 93/99 Iteration: 7480 Training loss: 0.28819\n",
      "Epoch: 93/99 Iteration: 7481 Training loss: 0.18732\n",
      "Epoch: 93/99 Iteration: 7482 Training loss: 0.24367\n",
      "Epoch: 93/99 Iteration: 7483 Training loss: 0.22759\n",
      "Epoch: 93/99 Iteration: 7484 Training loss: 0.34160\n",
      "Epoch: 93/99 Iteration: 7485 Training loss: 0.27410\n",
      "Epoch: 93/99 Iteration: 7486 Training loss: 0.37501\n",
      "Epoch: 93/99 Iteration: 7487 Training loss: 0.36785\n",
      "Epoch: 93/99 Iteration: 7488 Training loss: 0.37821\n",
      "Epoch: 93/99 Iteration: 7489 Training loss: 0.19095\n",
      "Epoch: 93/99 Iteration: 7490 Training loss: 0.27819\n",
      "Epoch: 93/99 Iteration: 7491 Training loss: 0.41320\n",
      "Epoch: 93/99 Iteration: 7492 Training loss: 0.28233\n",
      "Epoch: 93/99 Iteration: 7493 Training loss: 0.24200\n",
      "Epoch: 93/99 Iteration: 7494 Training loss: 0.32545\n",
      "Epoch: 93/99 Iteration: 7495 Training loss: 0.23767\n",
      "Epoch: 93/99 Iteration: 7496 Training loss: 0.19731\n",
      "Epoch: 93/99 Iteration: 7497 Training loss: 0.19774\n",
      "Epoch: 93/99 Iteration: 7498 Training loss: 0.22518\n",
      "Epoch: 93/99 Iteration: 7499 Training loss: 0.28992\n",
      "Epoch: 93/99 Iteration: 7500 Training loss: 0.17014\n",
      "***\n",
      "Epoch: 93/99 Iteration: 7500 Validation Acc: 0.8760\n",
      "***\n",
      "Epoch: 93/99 Iteration: 7501 Training loss: 0.32414\n",
      "Epoch: 93/99 Iteration: 7502 Training loss: 0.38634\n",
      "Epoch: 93/99 Iteration: 7503 Training loss: 0.51177\n",
      "Epoch: 93/99 Iteration: 7504 Training loss: 0.28729\n",
      "Epoch: 93/99 Iteration: 7505 Training loss: 0.29840\n",
      "Epoch: 93/99 Iteration: 7506 Training loss: 0.34205\n",
      "Epoch: 93/99 Iteration: 7507 Training loss: 0.30024\n",
      "Epoch: 93/99 Iteration: 7508 Training loss: 0.23012\n",
      "Epoch: 93/99 Iteration: 7509 Training loss: 0.32375\n",
      "Epoch: 93/99 Iteration: 7510 Training loss: 0.30846\n",
      "Epoch: 93/99 Iteration: 7511 Training loss: 0.25023\n",
      "Epoch: 93/99 Iteration: 7512 Training loss: 0.24220\n",
      "Epoch: 93/99 Iteration: 7513 Training loss: 0.26368\n",
      "Epoch: 93/99 Iteration: 7514 Training loss: 0.20946\n",
      "Epoch: 93/99 Iteration: 7515 Training loss: 0.19969\n",
      "Epoch: 93/99 Iteration: 7516 Training loss: 0.27558\n",
      "Epoch: 93/99 Iteration: 7517 Training loss: 0.31976\n",
      "Epoch: 93/99 Iteration: 7518 Training loss: 0.32614\n",
      "Epoch: 93/99 Iteration: 7519 Training loss: 0.25471\n",
      "Epoch: 94/99 Iteration: 7520 Training loss: 0.17809\n",
      "Epoch: 94/99 Iteration: 7521 Training loss: 0.38791\n",
      "Epoch: 94/99 Iteration: 7522 Training loss: 0.23559\n",
      "Epoch: 94/99 Iteration: 7523 Training loss: 0.26196\n",
      "Epoch: 94/99 Iteration: 7524 Training loss: 0.23371\n",
      "Epoch: 94/99 Iteration: 7525 Training loss: 0.26560\n",
      "Epoch: 94/99 Iteration: 7526 Training loss: 0.23426\n",
      "Epoch: 94/99 Iteration: 7527 Training loss: 0.24466\n",
      "Epoch: 94/99 Iteration: 7528 Training loss: 0.34873\n",
      "Epoch: 94/99 Iteration: 7529 Training loss: 0.27438\n",
      "Epoch: 94/99 Iteration: 7530 Training loss: 0.42731\n",
      "Epoch: 94/99 Iteration: 7531 Training loss: 0.27543\n",
      "Epoch: 94/99 Iteration: 7532 Training loss: 0.25395\n",
      "Epoch: 94/99 Iteration: 7533 Training loss: 0.42522\n",
      "Epoch: 94/99 Iteration: 7534 Training loss: 0.47431\n",
      "Epoch: 94/99 Iteration: 7535 Training loss: 0.33882\n",
      "Epoch: 94/99 Iteration: 7536 Training loss: 0.34933\n",
      "Epoch: 94/99 Iteration: 7537 Training loss: 0.22468\n",
      "Epoch: 94/99 Iteration: 7538 Training loss: 0.36053\n",
      "Epoch: 94/99 Iteration: 7539 Training loss: 0.36214\n",
      "Epoch: 94/99 Iteration: 7540 Training loss: 0.28211\n",
      "Epoch: 94/99 Iteration: 7541 Training loss: 0.27116\n",
      "Epoch: 94/99 Iteration: 7542 Training loss: 0.28705\n",
      "Epoch: 94/99 Iteration: 7543 Training loss: 0.35397\n",
      "Epoch: 94/99 Iteration: 7544 Training loss: 0.34686\n",
      "Epoch: 94/99 Iteration: 7545 Training loss: 0.30553\n",
      "Epoch: 94/99 Iteration: 7546 Training loss: 0.36456\n",
      "Epoch: 94/99 Iteration: 7547 Training loss: 0.26532\n",
      "Epoch: 94/99 Iteration: 7548 Training loss: 0.39902\n",
      "Epoch: 94/99 Iteration: 7549 Training loss: 0.37653\n",
      "Epoch: 94/99 Iteration: 7550 Training loss: 0.28276\n",
      "***\n",
      "Epoch: 94/99 Iteration: 7550 Validation Acc: 0.8560\n",
      "***\n",
      "Epoch: 94/99 Iteration: 7551 Training loss: 0.31092\n",
      "Epoch: 94/99 Iteration: 7552 Training loss: 0.22397\n",
      "Epoch: 94/99 Iteration: 7553 Training loss: 0.31483\n",
      "Epoch: 94/99 Iteration: 7554 Training loss: 0.38349\n",
      "Epoch: 94/99 Iteration: 7555 Training loss: 0.33823\n",
      "Epoch: 94/99 Iteration: 7556 Training loss: 0.47503\n",
      "Epoch: 94/99 Iteration: 7557 Training loss: 0.24221\n",
      "Epoch: 94/99 Iteration: 7558 Training loss: 0.30148\n",
      "Epoch: 94/99 Iteration: 7559 Training loss: 0.27856\n",
      "Epoch: 94/99 Iteration: 7560 Training loss: 0.26112\n",
      "Epoch: 94/99 Iteration: 7561 Training loss: 0.30431\n",
      "Epoch: 94/99 Iteration: 7562 Training loss: 0.47921\n",
      "Epoch: 94/99 Iteration: 7563 Training loss: 0.23318\n",
      "Epoch: 94/99 Iteration: 7564 Training loss: 0.44079\n",
      "Epoch: 94/99 Iteration: 7565 Training loss: 0.25630\n",
      "Epoch: 94/99 Iteration: 7566 Training loss: 0.36689\n",
      "Epoch: 94/99 Iteration: 7567 Training loss: 0.33922\n",
      "Epoch: 94/99 Iteration: 7568 Training loss: 0.30908\n",
      "Epoch: 94/99 Iteration: 7569 Training loss: 0.24570\n",
      "Epoch: 94/99 Iteration: 7570 Training loss: 0.35219\n",
      "Epoch: 94/99 Iteration: 7571 Training loss: 0.31454\n",
      "Epoch: 94/99 Iteration: 7572 Training loss: 0.24677\n",
      "Epoch: 94/99 Iteration: 7573 Training loss: 0.25255\n",
      "Epoch: 94/99 Iteration: 7574 Training loss: 0.42466\n",
      "Epoch: 94/99 Iteration: 7575 Training loss: 0.13917\n",
      "Epoch: 94/99 Iteration: 7576 Training loss: 0.49387\n",
      "Epoch: 94/99 Iteration: 7577 Training loss: 0.33177\n",
      "Epoch: 94/99 Iteration: 7578 Training loss: 0.30271\n",
      "Epoch: 94/99 Iteration: 7579 Training loss: 0.26229\n",
      "Epoch: 94/99 Iteration: 7580 Training loss: 0.32175\n",
      "Epoch: 94/99 Iteration: 7581 Training loss: 0.30880\n",
      "Epoch: 94/99 Iteration: 7582 Training loss: 0.22694\n",
      "Epoch: 94/99 Iteration: 7583 Training loss: 0.36973\n",
      "Epoch: 94/99 Iteration: 7584 Training loss: 0.26111\n",
      "Epoch: 94/99 Iteration: 7585 Training loss: 0.38910\n",
      "Epoch: 94/99 Iteration: 7586 Training loss: 0.22144\n",
      "Epoch: 94/99 Iteration: 7587 Training loss: 0.38008\n",
      "Epoch: 94/99 Iteration: 7588 Training loss: 0.22292\n",
      "Epoch: 94/99 Iteration: 7589 Training loss: 0.15662\n",
      "Epoch: 94/99 Iteration: 7590 Training loss: 0.34196\n",
      "Epoch: 94/99 Iteration: 7591 Training loss: 0.19993\n",
      "Epoch: 94/99 Iteration: 7592 Training loss: 0.24712\n",
      "Epoch: 94/99 Iteration: 7593 Training loss: 0.32549\n",
      "Epoch: 94/99 Iteration: 7594 Training loss: 0.22053\n",
      "Epoch: 94/99 Iteration: 7595 Training loss: 0.25208\n",
      "Epoch: 94/99 Iteration: 7596 Training loss: 0.29845\n",
      "Epoch: 94/99 Iteration: 7597 Training loss: 0.37944\n",
      "Epoch: 94/99 Iteration: 7598 Training loss: 0.22089\n",
      "Epoch: 94/99 Iteration: 7599 Training loss: 0.22531\n",
      "Epoch: 95/99 Iteration: 7600 Training loss: 0.23761\n",
      "***\n",
      "Epoch: 95/99 Iteration: 7600 Validation Acc: 0.8580\n",
      "***\n",
      "Epoch: 95/99 Iteration: 7601 Training loss: 0.42756\n",
      "Epoch: 95/99 Iteration: 7602 Training loss: 0.51341\n",
      "Epoch: 95/99 Iteration: 7603 Training loss: 0.28542\n",
      "Epoch: 95/99 Iteration: 7604 Training loss: 0.28126\n",
      "Epoch: 95/99 Iteration: 7605 Training loss: 0.22112\n",
      "Epoch: 95/99 Iteration: 7606 Training loss: 0.35214\n",
      "Epoch: 95/99 Iteration: 7607 Training loss: 0.25832\n",
      "Epoch: 95/99 Iteration: 7608 Training loss: 0.45281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/99 Iteration: 7609 Training loss: 0.24256\n",
      "Epoch: 95/99 Iteration: 7610 Training loss: 0.29489\n",
      "Epoch: 95/99 Iteration: 7611 Training loss: 0.22092\n",
      "Epoch: 95/99 Iteration: 7612 Training loss: 0.35197\n",
      "Epoch: 95/99 Iteration: 7613 Training loss: 0.33571\n",
      "Epoch: 95/99 Iteration: 7614 Training loss: 0.46883\n",
      "Epoch: 95/99 Iteration: 7615 Training loss: 0.30627\n",
      "Epoch: 95/99 Iteration: 7616 Training loss: 0.28178\n",
      "Epoch: 95/99 Iteration: 7617 Training loss: 0.37208\n",
      "Epoch: 95/99 Iteration: 7618 Training loss: 0.27527\n",
      "Epoch: 95/99 Iteration: 7619 Training loss: 0.34539\n",
      "Epoch: 95/99 Iteration: 7620 Training loss: 0.35788\n",
      "Epoch: 95/99 Iteration: 7621 Training loss: 0.25583\n",
      "Epoch: 95/99 Iteration: 7622 Training loss: 0.30339\n",
      "Epoch: 95/99 Iteration: 7623 Training loss: 0.40118\n",
      "Epoch: 95/99 Iteration: 7624 Training loss: 0.21622\n",
      "Epoch: 95/99 Iteration: 7625 Training loss: 0.24535\n",
      "Epoch: 95/99 Iteration: 7626 Training loss: 0.34335\n",
      "Epoch: 95/99 Iteration: 7627 Training loss: 0.25308\n",
      "Epoch: 95/99 Iteration: 7628 Training loss: 0.35128\n",
      "Epoch: 95/99 Iteration: 7629 Training loss: 0.47714\n",
      "Epoch: 95/99 Iteration: 7630 Training loss: 0.18849\n",
      "Epoch: 95/99 Iteration: 7631 Training loss: 0.21947\n",
      "Epoch: 95/99 Iteration: 7632 Training loss: 0.30297\n",
      "Epoch: 95/99 Iteration: 7633 Training loss: 0.33506\n",
      "Epoch: 95/99 Iteration: 7634 Training loss: 0.19273\n",
      "Epoch: 95/99 Iteration: 7635 Training loss: 0.22951\n",
      "Epoch: 95/99 Iteration: 7636 Training loss: 0.47515\n",
      "Epoch: 95/99 Iteration: 7637 Training loss: 0.26238\n",
      "Epoch: 95/99 Iteration: 7638 Training loss: 0.30881\n",
      "Epoch: 95/99 Iteration: 7639 Training loss: 0.27143\n",
      "Epoch: 95/99 Iteration: 7640 Training loss: 0.29244\n",
      "Epoch: 95/99 Iteration: 7641 Training loss: 0.26368\n",
      "Epoch: 95/99 Iteration: 7642 Training loss: 0.29884\n",
      "Epoch: 95/99 Iteration: 7643 Training loss: 0.20458\n",
      "Epoch: 95/99 Iteration: 7644 Training loss: 0.17900\n",
      "Epoch: 95/99 Iteration: 7645 Training loss: 0.18484\n",
      "Epoch: 95/99 Iteration: 7646 Training loss: 0.40321\n",
      "Epoch: 95/99 Iteration: 7647 Training loss: 0.29262\n",
      "Epoch: 95/99 Iteration: 7648 Training loss: 0.39335\n",
      "Epoch: 95/99 Iteration: 7649 Training loss: 0.27521\n",
      "Epoch: 95/99 Iteration: 7650 Training loss: 0.15220\n",
      "***\n",
      "Epoch: 95/99 Iteration: 7650 Validation Acc: 0.8760\n",
      "***\n",
      "Epoch: 95/99 Iteration: 7651 Training loss: 0.31972\n",
      "Epoch: 95/99 Iteration: 7652 Training loss: 0.20841\n",
      "Epoch: 95/99 Iteration: 7653 Training loss: 0.21985\n",
      "Epoch: 95/99 Iteration: 7654 Training loss: 0.23067\n",
      "Epoch: 95/99 Iteration: 7655 Training loss: 0.15925\n",
      "Epoch: 95/99 Iteration: 7656 Training loss: 0.24121\n",
      "Epoch: 95/99 Iteration: 7657 Training loss: 0.19721\n",
      "Epoch: 95/99 Iteration: 7658 Training loss: 0.22993\n",
      "Epoch: 95/99 Iteration: 7659 Training loss: 0.33707\n",
      "Epoch: 95/99 Iteration: 7660 Training loss: 0.31864\n",
      "Epoch: 95/99 Iteration: 7661 Training loss: 0.27488\n",
      "Epoch: 95/99 Iteration: 7662 Training loss: 0.28080\n",
      "Epoch: 95/99 Iteration: 7663 Training loss: 0.29107\n",
      "Epoch: 95/99 Iteration: 7664 Training loss: 0.23796\n",
      "Epoch: 95/99 Iteration: 7665 Training loss: 0.31058\n",
      "Epoch: 95/99 Iteration: 7666 Training loss: 0.31186\n",
      "Epoch: 95/99 Iteration: 7667 Training loss: 0.29859\n",
      "Epoch: 95/99 Iteration: 7668 Training loss: 0.23560\n",
      "Epoch: 95/99 Iteration: 7669 Training loss: 0.22721\n",
      "Epoch: 95/99 Iteration: 7670 Training loss: 0.40613\n",
      "Epoch: 95/99 Iteration: 7671 Training loss: 0.37560\n",
      "Epoch: 95/99 Iteration: 7672 Training loss: 0.31671\n",
      "Epoch: 95/99 Iteration: 7673 Training loss: 0.19371\n",
      "Epoch: 95/99 Iteration: 7674 Training loss: 0.41073\n",
      "Epoch: 95/99 Iteration: 7675 Training loss: 0.43772\n",
      "Epoch: 95/99 Iteration: 7676 Training loss: 0.26960\n",
      "Epoch: 95/99 Iteration: 7677 Training loss: 0.33941\n",
      "Epoch: 95/99 Iteration: 7678 Training loss: 0.34256\n",
      "Epoch: 95/99 Iteration: 7679 Training loss: 0.52902\n",
      "Epoch: 96/99 Iteration: 7680 Training loss: 0.20017\n",
      "Epoch: 96/99 Iteration: 7681 Training loss: 0.30257\n",
      "Epoch: 96/99 Iteration: 7682 Training loss: 0.22540\n",
      "Epoch: 96/99 Iteration: 7683 Training loss: 0.24753\n",
      "Epoch: 96/99 Iteration: 7684 Training loss: 0.20533\n",
      "Epoch: 96/99 Iteration: 7685 Training loss: 0.20991\n",
      "Epoch: 96/99 Iteration: 7686 Training loss: 0.29934\n",
      "Epoch: 96/99 Iteration: 7687 Training loss: 0.25811\n",
      "Epoch: 96/99 Iteration: 7688 Training loss: 0.36254\n",
      "Epoch: 96/99 Iteration: 7689 Training loss: 0.24470\n",
      "Epoch: 96/99 Iteration: 7690 Training loss: 0.28198\n",
      "Epoch: 96/99 Iteration: 7691 Training loss: 0.24857\n",
      "Epoch: 96/99 Iteration: 7692 Training loss: 0.17515\n",
      "Epoch: 96/99 Iteration: 7693 Training loss: 0.39695\n",
      "Epoch: 96/99 Iteration: 7694 Training loss: 0.36136\n",
      "Epoch: 96/99 Iteration: 7695 Training loss: 0.19392\n",
      "Epoch: 96/99 Iteration: 7696 Training loss: 0.18216\n",
      "Epoch: 96/99 Iteration: 7697 Training loss: 0.27872\n",
      "Epoch: 96/99 Iteration: 7698 Training loss: 0.29152\n",
      "Epoch: 96/99 Iteration: 7699 Training loss: 0.13213\n",
      "Epoch: 96/99 Iteration: 7700 Training loss: 0.18032\n",
      "***\n",
      "Epoch: 96/99 Iteration: 7700 Validation Acc: 0.8700\n",
      "***\n",
      "Epoch: 96/99 Iteration: 7701 Training loss: 0.22253\n",
      "Epoch: 96/99 Iteration: 7702 Training loss: 0.19374\n",
      "Epoch: 96/99 Iteration: 7703 Training loss: 0.22192\n",
      "Epoch: 96/99 Iteration: 7704 Training loss: 0.14250\n",
      "Epoch: 96/99 Iteration: 7705 Training loss: 0.24909\n",
      "Epoch: 96/99 Iteration: 7706 Training loss: 0.24677\n",
      "Epoch: 96/99 Iteration: 7707 Training loss: 0.23569\n",
      "Epoch: 96/99 Iteration: 7708 Training loss: 0.26828\n",
      "Epoch: 96/99 Iteration: 7709 Training loss: 0.41454\n",
      "Epoch: 96/99 Iteration: 7710 Training loss: 0.15489\n",
      "Epoch: 96/99 Iteration: 7711 Training loss: 0.21645\n",
      "Epoch: 96/99 Iteration: 7712 Training loss: 0.23667\n",
      "Epoch: 96/99 Iteration: 7713 Training loss: 0.39108\n",
      "Epoch: 96/99 Iteration: 7714 Training loss: 0.29527\n",
      "Epoch: 96/99 Iteration: 7715 Training loss: 0.18976\n",
      "Epoch: 96/99 Iteration: 7716 Training loss: 0.43225\n",
      "Epoch: 96/99 Iteration: 7717 Training loss: 0.26041\n",
      "Epoch: 96/99 Iteration: 7718 Training loss: 0.26116\n",
      "Epoch: 96/99 Iteration: 7719 Training loss: 0.31882\n",
      "Epoch: 96/99 Iteration: 7720 Training loss: 0.31355\n",
      "Epoch: 96/99 Iteration: 7721 Training loss: 0.21381\n",
      "Epoch: 96/99 Iteration: 7722 Training loss: 0.41963\n",
      "Epoch: 96/99 Iteration: 7723 Training loss: 0.09566\n",
      "Epoch: 96/99 Iteration: 7724 Training loss: 0.29445\n",
      "Epoch: 96/99 Iteration: 7725 Training loss: 0.13225\n",
      "Epoch: 96/99 Iteration: 7726 Training loss: 0.47483\n",
      "Epoch: 96/99 Iteration: 7727 Training loss: 0.28497\n",
      "Epoch: 96/99 Iteration: 7728 Training loss: 0.32891\n",
      "Epoch: 96/99 Iteration: 7729 Training loss: 0.20818\n",
      "Epoch: 96/99 Iteration: 7730 Training loss: 0.33572\n",
      "Epoch: 96/99 Iteration: 7731 Training loss: 0.29943\n",
      "Epoch: 96/99 Iteration: 7732 Training loss: 0.26638\n",
      "Epoch: 96/99 Iteration: 7733 Training loss: 0.36350\n",
      "Epoch: 96/99 Iteration: 7734 Training loss: 0.32889\n",
      "Epoch: 96/99 Iteration: 7735 Training loss: 0.28443\n",
      "Epoch: 96/99 Iteration: 7736 Training loss: 0.27477\n",
      "Epoch: 96/99 Iteration: 7737 Training loss: 0.35731\n",
      "Epoch: 96/99 Iteration: 7738 Training loss: 0.23895\n",
      "Epoch: 96/99 Iteration: 7739 Training loss: 0.28843\n",
      "Epoch: 96/99 Iteration: 7740 Training loss: 0.24482\n",
      "Epoch: 96/99 Iteration: 7741 Training loss: 0.26779\n",
      "Epoch: 96/99 Iteration: 7742 Training loss: 0.34340\n",
      "Epoch: 96/99 Iteration: 7743 Training loss: 0.29624\n",
      "Epoch: 96/99 Iteration: 7744 Training loss: 0.25388\n",
      "Epoch: 96/99 Iteration: 7745 Training loss: 0.33783\n",
      "Epoch: 96/99 Iteration: 7746 Training loss: 0.41870\n",
      "Epoch: 96/99 Iteration: 7747 Training loss: 0.27391\n",
      "Epoch: 96/99 Iteration: 7748 Training loss: 0.22767\n",
      "Epoch: 96/99 Iteration: 7749 Training loss: 0.25558\n",
      "Epoch: 96/99 Iteration: 7750 Training loss: 0.37780\n",
      "***\n",
      "Epoch: 96/99 Iteration: 7750 Validation Acc: 0.8590\n",
      "***\n",
      "Epoch: 96/99 Iteration: 7751 Training loss: 0.23713\n",
      "Epoch: 96/99 Iteration: 7752 Training loss: 0.30989\n",
      "Epoch: 96/99 Iteration: 7753 Training loss: 0.29650\n",
      "Epoch: 96/99 Iteration: 7754 Training loss: 0.26048\n",
      "Epoch: 96/99 Iteration: 7755 Training loss: 0.24116\n",
      "Epoch: 96/99 Iteration: 7756 Training loss: 0.24621\n",
      "Epoch: 96/99 Iteration: 7757 Training loss: 0.46790\n",
      "Epoch: 96/99 Iteration: 7758 Training loss: 0.28208\n",
      "Epoch: 96/99 Iteration: 7759 Training loss: 0.17811\n",
      "Epoch: 97/99 Iteration: 7760 Training loss: 0.27461\n",
      "Epoch: 97/99 Iteration: 7761 Training loss: 0.29196\n",
      "Epoch: 97/99 Iteration: 7762 Training loss: 0.26883\n",
      "Epoch: 97/99 Iteration: 7763 Training loss: 0.24979\n",
      "Epoch: 97/99 Iteration: 7764 Training loss: 0.19270\n",
      "Epoch: 97/99 Iteration: 7765 Training loss: 0.23782\n",
      "Epoch: 97/99 Iteration: 7766 Training loss: 0.26468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/99 Iteration: 7767 Training loss: 0.50592\n",
      "Epoch: 97/99 Iteration: 7768 Training loss: 0.35932\n",
      "Epoch: 97/99 Iteration: 7769 Training loss: 0.27380\n",
      "Epoch: 97/99 Iteration: 7770 Training loss: 0.29748\n",
      "Epoch: 97/99 Iteration: 7771 Training loss: 0.30815\n",
      "Epoch: 97/99 Iteration: 7772 Training loss: 0.15438\n",
      "Epoch: 97/99 Iteration: 7773 Training loss: 0.35209\n",
      "Epoch: 97/99 Iteration: 7774 Training loss: 0.28935\n",
      "Epoch: 97/99 Iteration: 7775 Training loss: 0.16640\n",
      "Epoch: 97/99 Iteration: 7776 Training loss: 0.22993\n",
      "Epoch: 97/99 Iteration: 7777 Training loss: 0.21687\n",
      "Epoch: 97/99 Iteration: 7778 Training loss: 0.57719\n",
      "Epoch: 97/99 Iteration: 7779 Training loss: 0.26923\n",
      "Epoch: 97/99 Iteration: 7780 Training loss: 0.22404\n",
      "Epoch: 97/99 Iteration: 7781 Training loss: 0.23338\n",
      "Epoch: 97/99 Iteration: 7782 Training loss: 0.33074\n",
      "Epoch: 97/99 Iteration: 7783 Training loss: 0.22342\n",
      "Epoch: 97/99 Iteration: 7784 Training loss: 0.25046\n",
      "Epoch: 97/99 Iteration: 7785 Training loss: 0.28425\n",
      "Epoch: 97/99 Iteration: 7786 Training loss: 0.29054\n",
      "Epoch: 97/99 Iteration: 7787 Training loss: 0.29249\n",
      "Epoch: 97/99 Iteration: 7788 Training loss: 0.48131\n",
      "Epoch: 97/99 Iteration: 7789 Training loss: 0.47186\n",
      "Epoch: 97/99 Iteration: 7790 Training loss: 0.18644\n",
      "Epoch: 97/99 Iteration: 7791 Training loss: 0.22797\n",
      "Epoch: 97/99 Iteration: 7792 Training loss: 0.27255\n",
      "Epoch: 97/99 Iteration: 7793 Training loss: 0.22484\n",
      "Epoch: 97/99 Iteration: 7794 Training loss: 0.28284\n",
      "Epoch: 97/99 Iteration: 7795 Training loss: 0.33144\n",
      "Epoch: 97/99 Iteration: 7796 Training loss: 0.29791\n",
      "Epoch: 97/99 Iteration: 7797 Training loss: 0.22309\n",
      "Epoch: 97/99 Iteration: 7798 Training loss: 0.29974\n",
      "Epoch: 97/99 Iteration: 7799 Training loss: 0.27838\n",
      "Epoch: 97/99 Iteration: 7800 Training loss: 0.39076\n",
      "***\n",
      "Epoch: 97/99 Iteration: 7800 Validation Acc: 0.8720\n",
      "***\n",
      "Epoch: 97/99 Iteration: 7801 Training loss: 0.35136\n",
      "Epoch: 97/99 Iteration: 7802 Training loss: 0.27231\n",
      "Epoch: 97/99 Iteration: 7803 Training loss: 0.18632\n",
      "Epoch: 97/99 Iteration: 7804 Training loss: 0.28273\n",
      "Epoch: 97/99 Iteration: 7805 Training loss: 0.19045\n",
      "Epoch: 97/99 Iteration: 7806 Training loss: 0.40029\n",
      "Epoch: 97/99 Iteration: 7807 Training loss: 0.18419\n",
      "Epoch: 97/99 Iteration: 7808 Training loss: 0.33820\n",
      "Epoch: 97/99 Iteration: 7809 Training loss: 0.19318\n",
      "Epoch: 97/99 Iteration: 7810 Training loss: 0.22528\n",
      "Epoch: 97/99 Iteration: 7811 Training loss: 0.35381\n",
      "Epoch: 97/99 Iteration: 7812 Training loss: 0.34054\n",
      "Epoch: 97/99 Iteration: 7813 Training loss: 0.25224\n",
      "Epoch: 97/99 Iteration: 7814 Training loss: 0.39474\n",
      "Epoch: 97/99 Iteration: 7815 Training loss: 0.19760\n",
      "Epoch: 97/99 Iteration: 7816 Training loss: 0.25099\n",
      "Epoch: 97/99 Iteration: 7817 Training loss: 0.28844\n",
      "Epoch: 97/99 Iteration: 7818 Training loss: 0.31570\n",
      "Epoch: 97/99 Iteration: 7819 Training loss: 0.45239\n",
      "Epoch: 97/99 Iteration: 7820 Training loss: 0.29211\n",
      "Epoch: 97/99 Iteration: 7821 Training loss: 0.35331\n",
      "Epoch: 97/99 Iteration: 7822 Training loss: 0.21345\n",
      "Epoch: 97/99 Iteration: 7823 Training loss: 0.30682\n",
      "Epoch: 97/99 Iteration: 7824 Training loss: 0.33431\n",
      "Epoch: 97/99 Iteration: 7825 Training loss: 0.28629\n",
      "Epoch: 97/99 Iteration: 7826 Training loss: 0.47404\n",
      "Epoch: 97/99 Iteration: 7827 Training loss: 0.34534\n",
      "Epoch: 97/99 Iteration: 7828 Training loss: 0.24214\n",
      "Epoch: 97/99 Iteration: 7829 Training loss: 0.28244\n",
      "Epoch: 97/99 Iteration: 7830 Training loss: 0.31778\n",
      "Epoch: 97/99 Iteration: 7831 Training loss: 0.25267\n",
      "Epoch: 97/99 Iteration: 7832 Training loss: 0.29162\n",
      "Epoch: 97/99 Iteration: 7833 Training loss: 0.31606\n",
      "Epoch: 97/99 Iteration: 7834 Training loss: 0.34608\n",
      "Epoch: 97/99 Iteration: 7835 Training loss: 0.26558\n",
      "Epoch: 97/99 Iteration: 7836 Training loss: 0.29551\n",
      "Epoch: 97/99 Iteration: 7837 Training loss: 0.25692\n",
      "Epoch: 97/99 Iteration: 7838 Training loss: 0.28651\n",
      "Epoch: 97/99 Iteration: 7839 Training loss: 0.27995\n",
      "Epoch: 98/99 Iteration: 7840 Training loss: 0.20371\n",
      "Epoch: 98/99 Iteration: 7841 Training loss: 0.29911\n",
      "Epoch: 98/99 Iteration: 7842 Training loss: 0.42152\n",
      "Epoch: 98/99 Iteration: 7843 Training loss: 0.22728\n",
      "Epoch: 98/99 Iteration: 7844 Training loss: 0.29753\n",
      "Epoch: 98/99 Iteration: 7845 Training loss: 0.35443\n",
      "Epoch: 98/99 Iteration: 7846 Training loss: 0.27253\n",
      "Epoch: 98/99 Iteration: 7847 Training loss: 0.36493\n",
      "Epoch: 98/99 Iteration: 7848 Training loss: 0.30167\n",
      "Epoch: 98/99 Iteration: 7849 Training loss: 0.24637\n",
      "Epoch: 98/99 Iteration: 7850 Training loss: 0.15715\n",
      "***\n",
      "Epoch: 98/99 Iteration: 7850 Validation Acc: 0.8660\n",
      "***\n",
      "Epoch: 98/99 Iteration: 7851 Training loss: 0.21782\n",
      "Epoch: 98/99 Iteration: 7852 Training loss: 0.32570\n",
      "Epoch: 98/99 Iteration: 7853 Training loss: 0.33693\n",
      "Epoch: 98/99 Iteration: 7854 Training loss: 0.43608\n",
      "Epoch: 98/99 Iteration: 7855 Training loss: 0.25873\n",
      "Epoch: 98/99 Iteration: 7856 Training loss: 0.26762\n",
      "Epoch: 98/99 Iteration: 7857 Training loss: 0.18020\n",
      "Epoch: 98/99 Iteration: 7858 Training loss: 0.37084\n",
      "Epoch: 98/99 Iteration: 7859 Training loss: 0.35736\n",
      "Epoch: 98/99 Iteration: 7860 Training loss: 0.31721\n",
      "Epoch: 98/99 Iteration: 7861 Training loss: 0.33579\n",
      "Epoch: 98/99 Iteration: 7862 Training loss: 0.36180\n",
      "Epoch: 98/99 Iteration: 7863 Training loss: 0.38716\n",
      "Epoch: 98/99 Iteration: 7864 Training loss: 0.25736\n",
      "Epoch: 98/99 Iteration: 7865 Training loss: 0.42701\n",
      "Epoch: 98/99 Iteration: 7866 Training loss: 0.50137\n",
      "Epoch: 98/99 Iteration: 7867 Training loss: 0.41510\n",
      "Epoch: 98/99 Iteration: 7868 Training loss: 0.43836\n",
      "Epoch: 98/99 Iteration: 7869 Training loss: 0.46461\n",
      "Epoch: 98/99 Iteration: 7870 Training loss: 0.20410\n",
      "Epoch: 98/99 Iteration: 7871 Training loss: 0.33951\n",
      "Epoch: 98/99 Iteration: 7872 Training loss: 0.29733\n",
      "Epoch: 98/99 Iteration: 7873 Training loss: 0.13805\n",
      "Epoch: 98/99 Iteration: 7874 Training loss: 0.22005\n",
      "Epoch: 98/99 Iteration: 7875 Training loss: 0.24844\n",
      "Epoch: 98/99 Iteration: 7876 Training loss: 0.44281\n",
      "Epoch: 98/99 Iteration: 7877 Training loss: 0.22904\n",
      "Epoch: 98/99 Iteration: 7878 Training loss: 0.28436\n",
      "Epoch: 98/99 Iteration: 7879 Training loss: 0.25134\n",
      "Epoch: 98/99 Iteration: 7880 Training loss: 0.27732\n",
      "Epoch: 98/99 Iteration: 7881 Training loss: 0.31382\n",
      "Epoch: 98/99 Iteration: 7882 Training loss: 0.36058\n",
      "Epoch: 98/99 Iteration: 7883 Training loss: 0.29182\n",
      "Epoch: 98/99 Iteration: 7884 Training loss: 0.37049\n",
      "Epoch: 98/99 Iteration: 7885 Training loss: 0.23036\n",
      "Epoch: 98/99 Iteration: 7886 Training loss: 0.36332\n",
      "Epoch: 98/99 Iteration: 7887 Training loss: 0.25967\n",
      "Epoch: 98/99 Iteration: 7888 Training loss: 0.39656\n",
      "Epoch: 98/99 Iteration: 7889 Training loss: 0.25760\n",
      "Epoch: 98/99 Iteration: 7890 Training loss: 0.31001\n",
      "Epoch: 98/99 Iteration: 7891 Training loss: 0.22197\n",
      "Epoch: 98/99 Iteration: 7892 Training loss: 0.29404\n",
      "Epoch: 98/99 Iteration: 7893 Training loss: 0.24420\n",
      "Epoch: 98/99 Iteration: 7894 Training loss: 0.24040\n",
      "Epoch: 98/99 Iteration: 7895 Training loss: 0.35868\n",
      "Epoch: 98/99 Iteration: 7896 Training loss: 0.25912\n",
      "Epoch: 98/99 Iteration: 7897 Training loss: 0.24844\n",
      "Epoch: 98/99 Iteration: 7898 Training loss: 0.21183\n",
      "Epoch: 98/99 Iteration: 7899 Training loss: 0.32362\n",
      "Epoch: 98/99 Iteration: 7900 Training loss: 0.31613\n",
      "***\n",
      "Epoch: 98/99 Iteration: 7900 Validation Acc: 0.8660\n",
      "***\n",
      "Epoch: 98/99 Iteration: 7901 Training loss: 0.22966\n",
      "Epoch: 98/99 Iteration: 7902 Training loss: 0.18207\n",
      "Epoch: 98/99 Iteration: 7903 Training loss: 0.45132\n",
      "Epoch: 98/99 Iteration: 7904 Training loss: 0.25906\n",
      "Epoch: 98/99 Iteration: 7905 Training loss: 0.28322\n",
      "Epoch: 98/99 Iteration: 7906 Training loss: 0.33380\n",
      "Epoch: 98/99 Iteration: 7907 Training loss: 0.34990\n",
      "Epoch: 98/99 Iteration: 7908 Training loss: 0.25163\n",
      "Epoch: 98/99 Iteration: 7909 Training loss: 0.35549\n",
      "Epoch: 98/99 Iteration: 7910 Training loss: 0.39208\n",
      "Epoch: 98/99 Iteration: 7911 Training loss: 0.23887\n",
      "Epoch: 98/99 Iteration: 7912 Training loss: 0.23917\n",
      "Epoch: 98/99 Iteration: 7913 Training loss: 0.20664\n",
      "Epoch: 98/99 Iteration: 7914 Training loss: 0.27850\n",
      "Epoch: 98/99 Iteration: 7915 Training loss: 0.29484\n",
      "Epoch: 98/99 Iteration: 7916 Training loss: 0.22819\n",
      "Epoch: 98/99 Iteration: 7917 Training loss: 0.29484\n",
      "Epoch: 98/99 Iteration: 7918 Training loss: 0.24011\n",
      "Epoch: 98/99 Iteration: 7919 Training loss: 0.33085\n",
      "Epoch: 99/99 Iteration: 7920 Training loss: 0.14467\n",
      "Epoch: 99/99 Iteration: 7921 Training loss: 0.39313\n",
      "Epoch: 99/99 Iteration: 7922 Training loss: 0.24157\n",
      "Epoch: 99/99 Iteration: 7923 Training loss: 0.25839\n",
      "Epoch: 99/99 Iteration: 7924 Training loss: 0.28878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/99 Iteration: 7925 Training loss: 0.22689\n",
      "Epoch: 99/99 Iteration: 7926 Training loss: 0.35536\n",
      "Epoch: 99/99 Iteration: 7927 Training loss: 0.19873\n",
      "Epoch: 99/99 Iteration: 7928 Training loss: 0.34492\n",
      "Epoch: 99/99 Iteration: 7929 Training loss: 0.27067\n",
      "Epoch: 99/99 Iteration: 7930 Training loss: 0.27004\n",
      "Epoch: 99/99 Iteration: 7931 Training loss: 0.26256\n",
      "Epoch: 99/99 Iteration: 7932 Training loss: 0.20734\n",
      "Epoch: 99/99 Iteration: 7933 Training loss: 0.28996\n",
      "Epoch: 99/99 Iteration: 7934 Training loss: 0.22573\n",
      "Epoch: 99/99 Iteration: 7935 Training loss: 0.18059\n",
      "Epoch: 99/99 Iteration: 7936 Training loss: 0.21314\n",
      "Epoch: 99/99 Iteration: 7937 Training loss: 0.18527\n",
      "Epoch: 99/99 Iteration: 7938 Training loss: 0.41194\n",
      "Epoch: 99/99 Iteration: 7939 Training loss: 0.29342\n",
      "Epoch: 99/99 Iteration: 7940 Training loss: 0.32040\n",
      "Epoch: 99/99 Iteration: 7941 Training loss: 0.25793\n",
      "Epoch: 99/99 Iteration: 7942 Training loss: 0.21144\n",
      "Epoch: 99/99 Iteration: 7943 Training loss: 0.29969\n",
      "Epoch: 99/99 Iteration: 7944 Training loss: 0.23145\n",
      "Epoch: 99/99 Iteration: 7945 Training loss: 0.28684\n",
      "Epoch: 99/99 Iteration: 7946 Training loss: 0.29842\n",
      "Epoch: 99/99 Iteration: 7947 Training loss: 0.17177\n",
      "Epoch: 99/99 Iteration: 7948 Training loss: 0.34812\n",
      "Epoch: 99/99 Iteration: 7949 Training loss: 0.39225\n",
      "Epoch: 99/99 Iteration: 7950 Training loss: 0.24901\n",
      "***\n",
      "Epoch: 99/99 Iteration: 7950 Validation Acc: 0.8640\n",
      "***\n",
      "Epoch: 99/99 Iteration: 7951 Training loss: 0.21171\n",
      "Epoch: 99/99 Iteration: 7952 Training loss: 0.34531\n",
      "Epoch: 99/99 Iteration: 7953 Training loss: 0.18174\n",
      "Epoch: 99/99 Iteration: 7954 Training loss: 0.18406\n",
      "Epoch: 99/99 Iteration: 7955 Training loss: 0.26093\n",
      "Epoch: 99/99 Iteration: 7956 Training loss: 0.33390\n",
      "Epoch: 99/99 Iteration: 7957 Training loss: 0.21094\n",
      "Epoch: 99/99 Iteration: 7958 Training loss: 0.18636\n",
      "Epoch: 99/99 Iteration: 7959 Training loss: 0.27991\n",
      "Epoch: 99/99 Iteration: 7960 Training loss: 0.31093\n",
      "Epoch: 99/99 Iteration: 7961 Training loss: 0.32904\n",
      "Epoch: 99/99 Iteration: 7962 Training loss: 0.26520\n",
      "Epoch: 99/99 Iteration: 7963 Training loss: 0.28232\n",
      "Epoch: 99/99 Iteration: 7964 Training loss: 0.36515\n",
      "Epoch: 99/99 Iteration: 7965 Training loss: 0.23011\n",
      "Epoch: 99/99 Iteration: 7966 Training loss: 0.26744\n",
      "Epoch: 99/99 Iteration: 7967 Training loss: 0.19380\n",
      "Epoch: 99/99 Iteration: 7968 Training loss: 0.46078\n",
      "Epoch: 99/99 Iteration: 7969 Training loss: 0.28186\n",
      "Epoch: 99/99 Iteration: 7970 Training loss: 0.23703\n",
      "Epoch: 99/99 Iteration: 7971 Training loss: 0.27112\n",
      "Epoch: 99/99 Iteration: 7972 Training loss: 0.21565\n",
      "Epoch: 99/99 Iteration: 7973 Training loss: 0.22355\n",
      "Epoch: 99/99 Iteration: 7974 Training loss: 0.30738\n",
      "Epoch: 99/99 Iteration: 7975 Training loss: 0.21437\n",
      "Epoch: 99/99 Iteration: 7976 Training loss: 0.27268\n",
      "Epoch: 99/99 Iteration: 7977 Training loss: 0.25706\n",
      "Epoch: 99/99 Iteration: 7978 Training loss: 0.23482\n",
      "Epoch: 99/99 Iteration: 7979 Training loss: 0.27539\n",
      "Epoch: 99/99 Iteration: 7980 Training loss: 0.25862\n",
      "Epoch: 99/99 Iteration: 7981 Training loss: 0.19854\n",
      "Epoch: 99/99 Iteration: 7982 Training loss: 0.29840\n",
      "Epoch: 99/99 Iteration: 7983 Training loss: 0.28783\n",
      "Epoch: 99/99 Iteration: 7984 Training loss: 0.27093\n",
      "Epoch: 99/99 Iteration: 7985 Training loss: 0.36354\n",
      "Epoch: 99/99 Iteration: 7986 Training loss: 0.33702\n",
      "Epoch: 99/99 Iteration: 7987 Training loss: 0.32881\n",
      "Epoch: 99/99 Iteration: 7988 Training loss: 0.19759\n",
      "Epoch: 99/99 Iteration: 7989 Training loss: 0.19907\n",
      "Epoch: 99/99 Iteration: 7990 Training loss: 0.31335\n",
      "Epoch: 99/99 Iteration: 7991 Training loss: 0.13695\n",
      "Epoch: 99/99 Iteration: 7992 Training loss: 0.29877\n",
      "Epoch: 99/99 Iteration: 7993 Training loss: 0.18881\n",
      "Epoch: 99/99 Iteration: 7994 Training loss: 0.22157\n",
      "Epoch: 99/99 Iteration: 7995 Training loss: 0.19163\n",
      "Epoch: 99/99 Iteration: 7996 Training loss: 0.17114\n",
      "Epoch: 99/99 Iteration: 7997 Training loss: 0.40640\n",
      "Epoch: 99/99 Iteration: 7998 Training loss: 0.26134\n",
      "Epoch: 99/99 Iteration: 7999 Training loss: 0.20821\n",
      "***\n",
      "Epoch: 99/99 Iteration: 7999 Validation Acc: 0.8830\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "iteration = 0\n",
    "\n",
    "if not os.path.isdir('./checkpoints'):\n",
    "    !mkdir ./checkpoints\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in batch_creator(train_x, train_y, batch_size=batch_size, flatted=True):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 0.5\n",
    "                   }\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs - 1),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            \n",
    "            if iteration % 50 == 0 or iteration == ((train_x.shape[0] * epochs / batch_size) - 1):\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y,\n",
    "                        keep_prob: 1.0\n",
    "                       }\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                print(\"***\\nEpoch: {}/{}\".format(e, epochs - 1),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\\n***\".format(val_acc))\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    saver.save(sess, \"./checkpoints/svhn.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.20%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y,\n",
    "            keep_prob: 1.0\n",
    "           }\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:3.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with a sample image\n",
    "\n",
    "Maybe with more data and more training the model can still be improved, but it is possible to see tha with the parameters used the model already achieves a very good accuracy. And even with more than one digit in the image it knows that the relevant one it the middle one as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_image_idx = np.random.randint(0, labels.shape[0])\n",
    "sample_image = features[:,:,:,sample_image_idx]\n",
    "sample_label = labels[sample_image_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAAOzCAYAAAAROpOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+C8wGhiuEoUUHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f022ac48390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample_images(features, labels, 1, test_data=sample_image_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    resized_image = resize_images(sample_image[:,:,:,np.newaxis])\n",
    "    feed_dict = {images: resized_image}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: code, keep_prob: 1.0}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasT/AzkdQMkcYgg0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f022af7ce90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(np.arange(10), prediction)\n",
    "plt.title('True {} | Predicted {}'.format(sample_label[0], np.argmax(prediction) + 1))\n",
    "plt.yticks(np.arange(10), np.unique(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
