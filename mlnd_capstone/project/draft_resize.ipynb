{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Street View House Numbers (SVHN) Dataset\n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. The dataset and more information can be founded [`here`](http://ufldl.stanford.edu/housenumbers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from __future__ import print_function\n",
    "from urllib import urlretrieve\n",
    "from scipy.io import loadmat\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalization(labels, size=1000):\n",
    "    idx = None\n",
    "    for i in np.unique(labels):\n",
    "        draw_bucket, _ = np.where(labels == i)\n",
    "        samples = np.random.choice(draw_bucket, size=int(size / len(np.unique(labels))))\n",
    "        if idx is None:\n",
    "            idx = samples\n",
    "        else:\n",
    "            idx = np.concatenate((idx, samples))\n",
    "    np.random.shuffle(idx)\n",
    "    return list(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_sample_images(features, labels, n, test_data=None, normalized=False):\n",
    "    \"\"\"\n",
    "    Displays a set of images from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    features: array of images.\n",
    "    labels: array of the feature's labels.\n",
    "    n: int, number of images to display.\n",
    "    normalized: boolean, if True adjusts the function to work with the transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i + 1)\n",
    "        image_num = np.random.randint(0, high=len(labels))\n",
    "        if test_data is not None:\n",
    "            image_num = test_data\n",
    "        if normalized:\n",
    "            image_num = np.random.randint(0, high=10)\n",
    "            sample_image = np.squeeze(features[image_num], axis=2)\n",
    "        else:\n",
    "            sample_image = features[:,:,:,image_num]\n",
    "        sample_label = labels[image_num]\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.title('Label: {}\\nFormat: {}\\nMin: {:.2f} | Max: {:.2f}'.format(\n",
    "            sample_label,\n",
    "            sample_image.shape,\n",
    "            sample_image.min(),\n",
    "            sample_image.max()))\n",
    "\n",
    "        plt.imshow(sample_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(features, new_size=(224,224)):\n",
    "    n = features.shape[3]\n",
    "    resized_images = []\n",
    "    for i in range(n):\n",
    "        image = features[:,:,:,i]\n",
    "        image = resize(image, new_size)\n",
    "        resized_images.append(image)\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    \n",
    "    Args:\n",
    "    ====\n",
    "    labels: List of sample labels\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(labels)):\n",
    "        result.append([1 if labels[i] == j + 1 else 0 for j in range(10)])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_creator(features, labels, batch_size, val_size=None, flatted=False):\n",
    "    if flatted:\n",
    "        for start in range(0, len(features), batch_size):\n",
    "            end = min(start + batch_size, len(features))\n",
    "            if val_size is not None:\n",
    "                feat_batch_train, \\\n",
    "                feat_batch_val,\\\n",
    "                lab_batch_train, \\\n",
    "                lab_batch_val = train_test_split(features[start:end,:], labels[start:end], test_size=val_size)\n",
    "            \n",
    "                yield feat_batch_train, feat_batch_val, lab_batch_train, lab_batch_val\n",
    "            else:\n",
    "                yield features[start:end,:], labels[start:end]\n",
    "    else:\n",
    "        for start in range(0, features.shape[3], batch_size):\n",
    "            end = min(start + batch_size, features.shape[3])\n",
    "            if val_size is not None:\n",
    "                feat_batch_train, \\\n",
    "                feat_batch_val,\\\n",
    "                lab_batch_train, \\\n",
    "                lab_batch_val = train_test_split(features[:,:,:,start:end],\n",
    "                    labels[start:end],\n",
    "                    test_size=val_size)\n",
    "            \n",
    "                yield feat_batch_train, feat_batch_val, lab_batch_train, lab_batch_val\n",
    "            else:\n",
    "                yield features[:,:,:,start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and understanding the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('../data/train_32x32.mat'):\n",
    "    if not os.path.isdir('../data'):\n",
    "        os.mkdir('../data')\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "            '../data/train_32x32.mat',\n",
    "            pbar.hook)\n",
    "\n",
    "svhn_data = loadmat('../data/train_32x32.mat')\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data info:  ['y', 'X', '__version__', '__header__', '__globals__']\n",
      "Features shape:  (32, 32, 3, 73257)\n",
      "Labels shape:  (73257, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Data info: ', svhn_data.keys())\n",
    "print('Features shape: ', svhn_data['X'].shape)\n",
    "print('Labels shape: ', svhn_data['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts: {1: 13861, 2: 10585, 3: 8497, 4: 7458, 5: 6882, 6: 5727, 7: 5595, 8: 5045, 9: 4659, 10: 4948}\n"
     ]
    }
   ],
   "source": [
    "features = svhn_data['X']\n",
    "labels = svhn_data['y']\n",
    "print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_sample_images(features, labels, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing and equalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_idx = equalization(labels, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  (32, 32, 3, 3000)\n",
      "Labels:  (3000, 1)\n",
      "Label Counts: {1: 300, 2: 300, 3: 300, 4: 300, 5: 300, 6: 300, 7: 300, 8: 300, 9: 300, 10: 300}\n"
     ]
    }
   ],
   "source": [
    "red_features = features[:,:,:,eq_idx]\n",
    "red_labels = labels[eq_idx]\n",
    "\n",
    "print('Features: ', red_features.shape)\n",
    "print('Labels: ', red_labels.shape)\n",
    "print('Label Counts: {}'.format(dict(zip(*np.unique(red_labels, return_counts=True)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing features with pre trained VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tensorflow_vgg'...\n",
      "remote: Counting objects: 109, done.\u001b[K\n",
      "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 109\u001b[K\n",
      "Receiving objects: 100% (109/109), 56.61 KiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (58/58), done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VGG16 Parameters: 553MB [05:24, 1.71MB/s]                               \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tensorflow_vvg import vgg16\n",
    "except:\n",
    "    !git clone https://github.com/machrisaa/tensorflow-vgg.git tensorflow_vgg\n",
    "    from tensorflow_vgg import vgg16\n",
    "\n",
    "with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "    urlretrieve(\n",
    "        'https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "        './tensorflow_vgg/vgg16.npy',\n",
    "        pbar.hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielcmresende/Documents/machine-learning/mlnd_capstone/project/tensorflow_vgg/vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 1s\n",
      "Processing batch #  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielcmresende/miniconda3/envs/mlcap/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch #  1\n",
      "Processing batch #  2\n",
      "Processing batch #  3\n",
      "Processing batch #  4\n",
      "Processing batch #  5\n",
      "Processing batch #  6\n",
      "Processing batch #  7\n",
      "Processing batch #  8\n",
      "Processing batch #  9\n",
      "Processing batch # 10\n",
      "Processing batch # 11\n",
      "Processing batch # 12\n",
      "Processing batch # 13\n",
      "Processing batch # 14\n",
      "Processing batch # 15\n",
      "Processing batch # 16\n",
      "Processing batch # 17\n",
      "Processing batch # 18\n",
      "Processing batch # 19\n",
      "Processing batch # 20\n",
      "Processing batch # 21\n",
      "Processing batch # 22\n",
      "Processing batch # 23\n",
      "Processing batch # 24\n",
      "Processing batch # 25\n",
      "Processing batch # 26\n",
      "Processing batch # 27\n",
      "Processing batch # 28\n",
      "Processing batch # 29\n",
      "Processing batch # 30\n",
      "Processing batch # 31\n",
      "Processing batch # 32\n",
      "Processing batch # 33\n",
      "Processing batch # 34\n",
      "Processing batch # 35\n",
      "Processing batch # 36\n",
      "Processing batch # 37\n",
      "Processing batch # 38\n",
      "Processing batch # 39\n",
      "Processing batch # 40\n",
      "Processing batch # 41\n",
      "Processing batch # 42\n",
      "Processing batch # 43\n",
      "Processing batch # 44\n",
      "Processing batch # 45\n",
      "Processing batch # 46\n",
      "Processing batch # 47\n",
      "Processing batch # 48\n",
      "Processing batch # 49\n",
      "Processing batch # 50\n",
      "Processing batch # 51\n",
      "Processing batch # 52\n",
      "Processing batch # 53\n",
      "Processing batch # 54\n",
      "Processing batch # 55\n",
      "Processing batch # 56\n",
      "Processing batch # 57\n",
      "Processing batch # 58\n",
      "Processing batch # 59\n",
      "Processing batch # 60\n",
      "Processing batch # 61\n",
      "Processing batch # 62\n",
      "Processing batch # 63\n",
      "Processing batch # 64\n",
      "Processing batch # 65\n",
      "Processing batch # 66\n",
      "Processing batch # 67\n",
      "Processing batch # 68\n",
      "Processing batch # 69\n",
      "Processing batch # 70\n",
      "Processing batch # 71\n",
      "Processing batch # 72\n",
      "Processing batch # 73\n",
      "Processing batch # 74\n",
      "Processing batch # 75\n",
      "Processing batch # 76\n",
      "Processing batch # 77\n",
      "Processing batch # 78\n",
      "Processing batch # 79\n",
      "Processing batch # 80\n",
      "Processing batch # 81\n",
      "Processing batch # 82\n",
      "Processing batch # 83\n",
      "Processing batch # 84\n",
      "Processing batch # 85\n",
      "Processing batch # 86\n",
      "Processing batch # 87\n",
      "Processing batch # 88\n",
      "Processing batch # 89\n",
      "Processing batch # 90\n",
      "Processing batch # 91\n",
      "Processing batch # 92\n",
      "Processing batch # 93\n",
      "Processing batch # 94\n",
      "Processing batch # 95\n",
      "Processing batch # 96\n",
      "Processing batch # 97\n",
      "Processing batch # 98\n",
      "Processing batch # 99\n",
      "Processing batch #100\n",
      "Processing batch #101\n",
      "Processing batch #102\n",
      "Processing batch #103\n",
      "Processing batch #104\n",
      "Processing batch #105\n",
      "Processing batch #106\n",
      "Processing batch #107\n",
      "Processing batch #108\n",
      "Processing batch #109\n",
      "Processing batch #110\n",
      "Processing batch #111\n",
      "Processing batch #112\n",
      "Processing batch #113\n",
      "Processing batch #114\n",
      "Processing batch #115\n",
      "Processing batch #116\n",
      "Processing batch #117\n",
      "Processing batch #118\n",
      "Processing batch #119\n",
      "Processing batch #120\n",
      "Processing batch #121\n",
      "Processing batch #122\n",
      "Processing batch #123\n",
      "Processing batch #124\n",
      "Processing batch #125\n",
      "Processing batch #126\n",
      "Processing batch #127\n",
      "Processing batch #128\n",
      "Processing batch #129\n",
      "Processing batch #130\n",
      "Processing batch #131\n",
      "Processing batch #132\n",
      "Processing batch #133\n",
      "Processing batch #134\n",
      "Processing batch #135\n",
      "Processing batch #136\n",
      "Processing batch #137\n",
      "Processing batch #138\n",
      "Processing batch #139\n",
      "Processing batch #140\n",
      "Processing batch #141\n",
      "Processing batch #142\n",
      "Processing batch #143\n",
      "Processing batch #144\n",
      "Processing batch #145\n",
      "Processing batch #146\n",
      "Processing batch #147\n",
      "Processing batch #148\n",
      "Processing batch #149\n",
      "Processing batch #150\n",
      "Processing batch #151\n",
      "Processing batch #152\n",
      "Processing batch #153\n",
      "Processing batch #154\n",
      "Processing batch #155\n",
      "Processing batch #156\n",
      "Processing batch #157\n",
      "Processing batch #158\n",
      "Processing batch #159\n",
      "Processing batch #160\n",
      "Processing batch #161\n",
      "Processing batch #162\n",
      "Processing batch #163\n",
      "Processing batch #164\n",
      "Processing batch #165\n",
      "Processing batch #166\n",
      "Processing batch #167\n",
      "Processing batch #168\n",
      "Processing batch #169\n",
      "Processing batch #170\n",
      "Processing batch #171\n",
      "Processing batch #172\n",
      "Processing batch #173\n",
      "Processing batch #174\n",
      "Processing batch #175\n",
      "Processing batch #176\n",
      "Processing batch #177\n",
      "Processing batch #178\n",
      "Processing batch #179\n",
      "Processing batch #180\n",
      "Processing batch #181\n",
      "Processing batch #182\n",
      "Processing batch #183\n",
      "Processing batch #184\n",
      "Processing batch #185\n",
      "Processing batch #186\n",
      "Processing batch #187\n",
      "Processing batch #188\n",
      "Processing batch #189\n",
      "Processing batch #190\n",
      "Processing batch #191\n",
      "Processing batch #192\n",
      "Processing batch #193\n",
      "Processing batch #194\n",
      "Processing batch #195\n",
      "Processing batch #196\n",
      "Processing batch #197\n",
      "Processing batch #198\n",
      "Processing batch #199\n",
      "Processing batch #200\n",
      "Processing batch #201\n",
      "Processing batch #202\n",
      "Processing batch #203\n",
      "Processing batch #204\n",
      "Processing batch #205\n",
      "Processing batch #206\n",
      "Processing batch #207\n",
      "Processing batch #208\n",
      "Processing batch #209\n",
      "Processing batch #210\n",
      "Processing batch #211\n",
      "Processing batch #212\n",
      "Processing batch #213\n",
      "Processing batch #214\n",
      "Processing batch #215\n",
      "Processing batch #216\n",
      "Processing batch #217\n",
      "Processing batch #218\n",
      "Processing batch #219\n",
      "Processing batch #220\n",
      "Processing batch #221\n",
      "Processing batch #222\n",
      "Processing batch #223\n",
      "Processing batch #224\n",
      "Processing batch #225\n",
      "Processing batch #226\n",
      "Processing batch #227\n",
      "Processing batch #228\n",
      "Processing batch #229\n",
      "Processing batch #230\n",
      "Processing batch #231\n",
      "Processing batch #232\n",
      "Processing batch #233\n",
      "Processing batch #234\n",
      "Processing batch #235\n",
      "Processing batch #236\n",
      "Processing batch #237\n",
      "Processing batch #238\n",
      "Processing batch #239\n",
      "Processing batch #240\n",
      "Processing batch #241\n",
      "Processing batch #242\n",
      "Processing batch #243\n",
      "Processing batch #244\n",
      "Processing batch #245\n",
      "Processing batch #246\n",
      "Processing batch #247\n",
      "Processing batch #248\n",
      "Processing batch #249\n",
      "Processing batch #250\n",
      "Processing batch #251\n",
      "Processing batch #252\n",
      "Processing batch #253\n",
      "Processing batch #254\n",
      "Processing batch #255\n",
      "Processing batch #256\n",
      "Processing batch #257\n",
      "Processing batch #258\n",
      "Processing batch #259\n",
      "Processing batch #260\n",
      "Processing batch #261\n",
      "Processing batch #262\n",
      "Processing batch #263\n",
      "Processing batch #264\n",
      "Processing batch #265\n",
      "Processing batch #266\n",
      "Processing batch #267\n",
      "Processing batch #268\n",
      "Processing batch #269\n",
      "Processing batch #270\n",
      "Processing batch #271\n",
      "Processing batch #272\n",
      "Processing batch #273\n",
      "Processing batch #274\n",
      "Processing batch #275\n",
      "Processing batch #276\n",
      "Processing batch #277\n",
      "Processing batch #278\n",
      "Processing batch #279\n",
      "Processing batch #280\n",
      "Processing batch #281\n",
      "Processing batch #282\n",
      "Processing batch #283\n",
      "Processing batch #284\n",
      "Processing batch #285\n",
      "Processing batch #286\n",
      "Processing batch #287\n",
      "Processing batch #288\n",
      "Processing batch #289\n",
      "Processing batch #290\n",
      "Processing batch #291\n",
      "Processing batch #292\n",
      "Processing batch #293\n",
      "Processing batch #294\n",
      "Processing batch #295\n",
      "Processing batch #296\n",
      "Processing batch #297\n",
      "Processing batch #298\n",
      "Processing batch #299\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./code_data.pkl'):\n",
    "    batch_size = 10\n",
    "    iteration = 0\n",
    "    codes = None\n",
    "    with tf.Session() as sess:\n",
    "        vgg = vgg16.Vgg16()\n",
    "        images = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        with tf.name_scope('content_vgg'):\n",
    "            vgg.build(images)\n",
    "    \n",
    "        for batch, _ in batch_creator(red_features, red_labels, batch_size):\n",
    "            print('Processing batch #{:3d}'.format(iteration))\n",
    "            resized_images = resize_images(batch)\n",
    "            feed_dict = {images: resized_images}\n",
    "            codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "            if codes is None:\n",
    "                codes = codes_batch\n",
    "            else:\n",
    "                codes = np.concatenate((codes, codes_batch))\n",
    "            iteration += 1\n",
    "        \n",
    "    with open('./code_data.pkl', 'wb') as f:\n",
    "        pickle.dump(codes, f)\n",
    "else:\n",
    "    with open('./code_data.pkl', 'rb') as f:\n",
    "        codes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(codes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot enconding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10)\n"
     ]
    }
   ],
   "source": [
    "one_hot_labels = one_hot_encode(red_labels)\n",
    "print(one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data for final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(ss.split(codes, one_hot_labels))\n",
    "\n",
    "half_val_len = int(len(val_idx) / 2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], one_hot_labels[train_idx]\n",
    "val_x, val_y = codes[val_idx], one_hot_labels[val_idx]\n",
    "test_x, test_y = codes[test_idx], one_hot_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (2400, 4096) (2400, 10)\n",
      "Validation shapes (x, y): (300, 4096) (300, 10)\n",
      "Test shapes (x, y): (300, 4096) (300, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, one_hot_labels.shape[1]])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "relu7 = tf.layers.dense(inputs_, codes.shape[1], activation=tf.nn.relu)\n",
    "drop7 = tf.nn.dropout(relu7, keep_prob)\n",
    "\n",
    "logits = tf.layers.dense(drop7, one_hot_labels.shape[1])\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/9 Iteration: 0 Training loss: 4.74416\n",
      "***\n",
      "Epoch: 0/9 Iteration: 0 Validation Acc: 0.1000\n",
      "***\n",
      "Epoch: 0/9 Iteration: 1 Training loss: 42.24430\n",
      "Epoch: 0/9 Iteration: 2 Training loss: 14.49187\n",
      "Epoch: 0/9 Iteration: 3 Training loss: 23.70087\n",
      "Epoch: 0/9 Iteration: 4 Training loss: 39.47435\n",
      "Epoch: 0/9 Iteration: 5 Training loss: 31.80557\n",
      "Epoch: 0/9 Iteration: 6 Training loss: 33.72009\n",
      "Epoch: 0/9 Iteration: 7 Training loss: 16.81004\n",
      "Epoch: 0/9 Iteration: 8 Training loss: 33.75824\n",
      "Epoch: 0/9 Iteration: 9 Training loss: 35.99119\n",
      "Epoch: 0/9 Iteration: 10 Training loss: 22.00358\n",
      "Epoch: 0/9 Iteration: 11 Training loss: 26.90046\n",
      "Epoch: 0/9 Iteration: 12 Training loss: 35.94595\n",
      "Epoch: 0/9 Iteration: 13 Training loss: 19.28878\n",
      "Epoch: 0/9 Iteration: 14 Training loss: 27.83956\n",
      "Epoch: 0/9 Iteration: 15 Training loss: 19.00770\n",
      "Epoch: 0/9 Iteration: 16 Training loss: 32.42262\n",
      "Epoch: 0/9 Iteration: 17 Training loss: 35.67191\n",
      "Epoch: 0/9 Iteration: 18 Training loss: 27.82884\n",
      "Epoch: 0/9 Iteration: 19 Training loss: 21.55810\n",
      "Epoch: 0/9 Iteration: 20 Training loss: 26.80662\n",
      "Epoch: 0/9 Iteration: 21 Training loss: 14.16065\n",
      "Epoch: 0/9 Iteration: 22 Training loss: 23.23559\n",
      "Epoch: 0/9 Iteration: 23 Training loss: 21.17776\n",
      "Epoch: 0/9 Iteration: 24 Training loss: 20.11246\n",
      "Epoch: 0/9 Iteration: 25 Training loss: 18.15967\n",
      "Epoch: 0/9 Iteration: 26 Training loss: 22.59392\n",
      "Epoch: 0/9 Iteration: 27 Training loss: 21.22843\n",
      "Epoch: 0/9 Iteration: 28 Training loss: 25.27028\n",
      "Epoch: 0/9 Iteration: 29 Training loss: 24.06973\n",
      "Epoch: 0/9 Iteration: 30 Training loss: 19.11889\n",
      "Epoch: 0/9 Iteration: 31 Training loss: 16.32491\n",
      "Epoch: 0/9 Iteration: 32 Training loss: 11.58007\n",
      "Epoch: 0/9 Iteration: 33 Training loss: 13.92208\n",
      "Epoch: 0/9 Iteration: 34 Training loss: 15.88426\n",
      "Epoch: 0/9 Iteration: 35 Training loss: 9.63645\n",
      "Epoch: 0/9 Iteration: 36 Training loss: 17.31876\n",
      "Epoch: 0/9 Iteration: 37 Training loss: 13.30143\n",
      "Epoch: 0/9 Iteration: 38 Training loss: 6.67783\n",
      "Epoch: 0/9 Iteration: 39 Training loss: 14.97939\n",
      "Epoch: 0/9 Iteration: 40 Training loss: 11.94462\n",
      "Epoch: 0/9 Iteration: 41 Training loss: 9.93432\n",
      "Epoch: 0/9 Iteration: 42 Training loss: 15.93459\n",
      "Epoch: 0/9 Iteration: 43 Training loss: 15.10430\n",
      "Epoch: 0/9 Iteration: 44 Training loss: 17.27211\n",
      "Epoch: 0/9 Iteration: 45 Training loss: 23.23316\n",
      "Epoch: 0/9 Iteration: 46 Training loss: 10.08765\n",
      "Epoch: 0/9 Iteration: 47 Training loss: 11.72840\n",
      "Epoch: 0/9 Iteration: 48 Training loss: 6.38617\n",
      "Epoch: 0/9 Iteration: 49 Training loss: 20.84638\n",
      "Epoch: 0/9 Iteration: 50 Training loss: 16.54684\n",
      "***\n",
      "Epoch: 0/9 Iteration: 50 Validation Acc: 0.3333\n",
      "***\n",
      "Epoch: 0/9 Iteration: 51 Training loss: 16.24842\n",
      "Epoch: 0/9 Iteration: 52 Training loss: 16.42049\n",
      "Epoch: 0/9 Iteration: 53 Training loss: 9.73565\n",
      "Epoch: 0/9 Iteration: 54 Training loss: 9.42233\n",
      "Epoch: 0/9 Iteration: 55 Training loss: 12.98990\n",
      "Epoch: 0/9 Iteration: 56 Training loss: 13.32788\n",
      "Epoch: 0/9 Iteration: 57 Training loss: 7.01312\n",
      "Epoch: 0/9 Iteration: 58 Training loss: 14.54123\n",
      "Epoch: 0/9 Iteration: 59 Training loss: 10.61286\n",
      "Epoch: 0/9 Iteration: 60 Training loss: 17.22585\n",
      "Epoch: 0/9 Iteration: 61 Training loss: 10.49265\n",
      "Epoch: 0/9 Iteration: 62 Training loss: 5.28033\n",
      "Epoch: 0/9 Iteration: 63 Training loss: 12.47528\n",
      "Epoch: 0/9 Iteration: 64 Training loss: 6.41958\n",
      "Epoch: 0/9 Iteration: 65 Training loss: 12.17875\n",
      "Epoch: 0/9 Iteration: 66 Training loss: 5.98363\n",
      "Epoch: 0/9 Iteration: 67 Training loss: 8.78729\n",
      "Epoch: 0/9 Iteration: 68 Training loss: 7.10551\n",
      "Epoch: 0/9 Iteration: 69 Training loss: 11.32142\n",
      "Epoch: 0/9 Iteration: 70 Training loss: 11.71792\n",
      "Epoch: 0/9 Iteration: 71 Training loss: 14.03915\n",
      "Epoch: 0/9 Iteration: 72 Training loss: 7.41329\n",
      "Epoch: 0/9 Iteration: 73 Training loss: 9.74471\n",
      "Epoch: 0/9 Iteration: 74 Training loss: 8.33377\n",
      "Epoch: 0/9 Iteration: 75 Training loss: 5.22927\n",
      "Epoch: 0/9 Iteration: 76 Training loss: 11.05501\n",
      "Epoch: 0/9 Iteration: 77 Training loss: 5.86363\n",
      "Epoch: 0/9 Iteration: 78 Training loss: 9.10659\n",
      "Epoch: 0/9 Iteration: 79 Training loss: 3.96211\n",
      "Epoch: 0/9 Iteration: 80 Training loss: 7.21956\n",
      "Epoch: 0/9 Iteration: 81 Training loss: 7.32179\n",
      "Epoch: 0/9 Iteration: 82 Training loss: 10.37511\n",
      "Epoch: 0/9 Iteration: 83 Training loss: 5.48296\n",
      "Epoch: 0/9 Iteration: 84 Training loss: 6.02769\n",
      "Epoch: 0/9 Iteration: 85 Training loss: 4.51424\n",
      "Epoch: 0/9 Iteration: 86 Training loss: 4.47360\n",
      "Epoch: 0/9 Iteration: 87 Training loss: 3.68216\n",
      "Epoch: 0/9 Iteration: 88 Training loss: 5.43920\n",
      "Epoch: 0/9 Iteration: 89 Training loss: 4.81736\n",
      "Epoch: 0/9 Iteration: 90 Training loss: 5.97908\n",
      "Epoch: 0/9 Iteration: 91 Training loss: 3.20261\n",
      "Epoch: 0/9 Iteration: 92 Training loss: 5.80760\n",
      "Epoch: 0/9 Iteration: 93 Training loss: 5.03629\n",
      "Epoch: 0/9 Iteration: 94 Training loss: 7.27614\n",
      "Epoch: 0/9 Iteration: 95 Training loss: 4.42210\n",
      "Epoch: 0/9 Iteration: 96 Training loss: 6.62881\n",
      "Epoch: 0/9 Iteration: 97 Training loss: 7.00279\n",
      "Epoch: 0/9 Iteration: 98 Training loss: 3.66233\n",
      "Epoch: 0/9 Iteration: 99 Training loss: 3.47378\n",
      "Epoch: 0/9 Iteration: 100 Training loss: 5.99488\n",
      "***\n",
      "Epoch: 0/9 Iteration: 100 Validation Acc: 0.4233\n",
      "***\n",
      "Epoch: 0/9 Iteration: 101 Training loss: 3.11696\n",
      "Epoch: 0/9 Iteration: 102 Training loss: 4.42224\n",
      "Epoch: 0/9 Iteration: 103 Training loss: 3.98584\n",
      "Epoch: 0/9 Iteration: 104 Training loss: 3.67247\n",
      "Epoch: 0/9 Iteration: 105 Training loss: 5.11988\n",
      "Epoch: 0/9 Iteration: 106 Training loss: 2.83347\n",
      "Epoch: 0/9 Iteration: 107 Training loss: 4.89201\n",
      "Epoch: 0/9 Iteration: 108 Training loss: 2.70475\n",
      "Epoch: 0/9 Iteration: 109 Training loss: 4.22464\n",
      "Epoch: 0/9 Iteration: 110 Training loss: 3.24604\n",
      "Epoch: 0/9 Iteration: 111 Training loss: 3.56334\n",
      "Epoch: 0/9 Iteration: 112 Training loss: 4.15340\n",
      "Epoch: 0/9 Iteration: 113 Training loss: 4.20375\n",
      "Epoch: 0/9 Iteration: 114 Training loss: 1.22905\n",
      "Epoch: 0/9 Iteration: 115 Training loss: 3.19528\n",
      "Epoch: 0/9 Iteration: 116 Training loss: 4.37394\n",
      "Epoch: 0/9 Iteration: 117 Training loss: 1.50710\n",
      "Epoch: 0/9 Iteration: 118 Training loss: 3.28377\n",
      "Epoch: 0/9 Iteration: 119 Training loss: 4.93248\n",
      "Epoch: 0/9 Iteration: 120 Training loss: 3.13016\n",
      "Epoch: 0/9 Iteration: 121 Training loss: 2.02665\n",
      "Epoch: 0/9 Iteration: 122 Training loss: 2.77335\n",
      "Epoch: 0/9 Iteration: 123 Training loss: 1.86425\n",
      "Epoch: 0/9 Iteration: 124 Training loss: 2.49114\n",
      "Epoch: 0/9 Iteration: 125 Training loss: 3.34050\n",
      "Epoch: 0/9 Iteration: 126 Training loss: 3.45214\n",
      "Epoch: 0/9 Iteration: 127 Training loss: 2.28033\n",
      "Epoch: 0/9 Iteration: 128 Training loss: 2.55826\n",
      "Epoch: 0/9 Iteration: 129 Training loss: 3.38369\n",
      "Epoch: 0/9 Iteration: 130 Training loss: 2.88770\n",
      "Epoch: 0/9 Iteration: 131 Training loss: 4.80952\n",
      "Epoch: 0/9 Iteration: 132 Training loss: 2.47606\n",
      "Epoch: 0/9 Iteration: 133 Training loss: 1.02651\n",
      "Epoch: 0/9 Iteration: 134 Training loss: 1.98367\n",
      "Epoch: 0/9 Iteration: 135 Training loss: 2.41289\n",
      "Epoch: 0/9 Iteration: 136 Training loss: 2.47086\n",
      "Epoch: 0/9 Iteration: 137 Training loss: 2.13609\n",
      "Epoch: 0/9 Iteration: 138 Training loss: 1.36533\n",
      "Epoch: 0/9 Iteration: 139 Training loss: 1.38063\n",
      "Epoch: 0/9 Iteration: 140 Training loss: 3.85547\n",
      "Epoch: 0/9 Iteration: 141 Training loss: 3.25671\n",
      "Epoch: 0/9 Iteration: 142 Training loss: 2.28187\n",
      "Epoch: 0/9 Iteration: 143 Training loss: 1.42563\n",
      "Epoch: 0/9 Iteration: 144 Training loss: 1.56262\n",
      "Epoch: 0/9 Iteration: 145 Training loss: 3.19686\n",
      "Epoch: 0/9 Iteration: 146 Training loss: 1.92895\n",
      "Epoch: 0/9 Iteration: 147 Training loss: 1.98393\n",
      "Epoch: 0/9 Iteration: 148 Training loss: 3.56273\n",
      "Epoch: 0/9 Iteration: 149 Training loss: 1.17097\n",
      "Epoch: 0/9 Iteration: 150 Training loss: 1.86716\n",
      "***\n",
      "Epoch: 0/9 Iteration: 150 Validation Acc: 0.4967\n",
      "***\n",
      "Epoch: 0/9 Iteration: 151 Training loss: 1.08808\n",
      "Epoch: 0/9 Iteration: 152 Training loss: 1.26599\n",
      "Epoch: 0/9 Iteration: 153 Training loss: 2.17527\n",
      "Epoch: 0/9 Iteration: 154 Training loss: 1.47421\n",
      "Epoch: 0/9 Iteration: 155 Training loss: 1.41743\n",
      "Epoch: 0/9 Iteration: 156 Training loss: 1.50186\n",
      "Epoch: 0/9 Iteration: 157 Training loss: 2.35610\n",
      "Epoch: 0/9 Iteration: 158 Training loss: 1.40365\n",
      "Epoch: 0/9 Iteration: 159 Training loss: 1.16667\n",
      "Epoch: 0/9 Iteration: 160 Training loss: 2.04308\n",
      "Epoch: 0/9 Iteration: 161 Training loss: 1.30960\n",
      "Epoch: 0/9 Iteration: 162 Training loss: 1.82885\n",
      "Epoch: 0/9 Iteration: 163 Training loss: 2.81287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/9 Iteration: 164 Training loss: 2.25657\n",
      "Epoch: 0/9 Iteration: 165 Training loss: 1.70601\n",
      "Epoch: 0/9 Iteration: 166 Training loss: 2.15330\n",
      "Epoch: 0/9 Iteration: 167 Training loss: 1.42998\n",
      "Epoch: 0/9 Iteration: 168 Training loss: 1.57300\n",
      "Epoch: 0/9 Iteration: 169 Training loss: 1.67423\n",
      "Epoch: 0/9 Iteration: 170 Training loss: 0.68469\n",
      "Epoch: 0/9 Iteration: 171 Training loss: 1.43719\n",
      "Epoch: 0/9 Iteration: 172 Training loss: 1.89705\n",
      "Epoch: 0/9 Iteration: 173 Training loss: 1.89079\n",
      "Epoch: 0/9 Iteration: 174 Training loss: 1.92152\n",
      "Epoch: 0/9 Iteration: 175 Training loss: 1.29521\n",
      "Epoch: 0/9 Iteration: 176 Training loss: 1.89476\n",
      "Epoch: 0/9 Iteration: 177 Training loss: 2.12278\n",
      "Epoch: 0/9 Iteration: 178 Training loss: 1.32889\n",
      "Epoch: 0/9 Iteration: 179 Training loss: 1.75346\n",
      "Epoch: 0/9 Iteration: 180 Training loss: 0.74003\n",
      "Epoch: 0/9 Iteration: 181 Training loss: 1.35781\n",
      "Epoch: 0/9 Iteration: 182 Training loss: 0.94869\n",
      "Epoch: 0/9 Iteration: 183 Training loss: 1.49411\n",
      "Epoch: 0/9 Iteration: 184 Training loss: 1.12590\n",
      "Epoch: 0/9 Iteration: 185 Training loss: 1.65077\n",
      "Epoch: 0/9 Iteration: 186 Training loss: 0.89527\n",
      "Epoch: 0/9 Iteration: 187 Training loss: 1.59134\n",
      "Epoch: 0/9 Iteration: 188 Training loss: 1.52859\n",
      "Epoch: 0/9 Iteration: 189 Training loss: 1.81228\n",
      "Epoch: 0/9 Iteration: 190 Training loss: 0.96319\n",
      "Epoch: 0/9 Iteration: 191 Training loss: 1.07355\n",
      "Epoch: 0/9 Iteration: 192 Training loss: 1.32154\n",
      "Epoch: 0/9 Iteration: 193 Training loss: 1.64062\n",
      "Epoch: 0/9 Iteration: 194 Training loss: 1.62534\n",
      "Epoch: 0/9 Iteration: 195 Training loss: 2.33620\n",
      "Epoch: 0/9 Iteration: 196 Training loss: 1.50647\n",
      "Epoch: 0/9 Iteration: 197 Training loss: 1.67654\n",
      "Epoch: 0/9 Iteration: 198 Training loss: 1.20759\n",
      "Epoch: 0/9 Iteration: 199 Training loss: 0.99413\n",
      "Epoch: 0/9 Iteration: 200 Training loss: 2.65832\n",
      "***\n",
      "Epoch: 0/9 Iteration: 200 Validation Acc: 0.5567\n",
      "***\n",
      "Epoch: 0/9 Iteration: 201 Training loss: 1.96759\n",
      "Epoch: 0/9 Iteration: 202 Training loss: 2.12004\n",
      "Epoch: 0/9 Iteration: 203 Training loss: 1.57636\n",
      "Epoch: 0/9 Iteration: 204 Training loss: 1.98638\n",
      "Epoch: 0/9 Iteration: 205 Training loss: 2.41275\n",
      "Epoch: 0/9 Iteration: 206 Training loss: 1.63968\n",
      "Epoch: 0/9 Iteration: 207 Training loss: 2.45014\n",
      "Epoch: 0/9 Iteration: 208 Training loss: 2.00928\n",
      "Epoch: 0/9 Iteration: 209 Training loss: 0.75769\n",
      "Epoch: 0/9 Iteration: 210 Training loss: 2.29267\n",
      "Epoch: 0/9 Iteration: 211 Training loss: 1.33769\n",
      "Epoch: 0/9 Iteration: 212 Training loss: 2.08118\n",
      "Epoch: 0/9 Iteration: 213 Training loss: 1.60005\n",
      "Epoch: 0/9 Iteration: 214 Training loss: 2.20217\n",
      "Epoch: 0/9 Iteration: 215 Training loss: 1.93438\n",
      "Epoch: 0/9 Iteration: 216 Training loss: 2.40295\n",
      "Epoch: 0/9 Iteration: 217 Training loss: 2.64014\n",
      "Epoch: 0/9 Iteration: 218 Training loss: 1.33516\n",
      "Epoch: 0/9 Iteration: 219 Training loss: 1.97406\n",
      "Epoch: 0/9 Iteration: 220 Training loss: 1.20245\n",
      "Epoch: 0/9 Iteration: 221 Training loss: 1.80283\n",
      "Epoch: 0/9 Iteration: 222 Training loss: 1.54662\n",
      "Epoch: 0/9 Iteration: 223 Training loss: 1.80663\n",
      "Epoch: 0/9 Iteration: 224 Training loss: 1.45268\n",
      "Epoch: 0/9 Iteration: 225 Training loss: 1.29140\n",
      "Epoch: 0/9 Iteration: 226 Training loss: 1.68415\n",
      "Epoch: 0/9 Iteration: 227 Training loss: 1.36556\n",
      "Epoch: 0/9 Iteration: 228 Training loss: 1.84748\n",
      "Epoch: 0/9 Iteration: 229 Training loss: 1.20437\n",
      "Epoch: 0/9 Iteration: 230 Training loss: 1.69751\n",
      "Epoch: 0/9 Iteration: 231 Training loss: 1.70070\n",
      "Epoch: 0/9 Iteration: 232 Training loss: 1.96471\n",
      "Epoch: 0/9 Iteration: 233 Training loss: 1.59351\n",
      "Epoch: 0/9 Iteration: 234 Training loss: 1.39981\n",
      "Epoch: 0/9 Iteration: 235 Training loss: 1.85566\n",
      "Epoch: 0/9 Iteration: 236 Training loss: 1.84962\n",
      "Epoch: 0/9 Iteration: 237 Training loss: 1.27409\n",
      "Epoch: 0/9 Iteration: 238 Training loss: 2.62871\n",
      "Epoch: 0/9 Iteration: 239 Training loss: 1.79378\n",
      "Epoch: 1/9 Iteration: 240 Training loss: 1.41737\n",
      "Epoch: 1/9 Iteration: 241 Training loss: 1.13844\n",
      "Epoch: 1/9 Iteration: 242 Training loss: 1.37107\n",
      "Epoch: 1/9 Iteration: 243 Training loss: 0.80674\n",
      "Epoch: 1/9 Iteration: 244 Training loss: 1.39000\n",
      "Epoch: 1/9 Iteration: 245 Training loss: 2.01417\n",
      "Epoch: 1/9 Iteration: 246 Training loss: 1.21669\n",
      "Epoch: 1/9 Iteration: 247 Training loss: 1.33662\n",
      "Epoch: 1/9 Iteration: 248 Training loss: 1.22200\n",
      "Epoch: 1/9 Iteration: 249 Training loss: 1.71940\n",
      "Epoch: 1/9 Iteration: 250 Training loss: 1.64080\n",
      "***\n",
      "Epoch: 1/9 Iteration: 250 Validation Acc: 0.5533\n",
      "***\n",
      "Epoch: 1/9 Iteration: 251 Training loss: 1.05423\n",
      "Epoch: 1/9 Iteration: 252 Training loss: 1.61860\n",
      "Epoch: 1/9 Iteration: 253 Training loss: 1.41997\n",
      "Epoch: 1/9 Iteration: 254 Training loss: 0.98163\n",
      "Epoch: 1/9 Iteration: 255 Training loss: 1.57316\n",
      "Epoch: 1/9 Iteration: 256 Training loss: 1.46736\n",
      "Epoch: 1/9 Iteration: 257 Training loss: 1.31138\n",
      "Epoch: 1/9 Iteration: 258 Training loss: 1.51053\n",
      "Epoch: 1/9 Iteration: 259 Training loss: 1.64935\n",
      "Epoch: 1/9 Iteration: 260 Training loss: 2.23474\n",
      "Epoch: 1/9 Iteration: 261 Training loss: 1.43714\n",
      "Epoch: 1/9 Iteration: 262 Training loss: 0.78551\n",
      "Epoch: 1/9 Iteration: 263 Training loss: 1.84529\n",
      "Epoch: 1/9 Iteration: 264 Training loss: 1.16587\n",
      "Epoch: 1/9 Iteration: 265 Training loss: 1.55873\n",
      "Epoch: 1/9 Iteration: 266 Training loss: 1.65755\n",
      "Epoch: 1/9 Iteration: 267 Training loss: 1.14398\n",
      "Epoch: 1/9 Iteration: 268 Training loss: 2.14625\n",
      "Epoch: 1/9 Iteration: 269 Training loss: 2.05452\n",
      "Epoch: 1/9 Iteration: 270 Training loss: 1.18741\n",
      "Epoch: 1/9 Iteration: 271 Training loss: 1.67495\n",
      "Epoch: 1/9 Iteration: 272 Training loss: 1.79737\n",
      "Epoch: 1/9 Iteration: 273 Training loss: 1.06679\n",
      "Epoch: 1/9 Iteration: 274 Training loss: 2.10676\n",
      "Epoch: 1/9 Iteration: 275 Training loss: 1.29417\n",
      "Epoch: 1/9 Iteration: 276 Training loss: 1.48946\n",
      "Epoch: 1/9 Iteration: 277 Training loss: 1.70468\n",
      "Epoch: 1/9 Iteration: 278 Training loss: 0.94732\n",
      "Epoch: 1/9 Iteration: 279 Training loss: 1.17938\n",
      "Epoch: 1/9 Iteration: 280 Training loss: 1.45914\n",
      "Epoch: 1/9 Iteration: 281 Training loss: 1.58398\n",
      "Epoch: 1/9 Iteration: 282 Training loss: 1.59895\n",
      "Epoch: 1/9 Iteration: 283 Training loss: 2.54184\n",
      "Epoch: 1/9 Iteration: 284 Training loss: 1.44527\n",
      "Epoch: 1/9 Iteration: 285 Training loss: 0.77270\n",
      "Epoch: 1/9 Iteration: 286 Training loss: 1.14666\n",
      "Epoch: 1/9 Iteration: 287 Training loss: 1.99027\n",
      "Epoch: 1/9 Iteration: 288 Training loss: 1.28834\n",
      "Epoch: 1/9 Iteration: 289 Training loss: 1.50786\n",
      "Epoch: 1/9 Iteration: 290 Training loss: 1.53687\n",
      "Epoch: 1/9 Iteration: 291 Training loss: 1.58091\n",
      "Epoch: 1/9 Iteration: 292 Training loss: 1.15926\n",
      "Epoch: 1/9 Iteration: 293 Training loss: 1.12698\n",
      "Epoch: 1/9 Iteration: 294 Training loss: 0.88303\n",
      "Epoch: 1/9 Iteration: 295 Training loss: 1.12326\n",
      "Epoch: 1/9 Iteration: 296 Training loss: 1.79547\n",
      "Epoch: 1/9 Iteration: 297 Training loss: 1.30224\n",
      "Epoch: 1/9 Iteration: 298 Training loss: 1.37450\n",
      "Epoch: 1/9 Iteration: 299 Training loss: 0.96507\n",
      "Epoch: 1/9 Iteration: 300 Training loss: 1.57646\n",
      "***\n",
      "Epoch: 1/9 Iteration: 300 Validation Acc: 0.6000\n",
      "***\n",
      "Epoch: 1/9 Iteration: 301 Training loss: 1.03529\n",
      "Epoch: 1/9 Iteration: 302 Training loss: 0.96754\n",
      "Epoch: 1/9 Iteration: 303 Training loss: 1.41612\n",
      "Epoch: 1/9 Iteration: 304 Training loss: 0.94350\n",
      "Epoch: 1/9 Iteration: 305 Training loss: 1.57682\n",
      "Epoch: 1/9 Iteration: 306 Training loss: 1.33441\n",
      "Epoch: 1/9 Iteration: 307 Training loss: 1.11539\n",
      "Epoch: 1/9 Iteration: 308 Training loss: 1.00454\n",
      "Epoch: 1/9 Iteration: 309 Training loss: 1.00915\n",
      "Epoch: 1/9 Iteration: 310 Training loss: 2.38119\n",
      "Epoch: 1/9 Iteration: 311 Training loss: 1.48164\n",
      "Epoch: 1/9 Iteration: 312 Training loss: 1.54941\n",
      "Epoch: 1/9 Iteration: 313 Training loss: 1.27489\n",
      "Epoch: 1/9 Iteration: 314 Training loss: 1.00529\n",
      "Epoch: 1/9 Iteration: 315 Training loss: 1.69432\n",
      "Epoch: 1/9 Iteration: 316 Training loss: 1.92431\n",
      "Epoch: 1/9 Iteration: 317 Training loss: 1.18286\n",
      "Epoch: 1/9 Iteration: 318 Training loss: 2.13065\n",
      "Epoch: 1/9 Iteration: 319 Training loss: 1.84668\n",
      "Epoch: 1/9 Iteration: 320 Training loss: 1.56628\n",
      "Epoch: 1/9 Iteration: 321 Training loss: 1.97751\n",
      "Epoch: 1/9 Iteration: 322 Training loss: 1.65621\n",
      "Epoch: 1/9 Iteration: 323 Training loss: 1.80604\n",
      "Epoch: 1/9 Iteration: 324 Training loss: 1.97752\n",
      "Epoch: 1/9 Iteration: 325 Training loss: 1.13704\n",
      "Epoch: 1/9 Iteration: 326 Training loss: 1.38073\n",
      "Epoch: 1/9 Iteration: 327 Training loss: 0.96688\n",
      "Epoch: 1/9 Iteration: 328 Training loss: 1.16375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9 Iteration: 329 Training loss: 2.10303\n",
      "Epoch: 1/9 Iteration: 330 Training loss: 2.61105\n",
      "Epoch: 1/9 Iteration: 331 Training loss: 1.54667\n",
      "Epoch: 1/9 Iteration: 332 Training loss: 1.20160\n",
      "Epoch: 1/9 Iteration: 333 Training loss: 1.27831\n",
      "Epoch: 1/9 Iteration: 334 Training loss: 1.66397\n",
      "Epoch: 1/9 Iteration: 335 Training loss: 1.25366\n",
      "Epoch: 1/9 Iteration: 336 Training loss: 2.01529\n",
      "Epoch: 1/9 Iteration: 337 Training loss: 1.49857\n",
      "Epoch: 1/9 Iteration: 338 Training loss: 1.34679\n",
      "Epoch: 1/9 Iteration: 339 Training loss: 0.68515\n",
      "Epoch: 1/9 Iteration: 340 Training loss: 1.79984\n",
      "Epoch: 1/9 Iteration: 341 Training loss: 1.69971\n",
      "Epoch: 1/9 Iteration: 342 Training loss: 1.38147\n",
      "Epoch: 1/9 Iteration: 343 Training loss: 1.16451\n",
      "Epoch: 1/9 Iteration: 344 Training loss: 1.15842\n",
      "Epoch: 1/9 Iteration: 345 Training loss: 1.25213\n",
      "Epoch: 1/9 Iteration: 346 Training loss: 1.01179\n",
      "Epoch: 1/9 Iteration: 347 Training loss: 1.36956\n",
      "Epoch: 1/9 Iteration: 348 Training loss: 0.99499\n",
      "Epoch: 1/9 Iteration: 349 Training loss: 1.52298\n",
      "Epoch: 1/9 Iteration: 350 Training loss: 1.26724\n",
      "***\n",
      "Epoch: 1/9 Iteration: 350 Validation Acc: 0.6233\n",
      "***\n",
      "Epoch: 1/9 Iteration: 351 Training loss: 1.49462\n",
      "Epoch: 1/9 Iteration: 352 Training loss: 1.73234\n",
      "Epoch: 1/9 Iteration: 353 Training loss: 1.02516\n",
      "Epoch: 1/9 Iteration: 354 Training loss: 1.33989\n",
      "Epoch: 1/9 Iteration: 355 Training loss: 1.18395\n",
      "Epoch: 1/9 Iteration: 356 Training loss: 0.81497\n",
      "Epoch: 1/9 Iteration: 357 Training loss: 1.47464\n",
      "Epoch: 1/9 Iteration: 358 Training loss: 2.10270\n",
      "Epoch: 1/9 Iteration: 359 Training loss: 1.46612\n",
      "Epoch: 1/9 Iteration: 360 Training loss: 0.86207\n",
      "Epoch: 1/9 Iteration: 361 Training loss: 1.44669\n",
      "Epoch: 1/9 Iteration: 362 Training loss: 0.92330\n",
      "Epoch: 1/9 Iteration: 363 Training loss: 1.22906\n",
      "Epoch: 1/9 Iteration: 364 Training loss: 1.54750\n",
      "Epoch: 1/9 Iteration: 365 Training loss: 2.02411\n",
      "Epoch: 1/9 Iteration: 366 Training loss: 1.34098\n",
      "Epoch: 1/9 Iteration: 367 Training loss: 1.50724\n",
      "Epoch: 1/9 Iteration: 368 Training loss: 1.27937\n",
      "Epoch: 1/9 Iteration: 369 Training loss: 1.16854\n",
      "Epoch: 1/9 Iteration: 370 Training loss: 2.19713\n",
      "Epoch: 1/9 Iteration: 371 Training loss: 2.15540\n",
      "Epoch: 1/9 Iteration: 372 Training loss: 0.61926\n",
      "Epoch: 1/9 Iteration: 373 Training loss: 1.12564\n",
      "Epoch: 1/9 Iteration: 374 Training loss: 1.09011\n",
      "Epoch: 1/9 Iteration: 375 Training loss: 0.67832\n",
      "Epoch: 1/9 Iteration: 376 Training loss: 1.47947\n",
      "Epoch: 1/9 Iteration: 377 Training loss: 1.44733\n",
      "Epoch: 1/9 Iteration: 378 Training loss: 1.07197\n",
      "Epoch: 1/9 Iteration: 379 Training loss: 1.06180\n",
      "Epoch: 1/9 Iteration: 380 Training loss: 2.32243\n",
      "Epoch: 1/9 Iteration: 381 Training loss: 2.04245\n",
      "Epoch: 1/9 Iteration: 382 Training loss: 2.03082\n",
      "Epoch: 1/9 Iteration: 383 Training loss: 0.82081\n",
      "Epoch: 1/9 Iteration: 384 Training loss: 1.07289\n",
      "Epoch: 1/9 Iteration: 385 Training loss: 1.33547\n",
      "Epoch: 1/9 Iteration: 386 Training loss: 1.30316\n",
      "Epoch: 1/9 Iteration: 387 Training loss: 1.79168\n",
      "Epoch: 1/9 Iteration: 388 Training loss: 0.78211\n",
      "Epoch: 1/9 Iteration: 389 Training loss: 0.49773\n",
      "Epoch: 1/9 Iteration: 390 Training loss: 1.37874\n",
      "Epoch: 1/9 Iteration: 391 Training loss: 0.80246\n",
      "Epoch: 1/9 Iteration: 392 Training loss: 1.48093\n",
      "Epoch: 1/9 Iteration: 393 Training loss: 2.03531\n",
      "Epoch: 1/9 Iteration: 394 Training loss: 1.84916\n",
      "Epoch: 1/9 Iteration: 395 Training loss: 1.83741\n",
      "Epoch: 1/9 Iteration: 396 Training loss: 1.61752\n",
      "Epoch: 1/9 Iteration: 397 Training loss: 1.56451\n",
      "Epoch: 1/9 Iteration: 398 Training loss: 0.76165\n",
      "Epoch: 1/9 Iteration: 399 Training loss: 1.42388\n",
      "Epoch: 1/9 Iteration: 400 Training loss: 0.87306\n",
      "***\n",
      "Epoch: 1/9 Iteration: 400 Validation Acc: 0.5933\n",
      "***\n",
      "Epoch: 1/9 Iteration: 401 Training loss: 1.38612\n",
      "Epoch: 1/9 Iteration: 402 Training loss: 1.13704\n",
      "Epoch: 1/9 Iteration: 403 Training loss: 1.40742\n",
      "Epoch: 1/9 Iteration: 404 Training loss: 1.39905\n",
      "Epoch: 1/9 Iteration: 405 Training loss: 1.27436\n",
      "Epoch: 1/9 Iteration: 406 Training loss: 1.85820\n",
      "Epoch: 1/9 Iteration: 407 Training loss: 1.12173\n",
      "Epoch: 1/9 Iteration: 408 Training loss: 0.66067\n",
      "Epoch: 1/9 Iteration: 409 Training loss: 1.39054\n",
      "Epoch: 1/9 Iteration: 410 Training loss: 0.72139\n",
      "Epoch: 1/9 Iteration: 411 Training loss: 1.03165\n",
      "Epoch: 1/9 Iteration: 412 Training loss: 1.36280\n",
      "Epoch: 1/9 Iteration: 413 Training loss: 1.39078\n",
      "Epoch: 1/9 Iteration: 414 Training loss: 1.61336\n",
      "Epoch: 1/9 Iteration: 415 Training loss: 1.90548\n",
      "Epoch: 1/9 Iteration: 416 Training loss: 1.25382\n",
      "Epoch: 1/9 Iteration: 417 Training loss: 1.96994\n",
      "Epoch: 1/9 Iteration: 418 Training loss: 1.36067\n",
      "Epoch: 1/9 Iteration: 419 Training loss: 1.63002\n",
      "Epoch: 1/9 Iteration: 420 Training loss: 1.42854\n",
      "Epoch: 1/9 Iteration: 421 Training loss: 0.83010\n",
      "Epoch: 1/9 Iteration: 422 Training loss: 0.81913\n",
      "Epoch: 1/9 Iteration: 423 Training loss: 1.28159\n",
      "Epoch: 1/9 Iteration: 424 Training loss: 1.80422\n",
      "Epoch: 1/9 Iteration: 425 Training loss: 1.34490\n",
      "Epoch: 1/9 Iteration: 426 Training loss: 0.79248\n",
      "Epoch: 1/9 Iteration: 427 Training loss: 0.70962\n",
      "Epoch: 1/9 Iteration: 428 Training loss: 1.54490\n",
      "Epoch: 1/9 Iteration: 429 Training loss: 1.59070\n",
      "Epoch: 1/9 Iteration: 430 Training loss: 1.06129\n",
      "Epoch: 1/9 Iteration: 431 Training loss: 1.06494\n",
      "Epoch: 1/9 Iteration: 432 Training loss: 1.19098\n",
      "Epoch: 1/9 Iteration: 433 Training loss: 1.69421\n",
      "Epoch: 1/9 Iteration: 434 Training loss: 1.51400\n",
      "Epoch: 1/9 Iteration: 435 Training loss: 1.22461\n",
      "Epoch: 1/9 Iteration: 436 Training loss: 1.63324\n",
      "Epoch: 1/9 Iteration: 437 Training loss: 1.43378\n",
      "Epoch: 1/9 Iteration: 438 Training loss: 1.32969\n",
      "Epoch: 1/9 Iteration: 439 Training loss: 0.92412\n",
      "Epoch: 1/9 Iteration: 440 Training loss: 1.89335\n",
      "Epoch: 1/9 Iteration: 441 Training loss: 1.52256\n",
      "Epoch: 1/9 Iteration: 442 Training loss: 2.76176\n",
      "Epoch: 1/9 Iteration: 443 Training loss: 1.38272\n",
      "Epoch: 1/9 Iteration: 444 Training loss: 1.39185\n",
      "Epoch: 1/9 Iteration: 445 Training loss: 1.37484\n",
      "Epoch: 1/9 Iteration: 446 Training loss: 1.17198\n",
      "Epoch: 1/9 Iteration: 447 Training loss: 1.31339\n",
      "Epoch: 1/9 Iteration: 448 Training loss: 1.60074\n",
      "Epoch: 1/9 Iteration: 449 Training loss: 1.00248\n",
      "Epoch: 1/9 Iteration: 450 Training loss: 1.65323\n",
      "***\n",
      "Epoch: 1/9 Iteration: 450 Validation Acc: 0.6267\n",
      "***\n",
      "Epoch: 1/9 Iteration: 451 Training loss: 1.31961\n",
      "Epoch: 1/9 Iteration: 452 Training loss: 2.19358\n",
      "Epoch: 1/9 Iteration: 453 Training loss: 1.49189\n",
      "Epoch: 1/9 Iteration: 454 Training loss: 1.33563\n",
      "Epoch: 1/9 Iteration: 455 Training loss: 1.38200\n",
      "Epoch: 1/9 Iteration: 456 Training loss: 2.18825\n",
      "Epoch: 1/9 Iteration: 457 Training loss: 2.18334\n",
      "Epoch: 1/9 Iteration: 458 Training loss: 0.59356\n",
      "Epoch: 1/9 Iteration: 459 Training loss: 1.67963\n",
      "Epoch: 1/9 Iteration: 460 Training loss: 0.96952\n",
      "Epoch: 1/9 Iteration: 461 Training loss: 1.56315\n",
      "Epoch: 1/9 Iteration: 462 Training loss: 1.45695\n",
      "Epoch: 1/9 Iteration: 463 Training loss: 1.36022\n",
      "Epoch: 1/9 Iteration: 464 Training loss: 1.11217\n",
      "Epoch: 1/9 Iteration: 465 Training loss: 1.19888\n",
      "Epoch: 1/9 Iteration: 466 Training loss: 1.86432\n",
      "Epoch: 1/9 Iteration: 467 Training loss: 1.16410\n",
      "Epoch: 1/9 Iteration: 468 Training loss: 1.02560\n",
      "Epoch: 1/9 Iteration: 469 Training loss: 1.24835\n",
      "Epoch: 1/9 Iteration: 470 Training loss: 1.36088\n",
      "Epoch: 1/9 Iteration: 471 Training loss: 0.95813\n",
      "Epoch: 1/9 Iteration: 472 Training loss: 2.08197\n",
      "Epoch: 1/9 Iteration: 473 Training loss: 1.51444\n",
      "Epoch: 1/9 Iteration: 474 Training loss: 1.20675\n",
      "Epoch: 1/9 Iteration: 475 Training loss: 1.26951\n",
      "Epoch: 1/9 Iteration: 476 Training loss: 2.09567\n",
      "Epoch: 1/9 Iteration: 477 Training loss: 0.90441\n",
      "Epoch: 1/9 Iteration: 478 Training loss: 2.13533\n",
      "Epoch: 1/9 Iteration: 479 Training loss: 1.60152\n",
      "Epoch: 2/9 Iteration: 480 Training loss: 1.75917\n",
      "Epoch: 2/9 Iteration: 481 Training loss: 0.99821\n",
      "Epoch: 2/9 Iteration: 482 Training loss: 1.30844\n",
      "Epoch: 2/9 Iteration: 483 Training loss: 0.39793\n",
      "Epoch: 2/9 Iteration: 484 Training loss: 1.75939\n",
      "Epoch: 2/9 Iteration: 485 Training loss: 1.50118\n",
      "Epoch: 2/9 Iteration: 486 Training loss: 0.94742\n",
      "Epoch: 2/9 Iteration: 487 Training loss: 1.60620\n",
      "Epoch: 2/9 Iteration: 488 Training loss: 1.79552\n",
      "Epoch: 2/9 Iteration: 489 Training loss: 0.78275\n",
      "Epoch: 2/9 Iteration: 490 Training loss: 2.30005\n",
      "Epoch: 2/9 Iteration: 491 Training loss: 0.95277\n",
      "Epoch: 2/9 Iteration: 492 Training loss: 1.46000\n",
      "Epoch: 2/9 Iteration: 493 Training loss: 1.38376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/9 Iteration: 494 Training loss: 1.11105\n",
      "Epoch: 2/9 Iteration: 495 Training loss: 1.09879\n",
      "Epoch: 2/9 Iteration: 496 Training loss: 1.53493\n",
      "Epoch: 2/9 Iteration: 497 Training loss: 0.76765\n",
      "Epoch: 2/9 Iteration: 498 Training loss: 1.85475\n",
      "Epoch: 2/9 Iteration: 499 Training loss: 1.69683\n",
      "Epoch: 2/9 Iteration: 500 Training loss: 1.64406\n",
      "***\n",
      "Epoch: 2/9 Iteration: 500 Validation Acc: 0.6733\n",
      "***\n",
      "Epoch: 2/9 Iteration: 501 Training loss: 0.74573\n",
      "Epoch: 2/9 Iteration: 502 Training loss: 0.75753\n",
      "Epoch: 2/9 Iteration: 503 Training loss: 1.84229\n",
      "Epoch: 2/9 Iteration: 504 Training loss: 1.51675\n",
      "Epoch: 2/9 Iteration: 505 Training loss: 1.53085\n",
      "Epoch: 2/9 Iteration: 506 Training loss: 1.72973\n",
      "Epoch: 2/9 Iteration: 507 Training loss: 1.21857\n",
      "Epoch: 2/9 Iteration: 508 Training loss: 1.56267\n",
      "Epoch: 2/9 Iteration: 509 Training loss: 2.43501\n",
      "Epoch: 2/9 Iteration: 510 Training loss: 1.20187\n",
      "Epoch: 2/9 Iteration: 511 Training loss: 1.89758\n",
      "Epoch: 2/9 Iteration: 512 Training loss: 1.11762\n",
      "Epoch: 2/9 Iteration: 513 Training loss: 1.09484\n",
      "Epoch: 2/9 Iteration: 514 Training loss: 1.14093\n",
      "Epoch: 2/9 Iteration: 515 Training loss: 1.42680\n",
      "Epoch: 2/9 Iteration: 516 Training loss: 1.54365\n",
      "Epoch: 2/9 Iteration: 517 Training loss: 1.12881\n",
      "Epoch: 2/9 Iteration: 518 Training loss: 1.33241\n",
      "Epoch: 2/9 Iteration: 519 Training loss: 1.49503\n",
      "Epoch: 2/9 Iteration: 520 Training loss: 1.93789\n",
      "Epoch: 2/9 Iteration: 521 Training loss: 1.41728\n",
      "Epoch: 2/9 Iteration: 522 Training loss: 1.25180\n",
      "Epoch: 2/9 Iteration: 523 Training loss: 1.89210\n",
      "Epoch: 2/9 Iteration: 524 Training loss: 1.14866\n",
      "Epoch: 2/9 Iteration: 525 Training loss: 1.05433\n",
      "Epoch: 2/9 Iteration: 526 Training loss: 1.57266\n",
      "Epoch: 2/9 Iteration: 527 Training loss: 1.58644\n",
      "Epoch: 2/9 Iteration: 528 Training loss: 0.53741\n",
      "Epoch: 2/9 Iteration: 529 Training loss: 0.95009\n",
      "Epoch: 2/9 Iteration: 530 Training loss: 1.66103\n",
      "Epoch: 2/9 Iteration: 531 Training loss: 1.56548\n",
      "Epoch: 2/9 Iteration: 532 Training loss: 1.19155\n",
      "Epoch: 2/9 Iteration: 533 Training loss: 1.28068\n",
      "Epoch: 2/9 Iteration: 534 Training loss: 1.04731\n",
      "Epoch: 2/9 Iteration: 535 Training loss: 1.18331\n",
      "Epoch: 2/9 Iteration: 536 Training loss: 1.37683\n",
      "Epoch: 2/9 Iteration: 537 Training loss: 1.21540\n",
      "Epoch: 2/9 Iteration: 538 Training loss: 1.36854\n",
      "Epoch: 2/9 Iteration: 539 Training loss: 1.15250\n",
      "Epoch: 2/9 Iteration: 540 Training loss: 1.72396\n",
      "Epoch: 2/9 Iteration: 541 Training loss: 0.89333\n",
      "Epoch: 2/9 Iteration: 542 Training loss: 0.59771\n",
      "Epoch: 2/9 Iteration: 543 Training loss: 1.42365\n",
      "Epoch: 2/9 Iteration: 544 Training loss: 0.54217\n",
      "Epoch: 2/9 Iteration: 545 Training loss: 1.59952\n",
      "Epoch: 2/9 Iteration: 546 Training loss: 1.10103\n",
      "Epoch: 2/9 Iteration: 547 Training loss: 1.26902\n",
      "Epoch: 2/9 Iteration: 548 Training loss: 1.16544\n",
      "Epoch: 2/9 Iteration: 549 Training loss: 1.20776\n",
      "Epoch: 2/9 Iteration: 550 Training loss: 2.51259\n",
      "***\n",
      "Epoch: 2/9 Iteration: 550 Validation Acc: 0.6767\n",
      "***\n",
      "Epoch: 2/9 Iteration: 551 Training loss: 1.64503\n",
      "Epoch: 2/9 Iteration: 552 Training loss: 1.17254\n",
      "Epoch: 2/9 Iteration: 553 Training loss: 0.79144\n",
      "Epoch: 2/9 Iteration: 554 Training loss: 0.57553\n",
      "Epoch: 2/9 Iteration: 555 Training loss: 1.26454\n",
      "Epoch: 2/9 Iteration: 556 Training loss: 1.39122\n",
      "Epoch: 2/9 Iteration: 557 Training loss: 1.27298\n",
      "Epoch: 2/9 Iteration: 558 Training loss: 1.23759\n",
      "Epoch: 2/9 Iteration: 559 Training loss: 1.88521\n",
      "Epoch: 2/9 Iteration: 560 Training loss: 1.15541\n",
      "Epoch: 2/9 Iteration: 561 Training loss: 2.01010\n",
      "Epoch: 2/9 Iteration: 562 Training loss: 1.39429\n",
      "Epoch: 2/9 Iteration: 563 Training loss: 0.85117\n",
      "Epoch: 2/9 Iteration: 564 Training loss: 1.19640\n",
      "Epoch: 2/9 Iteration: 565 Training loss: 0.75803\n",
      "Epoch: 2/9 Iteration: 566 Training loss: 1.33010\n",
      "Epoch: 2/9 Iteration: 567 Training loss: 0.99943\n",
      "Epoch: 2/9 Iteration: 568 Training loss: 1.17909\n",
      "Epoch: 2/9 Iteration: 569 Training loss: 1.60390\n",
      "Epoch: 2/9 Iteration: 570 Training loss: 1.46853\n",
      "Epoch: 2/9 Iteration: 571 Training loss: 1.49302\n",
      "Epoch: 2/9 Iteration: 572 Training loss: 0.71703\n",
      "Epoch: 2/9 Iteration: 573 Training loss: 1.57375\n",
      "Epoch: 2/9 Iteration: 574 Training loss: 1.49137\n",
      "Epoch: 2/9 Iteration: 575 Training loss: 1.64076\n",
      "Epoch: 2/9 Iteration: 576 Training loss: 1.22231\n",
      "Epoch: 2/9 Iteration: 577 Training loss: 1.41562\n",
      "Epoch: 2/9 Iteration: 578 Training loss: 1.22943\n",
      "Epoch: 2/9 Iteration: 579 Training loss: 0.73180\n",
      "Epoch: 2/9 Iteration: 580 Training loss: 1.71465\n",
      "Epoch: 2/9 Iteration: 581 Training loss: 0.98868\n",
      "Epoch: 2/9 Iteration: 582 Training loss: 1.08966\n",
      "Epoch: 2/9 Iteration: 583 Training loss: 0.94433\n",
      "Epoch: 2/9 Iteration: 584 Training loss: 1.30870\n",
      "Epoch: 2/9 Iteration: 585 Training loss: 1.30327\n",
      "Epoch: 2/9 Iteration: 586 Training loss: 0.78382\n",
      "Epoch: 2/9 Iteration: 587 Training loss: 1.73648\n",
      "Epoch: 2/9 Iteration: 588 Training loss: 0.74071\n",
      "Epoch: 2/9 Iteration: 589 Training loss: 0.97079\n",
      "Epoch: 2/9 Iteration: 590 Training loss: 1.28138\n",
      "Epoch: 2/9 Iteration: 591 Training loss: 1.34955\n",
      "Epoch: 2/9 Iteration: 592 Training loss: 1.31705\n",
      "Epoch: 2/9 Iteration: 593 Training loss: 1.28034\n",
      "Epoch: 2/9 Iteration: 594 Training loss: 1.06128\n",
      "Epoch: 2/9 Iteration: 595 Training loss: 1.38528\n",
      "Epoch: 2/9 Iteration: 596 Training loss: 0.71236\n",
      "Epoch: 2/9 Iteration: 597 Training loss: 0.73569\n",
      "Epoch: 2/9 Iteration: 598 Training loss: 1.02821\n",
      "Epoch: 2/9 Iteration: 599 Training loss: 1.70539\n",
      "Epoch: 2/9 Iteration: 600 Training loss: 0.78033\n",
      "***\n",
      "Epoch: 2/9 Iteration: 600 Validation Acc: 0.6200\n",
      "***\n",
      "Epoch: 2/9 Iteration: 601 Training loss: 2.17374\n",
      "Epoch: 2/9 Iteration: 602 Training loss: 0.92642\n",
      "Epoch: 2/9 Iteration: 603 Training loss: 0.87938\n",
      "Epoch: 2/9 Iteration: 604 Training loss: 2.09324\n",
      "Epoch: 2/9 Iteration: 605 Training loss: 1.49136\n",
      "Epoch: 2/9 Iteration: 606 Training loss: 1.00179\n",
      "Epoch: 2/9 Iteration: 607 Training loss: 1.28328\n",
      "Epoch: 2/9 Iteration: 608 Training loss: 1.45854\n",
      "Epoch: 2/9 Iteration: 609 Training loss: 1.28151\n",
      "Epoch: 2/9 Iteration: 610 Training loss: 1.49521\n",
      "Epoch: 2/9 Iteration: 611 Training loss: 1.86193\n",
      "Epoch: 2/9 Iteration: 612 Training loss: 0.72515\n",
      "Epoch: 2/9 Iteration: 613 Training loss: 0.70374\n",
      "Epoch: 2/9 Iteration: 614 Training loss: 0.67213\n",
      "Epoch: 2/9 Iteration: 615 Training loss: 1.28794\n",
      "Epoch: 2/9 Iteration: 616 Training loss: 2.03554\n",
      "Epoch: 2/9 Iteration: 617 Training loss: 1.97763\n",
      "Epoch: 2/9 Iteration: 618 Training loss: 0.69126\n",
      "Epoch: 2/9 Iteration: 619 Training loss: 1.59972\n",
      "Epoch: 2/9 Iteration: 620 Training loss: 1.62193\n",
      "Epoch: 2/9 Iteration: 621 Training loss: 1.50711\n",
      "Epoch: 2/9 Iteration: 622 Training loss: 1.66323\n",
      "Epoch: 2/9 Iteration: 623 Training loss: 0.99195\n",
      "Epoch: 2/9 Iteration: 624 Training loss: 1.24283\n",
      "Epoch: 2/9 Iteration: 625 Training loss: 1.68555\n",
      "Epoch: 2/9 Iteration: 626 Training loss: 1.65081\n",
      "Epoch: 2/9 Iteration: 627 Training loss: 1.53474\n",
      "Epoch: 2/9 Iteration: 628 Training loss: 1.02950\n",
      "Epoch: 2/9 Iteration: 629 Training loss: 0.66449\n",
      "Epoch: 2/9 Iteration: 630 Training loss: 1.17999\n",
      "Epoch: 2/9 Iteration: 631 Training loss: 0.81412\n",
      "Epoch: 2/9 Iteration: 632 Training loss: 1.32594\n",
      "Epoch: 2/9 Iteration: 633 Training loss: 1.16266\n",
      "Epoch: 2/9 Iteration: 634 Training loss: 1.44148\n",
      "Epoch: 2/9 Iteration: 635 Training loss: 1.23635\n",
      "Epoch: 2/9 Iteration: 636 Training loss: 1.11722\n",
      "Epoch: 2/9 Iteration: 637 Training loss: 0.75138\n",
      "Epoch: 2/9 Iteration: 638 Training loss: 0.96896\n",
      "Epoch: 2/9 Iteration: 639 Training loss: 0.90978\n",
      "Epoch: 2/9 Iteration: 640 Training loss: 0.55259\n",
      "Epoch: 2/9 Iteration: 641 Training loss: 1.12776\n",
      "Epoch: 2/9 Iteration: 642 Training loss: 1.11058\n",
      "Epoch: 2/9 Iteration: 643 Training loss: 1.40801\n",
      "Epoch: 2/9 Iteration: 644 Training loss: 1.67624\n",
      "Epoch: 2/9 Iteration: 645 Training loss: 1.11314\n",
      "Epoch: 2/9 Iteration: 646 Training loss: 2.11019\n",
      "Epoch: 2/9 Iteration: 647 Training loss: 1.25517\n",
      "Epoch: 2/9 Iteration: 648 Training loss: 0.83262\n",
      "Epoch: 2/9 Iteration: 649 Training loss: 1.30637\n",
      "Epoch: 2/9 Iteration: 650 Training loss: 0.52501\n",
      "***\n",
      "Epoch: 2/9 Iteration: 650 Validation Acc: 0.6600\n",
      "***\n",
      "Epoch: 2/9 Iteration: 651 Training loss: 0.79964\n",
      "Epoch: 2/9 Iteration: 652 Training loss: 1.06455\n",
      "Epoch: 2/9 Iteration: 653 Training loss: 1.08806\n",
      "Epoch: 2/9 Iteration: 654 Training loss: 1.58126\n",
      "Epoch: 2/9 Iteration: 655 Training loss: 2.04523\n",
      "Epoch: 2/9 Iteration: 656 Training loss: 1.58044\n",
      "Epoch: 2/9 Iteration: 657 Training loss: 2.02256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/9 Iteration: 658 Training loss: 1.03277\n",
      "Epoch: 2/9 Iteration: 659 Training loss: 1.02371\n",
      "Epoch: 2/9 Iteration: 660 Training loss: 0.55228\n",
      "Epoch: 2/9 Iteration: 661 Training loss: 1.34364\n",
      "Epoch: 2/9 Iteration: 662 Training loss: 1.02586\n",
      "Epoch: 2/9 Iteration: 663 Training loss: 0.69001\n",
      "Epoch: 2/9 Iteration: 664 Training loss: 1.83938\n",
      "Epoch: 2/9 Iteration: 665 Training loss: 1.91208\n",
      "Epoch: 2/9 Iteration: 666 Training loss: 1.43113\n",
      "Epoch: 2/9 Iteration: 667 Training loss: 1.22613\n",
      "Epoch: 2/9 Iteration: 668 Training loss: 1.08525\n",
      "Epoch: 2/9 Iteration: 669 Training loss: 1.30232\n",
      "Epoch: 2/9 Iteration: 670 Training loss: 1.08722\n",
      "Epoch: 2/9 Iteration: 671 Training loss: 1.20350\n",
      "Epoch: 2/9 Iteration: 672 Training loss: 1.09508\n",
      "Epoch: 2/9 Iteration: 673 Training loss: 1.54659\n",
      "Epoch: 2/9 Iteration: 674 Training loss: 1.11371\n",
      "Epoch: 2/9 Iteration: 675 Training loss: 0.89652\n",
      "Epoch: 2/9 Iteration: 676 Training loss: 1.38489\n",
      "Epoch: 2/9 Iteration: 677 Training loss: 1.27586\n",
      "Epoch: 2/9 Iteration: 678 Training loss: 0.85326\n",
      "Epoch: 2/9 Iteration: 679 Training loss: 0.81276\n",
      "Epoch: 2/9 Iteration: 680 Training loss: 1.49462\n",
      "Epoch: 2/9 Iteration: 681 Training loss: 1.36292\n",
      "Epoch: 2/9 Iteration: 682 Training loss: 2.19968\n",
      "Epoch: 2/9 Iteration: 683 Training loss: 0.83214\n",
      "Epoch: 2/9 Iteration: 684 Training loss: 1.81184\n",
      "Epoch: 2/9 Iteration: 685 Training loss: 2.02865\n",
      "Epoch: 2/9 Iteration: 686 Training loss: 1.00052\n",
      "Epoch: 2/9 Iteration: 687 Training loss: 1.13854\n",
      "Epoch: 2/9 Iteration: 688 Training loss: 0.96765\n",
      "Epoch: 2/9 Iteration: 689 Training loss: 0.42604\n",
      "Epoch: 2/9 Iteration: 690 Training loss: 1.61443\n",
      "Epoch: 2/9 Iteration: 691 Training loss: 1.02079\n",
      "Epoch: 2/9 Iteration: 692 Training loss: 1.60337\n",
      "Epoch: 2/9 Iteration: 693 Training loss: 1.70973\n",
      "Epoch: 2/9 Iteration: 694 Training loss: 1.19250\n",
      "Epoch: 2/9 Iteration: 695 Training loss: 2.01642\n",
      "Epoch: 2/9 Iteration: 696 Training loss: 1.16927\n",
      "Epoch: 2/9 Iteration: 697 Training loss: 1.26888\n",
      "Epoch: 2/9 Iteration: 698 Training loss: 0.77260\n",
      "Epoch: 2/9 Iteration: 699 Training loss: 1.37268\n",
      "Epoch: 2/9 Iteration: 700 Training loss: 1.13519\n",
      "***\n",
      "Epoch: 2/9 Iteration: 700 Validation Acc: 0.5733\n",
      "***\n",
      "Epoch: 2/9 Iteration: 701 Training loss: 1.68141\n",
      "Epoch: 2/9 Iteration: 702 Training loss: 0.97914\n",
      "Epoch: 2/9 Iteration: 703 Training loss: 1.73835\n",
      "Epoch: 2/9 Iteration: 704 Training loss: 1.13155\n",
      "Epoch: 2/9 Iteration: 705 Training loss: 0.75980\n",
      "Epoch: 2/9 Iteration: 706 Training loss: 1.52206\n",
      "Epoch: 2/9 Iteration: 707 Training loss: 1.21896\n",
      "Epoch: 2/9 Iteration: 708 Training loss: 1.16791\n",
      "Epoch: 2/9 Iteration: 709 Training loss: 1.17517\n",
      "Epoch: 2/9 Iteration: 710 Training loss: 0.99988\n",
      "Epoch: 2/9 Iteration: 711 Training loss: 1.09667\n",
      "Epoch: 2/9 Iteration: 712 Training loss: 2.02187\n",
      "Epoch: 2/9 Iteration: 713 Training loss: 1.23158\n",
      "Epoch: 2/9 Iteration: 714 Training loss: 1.11408\n",
      "Epoch: 2/9 Iteration: 715 Training loss: 1.37963\n",
      "Epoch: 2/9 Iteration: 716 Training loss: 1.20826\n",
      "Epoch: 2/9 Iteration: 717 Training loss: 0.66272\n",
      "Epoch: 2/9 Iteration: 718 Training loss: 1.91673\n",
      "Epoch: 2/9 Iteration: 719 Training loss: 2.12318\n",
      "Epoch: 3/9 Iteration: 720 Training loss: 1.37005\n",
      "Epoch: 3/9 Iteration: 721 Training loss: 1.06311\n",
      "Epoch: 3/9 Iteration: 722 Training loss: 1.08085\n",
      "Epoch: 3/9 Iteration: 723 Training loss: 0.47257\n",
      "Epoch: 3/9 Iteration: 724 Training loss: 1.57281\n",
      "Epoch: 3/9 Iteration: 725 Training loss: 1.71349\n",
      "Epoch: 3/9 Iteration: 726 Training loss: 1.12832\n",
      "Epoch: 3/9 Iteration: 727 Training loss: 1.39429\n",
      "Epoch: 3/9 Iteration: 728 Training loss: 1.30097\n",
      "Epoch: 3/9 Iteration: 729 Training loss: 1.06256\n",
      "Epoch: 3/9 Iteration: 730 Training loss: 1.94273\n",
      "Epoch: 3/9 Iteration: 731 Training loss: 0.87358\n",
      "Epoch: 3/9 Iteration: 732 Training loss: 1.40016\n",
      "Epoch: 3/9 Iteration: 733 Training loss: 0.97285\n",
      "Epoch: 3/9 Iteration: 734 Training loss: 1.06479\n",
      "Epoch: 3/9 Iteration: 735 Training loss: 1.57294\n",
      "Epoch: 3/9 Iteration: 736 Training loss: 1.58309\n",
      "Epoch: 3/9 Iteration: 737 Training loss: 1.13105\n",
      "Epoch: 3/9 Iteration: 738 Training loss: 1.59663\n",
      "Epoch: 3/9 Iteration: 739 Training loss: 1.35810\n",
      "Epoch: 3/9 Iteration: 740 Training loss: 1.37890\n",
      "Epoch: 3/9 Iteration: 741 Training loss: 0.67905\n",
      "Epoch: 3/9 Iteration: 742 Training loss: 0.81839\n",
      "Epoch: 3/9 Iteration: 743 Training loss: 1.17533\n",
      "Epoch: 3/9 Iteration: 744 Training loss: 0.85075\n",
      "Epoch: 3/9 Iteration: 745 Training loss: 1.11696\n",
      "Epoch: 3/9 Iteration: 746 Training loss: 1.70422\n",
      "Epoch: 3/9 Iteration: 747 Training loss: 1.35021\n",
      "Epoch: 3/9 Iteration: 748 Training loss: 2.62219\n",
      "Epoch: 3/9 Iteration: 749 Training loss: 2.03766\n",
      "Epoch: 3/9 Iteration: 750 Training loss: 1.27309\n",
      "***\n",
      "Epoch: 3/9 Iteration: 750 Validation Acc: 0.5033\n",
      "***\n",
      "Epoch: 3/9 Iteration: 751 Training loss: 1.40966\n",
      "Epoch: 3/9 Iteration: 752 Training loss: 1.19903\n",
      "Epoch: 3/9 Iteration: 753 Training loss: 1.13645\n",
      "Epoch: 3/9 Iteration: 754 Training loss: 1.33463\n",
      "Epoch: 3/9 Iteration: 755 Training loss: 1.63886\n",
      "Epoch: 3/9 Iteration: 756 Training loss: 0.91445\n",
      "Epoch: 3/9 Iteration: 757 Training loss: 0.87786\n",
      "Epoch: 3/9 Iteration: 758 Training loss: 0.94516\n",
      "Epoch: 3/9 Iteration: 759 Training loss: 0.95559\n",
      "Epoch: 3/9 Iteration: 760 Training loss: 1.38748\n",
      "Epoch: 3/9 Iteration: 761 Training loss: 1.76161\n",
      "Epoch: 3/9 Iteration: 762 Training loss: 1.64971\n",
      "Epoch: 3/9 Iteration: 763 Training loss: 1.89571\n",
      "Epoch: 3/9 Iteration: 764 Training loss: 1.23239\n",
      "Epoch: 3/9 Iteration: 765 Training loss: 0.71284\n",
      "Epoch: 3/9 Iteration: 766 Training loss: 1.54894\n",
      "Epoch: 3/9 Iteration: 767 Training loss: 1.24573\n",
      "Epoch: 3/9 Iteration: 768 Training loss: 0.54846\n",
      "Epoch: 3/9 Iteration: 769 Training loss: 1.48661\n",
      "Epoch: 3/9 Iteration: 770 Training loss: 1.82437\n",
      "Epoch: 3/9 Iteration: 771 Training loss: 1.32267\n",
      "Epoch: 3/9 Iteration: 772 Training loss: 0.97029\n",
      "Epoch: 3/9 Iteration: 773 Training loss: 0.86091\n",
      "Epoch: 3/9 Iteration: 774 Training loss: 1.02622\n",
      "Epoch: 3/9 Iteration: 775 Training loss: 1.13939\n",
      "Epoch: 3/9 Iteration: 776 Training loss: 1.17535\n",
      "Epoch: 3/9 Iteration: 777 Training loss: 0.85883\n",
      "Epoch: 3/9 Iteration: 778 Training loss: 0.86468\n",
      "Epoch: 3/9 Iteration: 779 Training loss: 0.83964\n",
      "Epoch: 3/9 Iteration: 780 Training loss: 1.16449\n",
      "Epoch: 3/9 Iteration: 781 Training loss: 0.85836\n",
      "Epoch: 3/9 Iteration: 782 Training loss: 0.80932\n",
      "Epoch: 3/9 Iteration: 783 Training loss: 1.37936\n",
      "Epoch: 3/9 Iteration: 784 Training loss: 0.94260\n",
      "Epoch: 3/9 Iteration: 785 Training loss: 0.94735\n",
      "Epoch: 3/9 Iteration: 786 Training loss: 1.08850\n",
      "Epoch: 3/9 Iteration: 787 Training loss: 1.17507\n",
      "Epoch: 3/9 Iteration: 788 Training loss: 1.28970\n",
      "Epoch: 3/9 Iteration: 789 Training loss: 0.85026\n",
      "Epoch: 3/9 Iteration: 790 Training loss: 2.57005\n",
      "Epoch: 3/9 Iteration: 791 Training loss: 1.52286\n",
      "Epoch: 3/9 Iteration: 792 Training loss: 1.18890\n",
      "Epoch: 3/9 Iteration: 793 Training loss: 0.80878\n",
      "Epoch: 3/9 Iteration: 794 Training loss: 0.83586\n",
      "Epoch: 3/9 Iteration: 795 Training loss: 1.48227\n",
      "Epoch: 3/9 Iteration: 796 Training loss: 1.56684\n",
      "Epoch: 3/9 Iteration: 797 Training loss: 1.32419\n",
      "Epoch: 3/9 Iteration: 798 Training loss: 1.29465\n",
      "Epoch: 3/9 Iteration: 799 Training loss: 1.58619\n",
      "Epoch: 3/9 Iteration: 800 Training loss: 1.32406\n",
      "***\n",
      "Epoch: 3/9 Iteration: 800 Validation Acc: 0.6833\n",
      "***\n",
      "Epoch: 3/9 Iteration: 801 Training loss: 1.28279\n",
      "Epoch: 3/9 Iteration: 802 Training loss: 1.12366\n",
      "Epoch: 3/9 Iteration: 803 Training loss: 0.92015\n",
      "Epoch: 3/9 Iteration: 804 Training loss: 1.25949\n",
      "Epoch: 3/9 Iteration: 805 Training loss: 0.89560\n",
      "Epoch: 3/9 Iteration: 806 Training loss: 0.98535\n",
      "Epoch: 3/9 Iteration: 807 Training loss: 1.18380\n",
      "Epoch: 3/9 Iteration: 808 Training loss: 1.50076\n",
      "Epoch: 3/9 Iteration: 809 Training loss: 1.31313\n",
      "Epoch: 3/9 Iteration: 810 Training loss: 1.01839\n",
      "Epoch: 3/9 Iteration: 811 Training loss: 1.17393\n",
      "Epoch: 3/9 Iteration: 812 Training loss: 0.67707\n",
      "Epoch: 3/9 Iteration: 813 Training loss: 1.11136\n",
      "Epoch: 3/9 Iteration: 814 Training loss: 0.92014\n",
      "Epoch: 3/9 Iteration: 815 Training loss: 1.24082\n",
      "Epoch: 3/9 Iteration: 816 Training loss: 1.18176\n",
      "Epoch: 3/9 Iteration: 817 Training loss: 1.06938\n",
      "Epoch: 3/9 Iteration: 818 Training loss: 0.74153\n",
      "Epoch: 3/9 Iteration: 819 Training loss: 0.79632\n",
      "Epoch: 3/9 Iteration: 820 Training loss: 0.99571\n",
      "Epoch: 3/9 Iteration: 821 Training loss: 1.19357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/9 Iteration: 822 Training loss: 1.03492\n",
      "Epoch: 3/9 Iteration: 823 Training loss: 1.08112\n",
      "Epoch: 3/9 Iteration: 824 Training loss: 0.73193\n",
      "Epoch: 3/9 Iteration: 825 Training loss: 0.77845\n",
      "Epoch: 3/9 Iteration: 826 Training loss: 1.05020\n",
      "Epoch: 3/9 Iteration: 827 Training loss: 1.89561\n",
      "Epoch: 3/9 Iteration: 828 Training loss: 1.05941\n",
      "Epoch: 3/9 Iteration: 829 Training loss: 1.22055\n",
      "Epoch: 3/9 Iteration: 830 Training loss: 0.82084\n",
      "Epoch: 3/9 Iteration: 831 Training loss: 1.30791\n",
      "Epoch: 3/9 Iteration: 832 Training loss: 1.21542\n",
      "Epoch: 3/9 Iteration: 833 Training loss: 0.70867\n",
      "Epoch: 3/9 Iteration: 834 Training loss: 0.75424\n",
      "Epoch: 3/9 Iteration: 835 Training loss: 0.57669\n",
      "Epoch: 3/9 Iteration: 836 Training loss: 0.61684\n",
      "Epoch: 3/9 Iteration: 837 Training loss: 0.72056\n",
      "Epoch: 3/9 Iteration: 838 Training loss: 1.56714\n",
      "Epoch: 3/9 Iteration: 839 Training loss: 1.58095\n",
      "Epoch: 3/9 Iteration: 840 Training loss: 1.11111\n",
      "Epoch: 3/9 Iteration: 841 Training loss: 1.17328\n",
      "Epoch: 3/9 Iteration: 842 Training loss: 0.81724\n",
      "Epoch: 3/9 Iteration: 843 Training loss: 0.70638\n",
      "Epoch: 3/9 Iteration: 844 Training loss: 1.51009\n",
      "Epoch: 3/9 Iteration: 845 Training loss: 1.14418\n",
      "Epoch: 3/9 Iteration: 846 Training loss: 1.06427\n",
      "Epoch: 3/9 Iteration: 847 Training loss: 0.76774\n",
      "Epoch: 3/9 Iteration: 848 Training loss: 1.20152\n",
      "Epoch: 3/9 Iteration: 849 Training loss: 0.83794\n",
      "Epoch: 3/9 Iteration: 850 Training loss: 1.72869\n",
      "***\n",
      "Epoch: 3/9 Iteration: 850 Validation Acc: 0.6667\n",
      "***\n",
      "Epoch: 3/9 Iteration: 851 Training loss: 1.25944\n",
      "Epoch: 3/9 Iteration: 852 Training loss: 0.89973\n",
      "Epoch: 3/9 Iteration: 853 Training loss: 0.91208\n",
      "Epoch: 3/9 Iteration: 854 Training loss: 1.17881\n",
      "Epoch: 3/9 Iteration: 855 Training loss: 1.04131\n",
      "Epoch: 3/9 Iteration: 856 Training loss: 1.72208\n",
      "Epoch: 3/9 Iteration: 857 Training loss: 1.45182\n",
      "Epoch: 3/9 Iteration: 858 Training loss: 0.97180\n",
      "Epoch: 3/9 Iteration: 859 Training loss: 1.14552\n",
      "Epoch: 3/9 Iteration: 860 Training loss: 1.78183\n",
      "Epoch: 3/9 Iteration: 861 Training loss: 1.48993\n",
      "Epoch: 3/9 Iteration: 862 Training loss: 2.35062\n",
      "Epoch: 3/9 Iteration: 863 Training loss: 1.47090\n",
      "Epoch: 3/9 Iteration: 864 Training loss: 1.60405\n",
      "Epoch: 3/9 Iteration: 865 Training loss: 1.50091\n",
      "Epoch: 3/9 Iteration: 866 Training loss: 1.25472\n",
      "Epoch: 3/9 Iteration: 867 Training loss: 1.09494\n",
      "Epoch: 3/9 Iteration: 868 Training loss: 1.05595\n",
      "Epoch: 3/9 Iteration: 869 Training loss: 0.63304\n",
      "Epoch: 3/9 Iteration: 870 Training loss: 0.93043\n",
      "Epoch: 3/9 Iteration: 871 Training loss: 0.73926\n",
      "Epoch: 3/9 Iteration: 872 Training loss: 1.48441\n",
      "Epoch: 3/9 Iteration: 873 Training loss: 1.29084\n",
      "Epoch: 3/9 Iteration: 874 Training loss: 2.00662\n",
      "Epoch: 3/9 Iteration: 875 Training loss: 1.15513\n",
      "Epoch: 3/9 Iteration: 876 Training loss: 1.28470\n",
      "Epoch: 3/9 Iteration: 877 Training loss: 1.42327\n",
      "Epoch: 3/9 Iteration: 878 Training loss: 1.13029\n",
      "Epoch: 3/9 Iteration: 879 Training loss: 0.82129\n",
      "Epoch: 3/9 Iteration: 880 Training loss: 0.70533\n",
      "Epoch: 3/9 Iteration: 881 Training loss: 0.96887\n",
      "Epoch: 3/9 Iteration: 882 Training loss: 0.86987\n",
      "Epoch: 3/9 Iteration: 883 Training loss: 0.61129\n",
      "Epoch: 3/9 Iteration: 884 Training loss: 1.36722\n",
      "Epoch: 3/9 Iteration: 885 Training loss: 1.21218\n",
      "Epoch: 3/9 Iteration: 886 Training loss: 1.69304\n",
      "Epoch: 3/9 Iteration: 887 Training loss: 0.64256\n",
      "Epoch: 3/9 Iteration: 888 Training loss: 0.53960\n",
      "Epoch: 3/9 Iteration: 889 Training loss: 1.63830\n",
      "Epoch: 3/9 Iteration: 890 Training loss: 0.42965\n",
      "Epoch: 3/9 Iteration: 891 Training loss: 1.45734\n",
      "Epoch: 3/9 Iteration: 892 Training loss: 1.40607\n",
      "Epoch: 3/9 Iteration: 893 Training loss: 1.41956\n",
      "Epoch: 3/9 Iteration: 894 Training loss: 1.18308\n",
      "Epoch: 3/9 Iteration: 895 Training loss: 1.14298\n",
      "Epoch: 3/9 Iteration: 896 Training loss: 1.32174\n",
      "Epoch: 3/9 Iteration: 897 Training loss: 1.56548\n",
      "Epoch: 3/9 Iteration: 898 Training loss: 1.58946\n",
      "Epoch: 3/9 Iteration: 899 Training loss: 1.42966\n",
      "Epoch: 3/9 Iteration: 900 Training loss: 0.78695\n",
      "***\n",
      "Epoch: 3/9 Iteration: 900 Validation Acc: 0.6400\n",
      "***\n",
      "Epoch: 3/9 Iteration: 901 Training loss: 0.57613\n",
      "Epoch: 3/9 Iteration: 902 Training loss: 0.70412\n",
      "Epoch: 3/9 Iteration: 903 Training loss: 0.84417\n",
      "Epoch: 3/9 Iteration: 904 Training loss: 2.36037\n",
      "Epoch: 3/9 Iteration: 905 Training loss: 1.81483\n",
      "Epoch: 3/9 Iteration: 906 Training loss: 0.71087\n",
      "Epoch: 3/9 Iteration: 907 Training loss: 0.76691\n",
      "Epoch: 3/9 Iteration: 908 Training loss: 1.61907\n",
      "Epoch: 3/9 Iteration: 909 Training loss: 1.24044\n",
      "Epoch: 3/9 Iteration: 910 Training loss: 0.90059\n",
      "Epoch: 3/9 Iteration: 911 Training loss: 1.02613\n",
      "Epoch: 3/9 Iteration: 912 Training loss: 1.02482\n",
      "Epoch: 3/9 Iteration: 913 Training loss: 1.31770\n",
      "Epoch: 3/9 Iteration: 914 Training loss: 1.25709\n",
      "Epoch: 3/9 Iteration: 915 Training loss: 1.13213\n",
      "Epoch: 3/9 Iteration: 916 Training loss: 1.54032\n",
      "Epoch: 3/9 Iteration: 917 Training loss: 1.28761\n",
      "Epoch: 3/9 Iteration: 918 Training loss: 0.56254\n",
      "Epoch: 3/9 Iteration: 919 Training loss: 1.18784\n",
      "Epoch: 3/9 Iteration: 920 Training loss: 1.63329\n",
      "Epoch: 3/9 Iteration: 921 Training loss: 1.33298\n",
      "Epoch: 3/9 Iteration: 922 Training loss: 1.65673\n",
      "Epoch: 3/9 Iteration: 923 Training loss: 0.84528\n",
      "Epoch: 3/9 Iteration: 924 Training loss: 1.81903\n",
      "Epoch: 3/9 Iteration: 925 Training loss: 0.97135\n",
      "Epoch: 3/9 Iteration: 926 Training loss: 0.91253\n",
      "Epoch: 3/9 Iteration: 927 Training loss: 1.61548\n",
      "Epoch: 3/9 Iteration: 928 Training loss: 1.26013\n",
      "Epoch: 3/9 Iteration: 929 Training loss: 0.52604\n",
      "Epoch: 3/9 Iteration: 930 Training loss: 1.18782\n",
      "Epoch: 3/9 Iteration: 931 Training loss: 1.31758\n",
      "Epoch: 3/9 Iteration: 932 Training loss: 1.92110\n",
      "Epoch: 3/9 Iteration: 933 Training loss: 1.44363\n",
      "Epoch: 3/9 Iteration: 934 Training loss: 1.39422\n",
      "Epoch: 3/9 Iteration: 935 Training loss: 1.03933\n",
      "Epoch: 3/9 Iteration: 936 Training loss: 0.88027\n",
      "Epoch: 3/9 Iteration: 937 Training loss: 1.27874\n",
      "Epoch: 3/9 Iteration: 938 Training loss: 0.57371\n",
      "Epoch: 3/9 Iteration: 939 Training loss: 1.04740\n",
      "Epoch: 3/9 Iteration: 940 Training loss: 0.50933\n",
      "Epoch: 3/9 Iteration: 941 Training loss: 1.23072\n",
      "Epoch: 3/9 Iteration: 942 Training loss: 1.35184\n",
      "Epoch: 3/9 Iteration: 943 Training loss: 1.87499\n",
      "Epoch: 3/9 Iteration: 944 Training loss: 1.16738\n",
      "Epoch: 3/9 Iteration: 945 Training loss: 0.70904\n",
      "Epoch: 3/9 Iteration: 946 Training loss: 1.57168\n",
      "Epoch: 3/9 Iteration: 947 Training loss: 1.58230\n",
      "Epoch: 3/9 Iteration: 948 Training loss: 1.28049\n",
      "Epoch: 3/9 Iteration: 949 Training loss: 1.34700\n",
      "Epoch: 3/9 Iteration: 950 Training loss: 1.13989\n",
      "***\n",
      "Epoch: 3/9 Iteration: 950 Validation Acc: 0.6800\n",
      "***\n",
      "Epoch: 3/9 Iteration: 951 Training loss: 0.91092\n",
      "Epoch: 3/9 Iteration: 952 Training loss: 1.53489\n",
      "Epoch: 3/9 Iteration: 953 Training loss: 1.15154\n",
      "Epoch: 3/9 Iteration: 954 Training loss: 1.23506\n",
      "Epoch: 3/9 Iteration: 955 Training loss: 1.07957\n",
      "Epoch: 3/9 Iteration: 956 Training loss: 1.12148\n",
      "Epoch: 3/9 Iteration: 957 Training loss: 0.94488\n",
      "Epoch: 3/9 Iteration: 958 Training loss: 1.73503\n",
      "Epoch: 3/9 Iteration: 959 Training loss: 1.35701\n",
      "Epoch: 4/9 Iteration: 960 Training loss: 1.26443\n",
      "Epoch: 4/9 Iteration: 961 Training loss: 1.12451\n",
      "Epoch: 4/9 Iteration: 962 Training loss: 1.35315\n",
      "Epoch: 4/9 Iteration: 963 Training loss: 0.16046\n",
      "Epoch: 4/9 Iteration: 964 Training loss: 1.06854\n",
      "Epoch: 4/9 Iteration: 965 Training loss: 0.83701\n",
      "Epoch: 4/9 Iteration: 966 Training loss: 0.66540\n",
      "Epoch: 4/9 Iteration: 967 Training loss: 1.78154\n",
      "Epoch: 4/9 Iteration: 968 Training loss: 1.69738\n",
      "Epoch: 4/9 Iteration: 969 Training loss: 0.73564\n",
      "Epoch: 4/9 Iteration: 970 Training loss: 1.58688\n",
      "Epoch: 4/9 Iteration: 971 Training loss: 0.92974\n",
      "Epoch: 4/9 Iteration: 972 Training loss: 1.50678\n",
      "Epoch: 4/9 Iteration: 973 Training loss: 1.18511\n",
      "Epoch: 4/9 Iteration: 974 Training loss: 1.07488\n",
      "Epoch: 4/9 Iteration: 975 Training loss: 1.30053\n",
      "Epoch: 4/9 Iteration: 976 Training loss: 1.05855\n",
      "Epoch: 4/9 Iteration: 977 Training loss: 1.20459\n",
      "Epoch: 4/9 Iteration: 978 Training loss: 1.53892\n",
      "Epoch: 4/9 Iteration: 979 Training loss: 1.76461\n",
      "Epoch: 4/9 Iteration: 980 Training loss: 0.79046\n",
      "Epoch: 4/9 Iteration: 981 Training loss: 0.86824\n",
      "Epoch: 4/9 Iteration: 982 Training loss: 0.76747\n",
      "Epoch: 4/9 Iteration: 983 Training loss: 1.40543\n",
      "Epoch: 4/9 Iteration: 984 Training loss: 1.49512\n",
      "Epoch: 4/9 Iteration: 985 Training loss: 0.92211\n",
      "Epoch: 4/9 Iteration: 986 Training loss: 2.27190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/9 Iteration: 987 Training loss: 1.49344\n",
      "Epoch: 4/9 Iteration: 988 Training loss: 1.61061\n",
      "Epoch: 4/9 Iteration: 989 Training loss: 1.16575\n",
      "Epoch: 4/9 Iteration: 990 Training loss: 1.42832\n",
      "Epoch: 4/9 Iteration: 991 Training loss: 0.72896\n",
      "Epoch: 4/9 Iteration: 992 Training loss: 1.12483\n",
      "Epoch: 4/9 Iteration: 993 Training loss: 0.46686\n",
      "Epoch: 4/9 Iteration: 994 Training loss: 1.08646\n",
      "Epoch: 4/9 Iteration: 995 Training loss: 0.85131\n",
      "Epoch: 4/9 Iteration: 996 Training loss: 1.13507\n",
      "Epoch: 4/9 Iteration: 997 Training loss: 0.67506\n",
      "Epoch: 4/9 Iteration: 998 Training loss: 1.58367\n",
      "Epoch: 4/9 Iteration: 999 Training loss: 1.06711\n",
      "Epoch: 4/9 Iteration: 1000 Training loss: 1.38699\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1000 Validation Acc: 0.6500\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1001 Training loss: 1.18226\n",
      "Epoch: 4/9 Iteration: 1002 Training loss: 1.52994\n",
      "Epoch: 4/9 Iteration: 1003 Training loss: 1.65823\n",
      "Epoch: 4/9 Iteration: 1004 Training loss: 0.76066\n",
      "Epoch: 4/9 Iteration: 1005 Training loss: 1.26503\n",
      "Epoch: 4/9 Iteration: 1006 Training loss: 1.34135\n",
      "Epoch: 4/9 Iteration: 1007 Training loss: 1.10154\n",
      "Epoch: 4/9 Iteration: 1008 Training loss: 0.77063\n",
      "Epoch: 4/9 Iteration: 1009 Training loss: 0.80329\n",
      "Epoch: 4/9 Iteration: 1010 Training loss: 2.01167\n",
      "Epoch: 4/9 Iteration: 1011 Training loss: 0.97227\n",
      "Epoch: 4/9 Iteration: 1012 Training loss: 1.06740\n",
      "Epoch: 4/9 Iteration: 1013 Training loss: 0.93939\n",
      "Epoch: 4/9 Iteration: 1014 Training loss: 0.90630\n",
      "Epoch: 4/9 Iteration: 1015 Training loss: 0.72877\n",
      "Epoch: 4/9 Iteration: 1016 Training loss: 1.22115\n",
      "Epoch: 4/9 Iteration: 1017 Training loss: 1.08285\n",
      "Epoch: 4/9 Iteration: 1018 Training loss: 1.13932\n",
      "Epoch: 4/9 Iteration: 1019 Training loss: 0.80412\n",
      "Epoch: 4/9 Iteration: 1020 Training loss: 1.55476\n",
      "Epoch: 4/9 Iteration: 1021 Training loss: 0.58559\n",
      "Epoch: 4/9 Iteration: 1022 Training loss: 0.71712\n",
      "Epoch: 4/9 Iteration: 1023 Training loss: 1.18106\n",
      "Epoch: 4/9 Iteration: 1024 Training loss: 1.02648\n",
      "Epoch: 4/9 Iteration: 1025 Training loss: 0.77952\n",
      "Epoch: 4/9 Iteration: 1026 Training loss: 1.05580\n",
      "Epoch: 4/9 Iteration: 1027 Training loss: 1.01524\n",
      "Epoch: 4/9 Iteration: 1028 Training loss: 1.23140\n",
      "Epoch: 4/9 Iteration: 1029 Training loss: 0.78334\n",
      "Epoch: 4/9 Iteration: 1030 Training loss: 2.23081\n",
      "Epoch: 4/9 Iteration: 1031 Training loss: 1.65097\n",
      "Epoch: 4/9 Iteration: 1032 Training loss: 1.73445\n",
      "Epoch: 4/9 Iteration: 1033 Training loss: 0.84642\n",
      "Epoch: 4/9 Iteration: 1034 Training loss: 0.64119\n",
      "Epoch: 4/9 Iteration: 1035 Training loss: 0.74844\n",
      "Epoch: 4/9 Iteration: 1036 Training loss: 2.09625\n",
      "Epoch: 4/9 Iteration: 1037 Training loss: 1.10362\n",
      "Epoch: 4/9 Iteration: 1038 Training loss: 1.52782\n",
      "Epoch: 4/9 Iteration: 1039 Training loss: 1.76033\n",
      "Epoch: 4/9 Iteration: 1040 Training loss: 1.25024\n",
      "Epoch: 4/9 Iteration: 1041 Training loss: 2.01914\n",
      "Epoch: 4/9 Iteration: 1042 Training loss: 1.12430\n",
      "Epoch: 4/9 Iteration: 1043 Training loss: 0.89291\n",
      "Epoch: 4/9 Iteration: 1044 Training loss: 1.63879\n",
      "Epoch: 4/9 Iteration: 1045 Training loss: 1.29288\n",
      "Epoch: 4/9 Iteration: 1046 Training loss: 1.44147\n",
      "Epoch: 4/9 Iteration: 1047 Training loss: 0.74496\n",
      "Epoch: 4/9 Iteration: 1048 Training loss: 0.98362\n",
      "Epoch: 4/9 Iteration: 1049 Training loss: 1.44395\n",
      "Epoch: 4/9 Iteration: 1050 Training loss: 0.96110\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1050 Validation Acc: 0.7200\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1051 Training loss: 1.13328\n",
      "Epoch: 4/9 Iteration: 1052 Training loss: 1.03662\n",
      "Epoch: 4/9 Iteration: 1053 Training loss: 1.67541\n",
      "Epoch: 4/9 Iteration: 1054 Training loss: 1.11919\n",
      "Epoch: 4/9 Iteration: 1055 Training loss: 1.37858\n",
      "Epoch: 4/9 Iteration: 1056 Training loss: 1.64009\n",
      "Epoch: 4/9 Iteration: 1057 Training loss: 1.08868\n",
      "Epoch: 4/9 Iteration: 1058 Training loss: 1.16042\n",
      "Epoch: 4/9 Iteration: 1059 Training loss: 0.54369\n",
      "Epoch: 4/9 Iteration: 1060 Training loss: 1.03012\n",
      "Epoch: 4/9 Iteration: 1061 Training loss: 1.52851\n",
      "Epoch: 4/9 Iteration: 1062 Training loss: 0.85672\n",
      "Epoch: 4/9 Iteration: 1063 Training loss: 0.69007\n",
      "Epoch: 4/9 Iteration: 1064 Training loss: 1.05006\n",
      "Epoch: 4/9 Iteration: 1065 Training loss: 1.66919\n",
      "Epoch: 4/9 Iteration: 1066 Training loss: 0.78289\n",
      "Epoch: 4/9 Iteration: 1067 Training loss: 1.56262\n",
      "Epoch: 4/9 Iteration: 1068 Training loss: 1.40708\n",
      "Epoch: 4/9 Iteration: 1069 Training loss: 1.18559\n",
      "Epoch: 4/9 Iteration: 1070 Training loss: 1.00919\n",
      "Epoch: 4/9 Iteration: 1071 Training loss: 1.14685\n",
      "Epoch: 4/9 Iteration: 1072 Training loss: 1.08368\n",
      "Epoch: 4/9 Iteration: 1073 Training loss: 2.03513\n",
      "Epoch: 4/9 Iteration: 1074 Training loss: 1.09206\n",
      "Epoch: 4/9 Iteration: 1075 Training loss: 1.55984\n",
      "Epoch: 4/9 Iteration: 1076 Training loss: 0.94292\n",
      "Epoch: 4/9 Iteration: 1077 Training loss: 1.19060\n",
      "Epoch: 4/9 Iteration: 1078 Training loss: 1.29918\n",
      "Epoch: 4/9 Iteration: 1079 Training loss: 1.43195\n",
      "Epoch: 4/9 Iteration: 1080 Training loss: 0.37480\n",
      "Epoch: 4/9 Iteration: 1081 Training loss: 1.27084\n",
      "Epoch: 4/9 Iteration: 1082 Training loss: 0.78300\n",
      "Epoch: 4/9 Iteration: 1083 Training loss: 0.71939\n",
      "Epoch: 4/9 Iteration: 1084 Training loss: 1.20624\n",
      "Epoch: 4/9 Iteration: 1085 Training loss: 1.26431\n",
      "Epoch: 4/9 Iteration: 1086 Training loss: 0.85833\n",
      "Epoch: 4/9 Iteration: 1087 Training loss: 1.01420\n",
      "Epoch: 4/9 Iteration: 1088 Training loss: 0.73578\n",
      "Epoch: 4/9 Iteration: 1089 Training loss: 0.77568\n",
      "Epoch: 4/9 Iteration: 1090 Training loss: 1.46363\n",
      "Epoch: 4/9 Iteration: 1091 Training loss: 1.36185\n",
      "Epoch: 4/9 Iteration: 1092 Training loss: 0.74512\n",
      "Epoch: 4/9 Iteration: 1093 Training loss: 0.46616\n",
      "Epoch: 4/9 Iteration: 1094 Training loss: 0.93843\n",
      "Epoch: 4/9 Iteration: 1095 Training loss: 0.97259\n",
      "Epoch: 4/9 Iteration: 1096 Training loss: 1.23015\n",
      "Epoch: 4/9 Iteration: 1097 Training loss: 1.94581\n",
      "Epoch: 4/9 Iteration: 1098 Training loss: 0.72418\n",
      "Epoch: 4/9 Iteration: 1099 Training loss: 0.75479\n",
      "Epoch: 4/9 Iteration: 1100 Training loss: 2.08379\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1100 Validation Acc: 0.6767\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1101 Training loss: 0.86495\n",
      "Epoch: 4/9 Iteration: 1102 Training loss: 1.38281\n",
      "Epoch: 4/9 Iteration: 1103 Training loss: 1.10348\n",
      "Epoch: 4/9 Iteration: 1104 Training loss: 0.72955\n",
      "Epoch: 4/9 Iteration: 1105 Training loss: 1.09406\n",
      "Epoch: 4/9 Iteration: 1106 Training loss: 0.87670\n",
      "Epoch: 4/9 Iteration: 1107 Training loss: 1.20924\n",
      "Epoch: 4/9 Iteration: 1108 Training loss: 0.80257\n",
      "Epoch: 4/9 Iteration: 1109 Training loss: 0.89991\n",
      "Epoch: 4/9 Iteration: 1110 Training loss: 1.13661\n",
      "Epoch: 4/9 Iteration: 1111 Training loss: 1.13465\n",
      "Epoch: 4/9 Iteration: 1112 Training loss: 1.02395\n",
      "Epoch: 4/9 Iteration: 1113 Training loss: 1.02096\n",
      "Epoch: 4/9 Iteration: 1114 Training loss: 0.90211\n",
      "Epoch: 4/9 Iteration: 1115 Training loss: 1.13894\n",
      "Epoch: 4/9 Iteration: 1116 Training loss: 1.13010\n",
      "Epoch: 4/9 Iteration: 1117 Training loss: 0.73029\n",
      "Epoch: 4/9 Iteration: 1118 Training loss: 0.99630\n",
      "Epoch: 4/9 Iteration: 1119 Training loss: 1.41894\n",
      "Epoch: 4/9 Iteration: 1120 Training loss: 0.97694\n",
      "Epoch: 4/9 Iteration: 1121 Training loss: 0.80939\n",
      "Epoch: 4/9 Iteration: 1122 Training loss: 1.01519\n",
      "Epoch: 4/9 Iteration: 1123 Training loss: 1.09173\n",
      "Epoch: 4/9 Iteration: 1124 Training loss: 1.41750\n",
      "Epoch: 4/9 Iteration: 1125 Training loss: 0.90570\n",
      "Epoch: 4/9 Iteration: 1126 Training loss: 1.92746\n",
      "Epoch: 4/9 Iteration: 1127 Training loss: 0.56044\n",
      "Epoch: 4/9 Iteration: 1128 Training loss: 0.66036\n",
      "Epoch: 4/9 Iteration: 1129 Training loss: 1.22071\n",
      "Epoch: 4/9 Iteration: 1130 Training loss: 0.42361\n",
      "Epoch: 4/9 Iteration: 1131 Training loss: 0.98686\n",
      "Epoch: 4/9 Iteration: 1132 Training loss: 1.22205\n",
      "Epoch: 4/9 Iteration: 1133 Training loss: 1.19171\n",
      "Epoch: 4/9 Iteration: 1134 Training loss: 0.90775\n",
      "Epoch: 4/9 Iteration: 1135 Training loss: 1.62363\n",
      "Epoch: 4/9 Iteration: 1136 Training loss: 0.80069\n",
      "Epoch: 4/9 Iteration: 1137 Training loss: 1.99001\n",
      "Epoch: 4/9 Iteration: 1138 Training loss: 1.22340\n",
      "Epoch: 4/9 Iteration: 1139 Training loss: 1.08335\n",
      "Epoch: 4/9 Iteration: 1140 Training loss: 0.48996\n",
      "Epoch: 4/9 Iteration: 1141 Training loss: 1.13470\n",
      "Epoch: 4/9 Iteration: 1142 Training loss: 0.89551\n",
      "Epoch: 4/9 Iteration: 1143 Training loss: 0.84999\n",
      "Epoch: 4/9 Iteration: 1144 Training loss: 1.24910\n",
      "Epoch: 4/9 Iteration: 1145 Training loss: 1.53868\n",
      "Epoch: 4/9 Iteration: 1146 Training loss: 0.70021\n",
      "Epoch: 4/9 Iteration: 1147 Training loss: 0.77848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/9 Iteration: 1148 Training loss: 1.22525\n",
      "Epoch: 4/9 Iteration: 1149 Training loss: 1.32644\n",
      "Epoch: 4/9 Iteration: 1150 Training loss: 0.53302\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1150 Validation Acc: 0.6400\n",
      "***\n",
      "Epoch: 4/9 Iteration: 1151 Training loss: 0.88962\n",
      "Epoch: 4/9 Iteration: 1152 Training loss: 0.98794\n",
      "Epoch: 4/9 Iteration: 1153 Training loss: 1.32487\n",
      "Epoch: 4/9 Iteration: 1154 Training loss: 1.56813\n",
      "Epoch: 4/9 Iteration: 1155 Training loss: 1.64239\n",
      "Epoch: 4/9 Iteration: 1156 Training loss: 0.86047\n",
      "Epoch: 4/9 Iteration: 1157 Training loss: 0.80646\n",
      "Epoch: 4/9 Iteration: 1158 Training loss: 0.92258\n",
      "Epoch: 4/9 Iteration: 1159 Training loss: 0.55679\n",
      "Epoch: 4/9 Iteration: 1160 Training loss: 1.30920\n",
      "Epoch: 4/9 Iteration: 1161 Training loss: 1.24741\n",
      "Epoch: 4/9 Iteration: 1162 Training loss: 2.07951\n",
      "Epoch: 4/9 Iteration: 1163 Training loss: 1.07563\n",
      "Epoch: 4/9 Iteration: 1164 Training loss: 1.47857\n",
      "Epoch: 4/9 Iteration: 1165 Training loss: 1.11878\n",
      "Epoch: 4/9 Iteration: 1166 Training loss: 0.97675\n",
      "Epoch: 4/9 Iteration: 1167 Training loss: 0.76696\n",
      "Epoch: 4/9 Iteration: 1168 Training loss: 1.37635\n",
      "Epoch: 4/9 Iteration: 1169 Training loss: 0.42040\n",
      "Epoch: 4/9 Iteration: 1170 Training loss: 1.64930\n",
      "Epoch: 4/9 Iteration: 1171 Training loss: 0.77311\n",
      "Epoch: 4/9 Iteration: 1172 Training loss: 1.67731\n",
      "Epoch: 4/9 Iteration: 1173 Training loss: 0.99782\n",
      "Epoch: 4/9 Iteration: 1174 Training loss: 1.32564\n",
      "Epoch: 4/9 Iteration: 1175 Training loss: 1.66191\n",
      "Epoch: 4/9 Iteration: 1176 Training loss: 1.43188\n",
      "Epoch: 4/9 Iteration: 1177 Training loss: 1.17759\n",
      "Epoch: 4/9 Iteration: 1178 Training loss: 0.57940\n",
      "Epoch: 4/9 Iteration: 1179 Training loss: 1.10373\n",
      "Epoch: 4/9 Iteration: 1180 Training loss: 0.58345\n",
      "Epoch: 4/9 Iteration: 1181 Training loss: 1.53865\n",
      "Epoch: 4/9 Iteration: 1182 Training loss: 1.24413\n",
      "Epoch: 4/9 Iteration: 1183 Training loss: 1.24479\n",
      "Epoch: 4/9 Iteration: 1184 Training loss: 1.09394\n",
      "Epoch: 4/9 Iteration: 1185 Training loss: 0.71704\n",
      "Epoch: 4/9 Iteration: 1186 Training loss: 1.72774\n",
      "Epoch: 4/9 Iteration: 1187 Training loss: 0.85592\n",
      "Epoch: 4/9 Iteration: 1188 Training loss: 1.28203\n",
      "Epoch: 4/9 Iteration: 1189 Training loss: 1.12573\n",
      "Epoch: 4/9 Iteration: 1190 Training loss: 1.16889\n",
      "Epoch: 4/9 Iteration: 1191 Training loss: 0.81705\n",
      "Epoch: 4/9 Iteration: 1192 Training loss: 1.78715\n",
      "Epoch: 4/9 Iteration: 1193 Training loss: 0.91003\n",
      "Epoch: 4/9 Iteration: 1194 Training loss: 0.78665\n",
      "Epoch: 4/9 Iteration: 1195 Training loss: 0.92465\n",
      "Epoch: 4/9 Iteration: 1196 Training loss: 1.55015\n",
      "Epoch: 4/9 Iteration: 1197 Training loss: 0.70565\n",
      "Epoch: 4/9 Iteration: 1198 Training loss: 1.74151\n",
      "Epoch: 4/9 Iteration: 1199 Training loss: 1.56403\n",
      "Epoch: 5/9 Iteration: 1200 Training loss: 1.09104\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1200 Validation Acc: 0.7033\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1201 Training loss: 0.92992\n",
      "Epoch: 5/9 Iteration: 1202 Training loss: 1.16046\n",
      "Epoch: 5/9 Iteration: 1203 Training loss: 1.01223\n",
      "Epoch: 5/9 Iteration: 1204 Training loss: 1.21052\n",
      "Epoch: 5/9 Iteration: 1205 Training loss: 1.14289\n",
      "Epoch: 5/9 Iteration: 1206 Training loss: 0.89558\n",
      "Epoch: 5/9 Iteration: 1207 Training loss: 1.52678\n",
      "Epoch: 5/9 Iteration: 1208 Training loss: 1.45175\n",
      "Epoch: 5/9 Iteration: 1209 Training loss: 1.11159\n",
      "Epoch: 5/9 Iteration: 1210 Training loss: 1.26388\n",
      "Epoch: 5/9 Iteration: 1211 Training loss: 1.01161\n",
      "Epoch: 5/9 Iteration: 1212 Training loss: 1.31087\n",
      "Epoch: 5/9 Iteration: 1213 Training loss: 1.18244\n",
      "Epoch: 5/9 Iteration: 1214 Training loss: 0.60192\n",
      "Epoch: 5/9 Iteration: 1215 Training loss: 1.27169\n",
      "Epoch: 5/9 Iteration: 1216 Training loss: 0.89861\n",
      "Epoch: 5/9 Iteration: 1217 Training loss: 0.98926\n",
      "Epoch: 5/9 Iteration: 1218 Training loss: 1.20712\n",
      "Epoch: 5/9 Iteration: 1219 Training loss: 2.01729\n",
      "Epoch: 5/9 Iteration: 1220 Training loss: 1.08644\n",
      "Epoch: 5/9 Iteration: 1221 Training loss: 0.55054\n",
      "Epoch: 5/9 Iteration: 1222 Training loss: 1.29714\n",
      "Epoch: 5/9 Iteration: 1223 Training loss: 1.14388\n",
      "Epoch: 5/9 Iteration: 1224 Training loss: 0.93889\n",
      "Epoch: 5/9 Iteration: 1225 Training loss: 0.59764\n",
      "Epoch: 5/9 Iteration: 1226 Training loss: 1.91519\n",
      "Epoch: 5/9 Iteration: 1227 Training loss: 1.27145\n",
      "Epoch: 5/9 Iteration: 1228 Training loss: 1.23917\n",
      "Epoch: 5/9 Iteration: 1229 Training loss: 1.26540\n",
      "Epoch: 5/9 Iteration: 1230 Training loss: 0.82192\n",
      "Epoch: 5/9 Iteration: 1231 Training loss: 1.12603\n",
      "Epoch: 5/9 Iteration: 1232 Training loss: 1.17034\n",
      "Epoch: 5/9 Iteration: 1233 Training loss: 0.77293\n",
      "Epoch: 5/9 Iteration: 1234 Training loss: 1.47131\n",
      "Epoch: 5/9 Iteration: 1235 Training loss: 1.43416\n",
      "Epoch: 5/9 Iteration: 1236 Training loss: 0.57131\n",
      "Epoch: 5/9 Iteration: 1237 Training loss: 0.35981\n",
      "Epoch: 5/9 Iteration: 1238 Training loss: 1.19276\n",
      "Epoch: 5/9 Iteration: 1239 Training loss: 0.92017\n",
      "Epoch: 5/9 Iteration: 1240 Training loss: 1.11491\n",
      "Epoch: 5/9 Iteration: 1241 Training loss: 1.00325\n",
      "Epoch: 5/9 Iteration: 1242 Training loss: 1.33099\n",
      "Epoch: 5/9 Iteration: 1243 Training loss: 1.69814\n",
      "Epoch: 5/9 Iteration: 1244 Training loss: 0.88344\n",
      "Epoch: 5/9 Iteration: 1245 Training loss: 0.85413\n",
      "Epoch: 5/9 Iteration: 1246 Training loss: 0.93292\n",
      "Epoch: 5/9 Iteration: 1247 Training loss: 0.93516\n",
      "Epoch: 5/9 Iteration: 1248 Training loss: 0.54269\n",
      "Epoch: 5/9 Iteration: 1249 Training loss: 1.13924\n",
      "Epoch: 5/9 Iteration: 1250 Training loss: 1.81249\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1250 Validation Acc: 0.6000\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1251 Training loss: 1.43358\n",
      "Epoch: 5/9 Iteration: 1252 Training loss: 0.46408\n",
      "Epoch: 5/9 Iteration: 1253 Training loss: 0.81091\n",
      "Epoch: 5/9 Iteration: 1254 Training loss: 1.02553\n",
      "Epoch: 5/9 Iteration: 1255 Training loss: 0.73435\n",
      "Epoch: 5/9 Iteration: 1256 Training loss: 1.09771\n",
      "Epoch: 5/9 Iteration: 1257 Training loss: 0.85530\n",
      "Epoch: 5/9 Iteration: 1258 Training loss: 1.12439\n",
      "Epoch: 5/9 Iteration: 1259 Training loss: 1.26507\n",
      "Epoch: 5/9 Iteration: 1260 Training loss: 1.32641\n",
      "Epoch: 5/9 Iteration: 1261 Training loss: 0.41427\n",
      "Epoch: 5/9 Iteration: 1262 Training loss: 0.41074\n",
      "Epoch: 5/9 Iteration: 1263 Training loss: 1.90574\n",
      "Epoch: 5/9 Iteration: 1264 Training loss: 1.17317\n",
      "Epoch: 5/9 Iteration: 1265 Training loss: 1.77485\n",
      "Epoch: 5/9 Iteration: 1266 Training loss: 1.71240\n",
      "Epoch: 5/9 Iteration: 1267 Training loss: 1.38145\n",
      "Epoch: 5/9 Iteration: 1268 Training loss: 0.89988\n",
      "Epoch: 5/9 Iteration: 1269 Training loss: 1.04920\n",
      "Epoch: 5/9 Iteration: 1270 Training loss: 2.25548\n",
      "Epoch: 5/9 Iteration: 1271 Training loss: 1.60702\n",
      "Epoch: 5/9 Iteration: 1272 Training loss: 1.46834\n",
      "Epoch: 5/9 Iteration: 1273 Training loss: 0.87534\n",
      "Epoch: 5/9 Iteration: 1274 Training loss: 0.57103\n",
      "Epoch: 5/9 Iteration: 1275 Training loss: 0.82489\n",
      "Epoch: 5/9 Iteration: 1276 Training loss: 1.60192\n",
      "Epoch: 5/9 Iteration: 1277 Training loss: 1.49904\n",
      "Epoch: 5/9 Iteration: 1278 Training loss: 1.17179\n",
      "Epoch: 5/9 Iteration: 1279 Training loss: 2.22695\n",
      "Epoch: 5/9 Iteration: 1280 Training loss: 0.85215\n",
      "Epoch: 5/9 Iteration: 1281 Training loss: 1.78941\n",
      "Epoch: 5/9 Iteration: 1282 Training loss: 1.94529\n",
      "Epoch: 5/9 Iteration: 1283 Training loss: 1.61151\n",
      "Epoch: 5/9 Iteration: 1284 Training loss: 1.27258\n",
      "Epoch: 5/9 Iteration: 1285 Training loss: 0.77319\n",
      "Epoch: 5/9 Iteration: 1286 Training loss: 0.63852\n",
      "Epoch: 5/9 Iteration: 1287 Training loss: 0.63775\n",
      "Epoch: 5/9 Iteration: 1288 Training loss: 1.09071\n",
      "Epoch: 5/9 Iteration: 1289 Training loss: 1.63672\n",
      "Epoch: 5/9 Iteration: 1290 Training loss: 0.66293\n",
      "Epoch: 5/9 Iteration: 1291 Training loss: 0.79859\n",
      "Epoch: 5/9 Iteration: 1292 Training loss: 1.26536\n",
      "Epoch: 5/9 Iteration: 1293 Training loss: 1.18764\n",
      "Epoch: 5/9 Iteration: 1294 Training loss: 1.28951\n",
      "Epoch: 5/9 Iteration: 1295 Training loss: 1.58164\n",
      "Epoch: 5/9 Iteration: 1296 Training loss: 0.90893\n",
      "Epoch: 5/9 Iteration: 1297 Training loss: 0.97151\n",
      "Epoch: 5/9 Iteration: 1298 Training loss: 0.83299\n",
      "Epoch: 5/9 Iteration: 1299 Training loss: 0.67423\n",
      "Epoch: 5/9 Iteration: 1300 Training loss: 1.29887\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1300 Validation Acc: 0.6333\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1301 Training loss: 0.77280\n",
      "Epoch: 5/9 Iteration: 1302 Training loss: 1.52228\n",
      "Epoch: 5/9 Iteration: 1303 Training loss: 1.69178\n",
      "Epoch: 5/9 Iteration: 1304 Training loss: 0.98896\n",
      "Epoch: 5/9 Iteration: 1305 Training loss: 1.04690\n",
      "Epoch: 5/9 Iteration: 1306 Training loss: 1.57637\n",
      "Epoch: 5/9 Iteration: 1307 Training loss: 1.04392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/9 Iteration: 1308 Training loss: 0.68754\n",
      "Epoch: 5/9 Iteration: 1309 Training loss: 0.99356\n",
      "Epoch: 5/9 Iteration: 1310 Training loss: 0.61431\n",
      "Epoch: 5/9 Iteration: 1311 Training loss: 0.85667\n",
      "Epoch: 5/9 Iteration: 1312 Training loss: 1.20909\n",
      "Epoch: 5/9 Iteration: 1313 Training loss: 1.65187\n",
      "Epoch: 5/9 Iteration: 1314 Training loss: 1.09766\n",
      "Epoch: 5/9 Iteration: 1315 Training loss: 0.91389\n",
      "Epoch: 5/9 Iteration: 1316 Training loss: 0.86000\n",
      "Epoch: 5/9 Iteration: 1317 Training loss: 0.77069\n",
      "Epoch: 5/9 Iteration: 1318 Training loss: 1.21302\n",
      "Epoch: 5/9 Iteration: 1319 Training loss: 1.32044\n",
      "Epoch: 5/9 Iteration: 1320 Training loss: 0.48661\n",
      "Epoch: 5/9 Iteration: 1321 Training loss: 1.24106\n",
      "Epoch: 5/9 Iteration: 1322 Training loss: 0.66154\n",
      "Epoch: 5/9 Iteration: 1323 Training loss: 0.62244\n",
      "Epoch: 5/9 Iteration: 1324 Training loss: 1.16134\n",
      "Epoch: 5/9 Iteration: 1325 Training loss: 1.46212\n",
      "Epoch: 5/9 Iteration: 1326 Training loss: 1.04012\n",
      "Epoch: 5/9 Iteration: 1327 Training loss: 1.57785\n",
      "Epoch: 5/9 Iteration: 1328 Training loss: 0.62854\n",
      "Epoch: 5/9 Iteration: 1329 Training loss: 0.89941\n",
      "Epoch: 5/9 Iteration: 1330 Training loss: 1.32575\n",
      "Epoch: 5/9 Iteration: 1331 Training loss: 1.34764\n",
      "Epoch: 5/9 Iteration: 1332 Training loss: 0.86282\n",
      "Epoch: 5/9 Iteration: 1333 Training loss: 0.69964\n",
      "Epoch: 5/9 Iteration: 1334 Training loss: 1.06057\n",
      "Epoch: 5/9 Iteration: 1335 Training loss: 0.52820\n",
      "Epoch: 5/9 Iteration: 1336 Training loss: 1.17689\n",
      "Epoch: 5/9 Iteration: 1337 Training loss: 1.47696\n",
      "Epoch: 5/9 Iteration: 1338 Training loss: 0.65877\n",
      "Epoch: 5/9 Iteration: 1339 Training loss: 1.01592\n",
      "Epoch: 5/9 Iteration: 1340 Training loss: 1.91978\n",
      "Epoch: 5/9 Iteration: 1341 Training loss: 0.77279\n",
      "Epoch: 5/9 Iteration: 1342 Training loss: 1.19566\n",
      "Epoch: 5/9 Iteration: 1343 Training loss: 1.19122\n",
      "Epoch: 5/9 Iteration: 1344 Training loss: 0.72640\n",
      "Epoch: 5/9 Iteration: 1345 Training loss: 1.43139\n",
      "Epoch: 5/9 Iteration: 1346 Training loss: 1.38763\n",
      "Epoch: 5/9 Iteration: 1347 Training loss: 1.48212\n",
      "Epoch: 5/9 Iteration: 1348 Training loss: 0.64229\n",
      "Epoch: 5/9 Iteration: 1349 Training loss: 0.47389\n",
      "Epoch: 5/9 Iteration: 1350 Training loss: 0.70584\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1350 Validation Acc: 0.6267\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1351 Training loss: 1.19907\n",
      "Epoch: 5/9 Iteration: 1352 Training loss: 1.77308\n",
      "Epoch: 5/9 Iteration: 1353 Training loss: 0.61063\n",
      "Epoch: 5/9 Iteration: 1354 Training loss: 1.14867\n",
      "Epoch: 5/9 Iteration: 1355 Training loss: 0.84618\n",
      "Epoch: 5/9 Iteration: 1356 Training loss: 1.60451\n",
      "Epoch: 5/9 Iteration: 1357 Training loss: 1.14748\n",
      "Epoch: 5/9 Iteration: 1358 Training loss: 1.01388\n",
      "Epoch: 5/9 Iteration: 1359 Training loss: 1.03257\n",
      "Epoch: 5/9 Iteration: 1360 Training loss: 0.88942\n",
      "Epoch: 5/9 Iteration: 1361 Training loss: 1.12984\n",
      "Epoch: 5/9 Iteration: 1362 Training loss: 1.24994\n",
      "Epoch: 5/9 Iteration: 1363 Training loss: 0.97477\n",
      "Epoch: 5/9 Iteration: 1364 Training loss: 1.39660\n",
      "Epoch: 5/9 Iteration: 1365 Training loss: 1.33067\n",
      "Epoch: 5/9 Iteration: 1366 Training loss: 1.55789\n",
      "Epoch: 5/9 Iteration: 1367 Training loss: 0.85540\n",
      "Epoch: 5/9 Iteration: 1368 Training loss: 0.72243\n",
      "Epoch: 5/9 Iteration: 1369 Training loss: 1.31127\n",
      "Epoch: 5/9 Iteration: 1370 Training loss: 0.25024\n",
      "Epoch: 5/9 Iteration: 1371 Training loss: 0.78333\n",
      "Epoch: 5/9 Iteration: 1372 Training loss: 1.58239\n",
      "Epoch: 5/9 Iteration: 1373 Training loss: 0.97541\n",
      "Epoch: 5/9 Iteration: 1374 Training loss: 1.09358\n",
      "Epoch: 5/9 Iteration: 1375 Training loss: 1.48533\n",
      "Epoch: 5/9 Iteration: 1376 Training loss: 1.38230\n",
      "Epoch: 5/9 Iteration: 1377 Training loss: 1.57502\n",
      "Epoch: 5/9 Iteration: 1378 Training loss: 0.91432\n",
      "Epoch: 5/9 Iteration: 1379 Training loss: 0.78638\n",
      "Epoch: 5/9 Iteration: 1380 Training loss: 0.81357\n",
      "Epoch: 5/9 Iteration: 1381 Training loss: 1.37432\n",
      "Epoch: 5/9 Iteration: 1382 Training loss: 0.97846\n",
      "Epoch: 5/9 Iteration: 1383 Training loss: 1.35006\n",
      "Epoch: 5/9 Iteration: 1384 Training loss: 1.39847\n",
      "Epoch: 5/9 Iteration: 1385 Training loss: 1.22437\n",
      "Epoch: 5/9 Iteration: 1386 Training loss: 0.51992\n",
      "Epoch: 5/9 Iteration: 1387 Training loss: 0.56185\n",
      "Epoch: 5/9 Iteration: 1388 Training loss: 1.24465\n",
      "Epoch: 5/9 Iteration: 1389 Training loss: 1.75393\n",
      "Epoch: 5/9 Iteration: 1390 Training loss: 0.61356\n",
      "Epoch: 5/9 Iteration: 1391 Training loss: 0.61408\n",
      "Epoch: 5/9 Iteration: 1392 Training loss: 0.94713\n",
      "Epoch: 5/9 Iteration: 1393 Training loss: 0.96682\n",
      "Epoch: 5/9 Iteration: 1394 Training loss: 0.55549\n",
      "Epoch: 5/9 Iteration: 1395 Training loss: 0.68560\n",
      "Epoch: 5/9 Iteration: 1396 Training loss: 1.51189\n",
      "Epoch: 5/9 Iteration: 1397 Training loss: 0.84960\n",
      "Epoch: 5/9 Iteration: 1398 Training loss: 0.40887\n",
      "Epoch: 5/9 Iteration: 1399 Training loss: 1.09375\n",
      "Epoch: 5/9 Iteration: 1400 Training loss: 1.86458\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1400 Validation Acc: 0.7400\n",
      "***\n",
      "Epoch: 5/9 Iteration: 1401 Training loss: 1.94844\n",
      "Epoch: 5/9 Iteration: 1402 Training loss: 0.81351\n",
      "Epoch: 5/9 Iteration: 1403 Training loss: 1.12331\n",
      "Epoch: 5/9 Iteration: 1404 Training loss: 1.08472\n",
      "Epoch: 5/9 Iteration: 1405 Training loss: 1.53879\n",
      "Epoch: 5/9 Iteration: 1406 Training loss: 0.36263\n",
      "Epoch: 5/9 Iteration: 1407 Training loss: 1.32421\n",
      "Epoch: 5/9 Iteration: 1408 Training loss: 1.05388\n",
      "Epoch: 5/9 Iteration: 1409 Training loss: 0.45781\n",
      "Epoch: 5/9 Iteration: 1410 Training loss: 1.39475\n",
      "Epoch: 5/9 Iteration: 1411 Training loss: 1.49785\n",
      "Epoch: 5/9 Iteration: 1412 Training loss: 2.06799\n",
      "Epoch: 5/9 Iteration: 1413 Training loss: 1.16553\n",
      "Epoch: 5/9 Iteration: 1414 Training loss: 1.27612\n",
      "Epoch: 5/9 Iteration: 1415 Training loss: 1.04822\n",
      "Epoch: 5/9 Iteration: 1416 Training loss: 1.20814\n",
      "Epoch: 5/9 Iteration: 1417 Training loss: 1.08170\n",
      "Epoch: 5/9 Iteration: 1418 Training loss: 0.44451\n",
      "Epoch: 5/9 Iteration: 1419 Training loss: 0.83865\n",
      "Epoch: 5/9 Iteration: 1420 Training loss: 0.39134\n",
      "Epoch: 5/9 Iteration: 1421 Training loss: 1.42606\n",
      "Epoch: 5/9 Iteration: 1422 Training loss: 0.87087\n",
      "Epoch: 5/9 Iteration: 1423 Training loss: 1.87707\n",
      "Epoch: 5/9 Iteration: 1424 Training loss: 0.76070\n",
      "Epoch: 5/9 Iteration: 1425 Training loss: 0.45912\n",
      "Epoch: 5/9 Iteration: 1426 Training loss: 1.13633\n",
      "Epoch: 5/9 Iteration: 1427 Training loss: 0.75167\n",
      "Epoch: 5/9 Iteration: 1428 Training loss: 1.32705\n",
      "Epoch: 5/9 Iteration: 1429 Training loss: 1.13461\n",
      "Epoch: 5/9 Iteration: 1430 Training loss: 1.57129\n",
      "Epoch: 5/9 Iteration: 1431 Training loss: 0.70193\n",
      "Epoch: 5/9 Iteration: 1432 Training loss: 1.65791\n",
      "Epoch: 5/9 Iteration: 1433 Training loss: 0.74586\n",
      "Epoch: 5/9 Iteration: 1434 Training loss: 1.18678\n",
      "Epoch: 5/9 Iteration: 1435 Training loss: 1.48692\n",
      "Epoch: 5/9 Iteration: 1436 Training loss: 1.47985\n",
      "Epoch: 5/9 Iteration: 1437 Training loss: 0.60580\n",
      "Epoch: 5/9 Iteration: 1438 Training loss: 1.51214\n",
      "Epoch: 5/9 Iteration: 1439 Training loss: 1.30294\n",
      "Epoch: 6/9 Iteration: 1440 Training loss: 1.38525\n",
      "Epoch: 6/9 Iteration: 1441 Training loss: 1.41529\n",
      "Epoch: 6/9 Iteration: 1442 Training loss: 2.00773\n",
      "Epoch: 6/9 Iteration: 1443 Training loss: 0.59688\n",
      "Epoch: 6/9 Iteration: 1444 Training loss: 1.94468\n",
      "Epoch: 6/9 Iteration: 1445 Training loss: 1.17204\n",
      "Epoch: 6/9 Iteration: 1446 Training loss: 0.72952\n",
      "Epoch: 6/9 Iteration: 1447 Training loss: 1.64714\n",
      "Epoch: 6/9 Iteration: 1448 Training loss: 1.50916\n",
      "Epoch: 6/9 Iteration: 1449 Training loss: 1.31854\n",
      "Epoch: 6/9 Iteration: 1450 Training loss: 1.47050\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1450 Validation Acc: 0.7267\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1451 Training loss: 0.99505\n",
      "Epoch: 6/9 Iteration: 1452 Training loss: 0.96289\n",
      "Epoch: 6/9 Iteration: 1453 Training loss: 0.83950\n",
      "Epoch: 6/9 Iteration: 1454 Training loss: 0.74369\n",
      "Epoch: 6/9 Iteration: 1455 Training loss: 1.08256\n",
      "Epoch: 6/9 Iteration: 1456 Training loss: 1.07934\n",
      "Epoch: 6/9 Iteration: 1457 Training loss: 0.70218\n",
      "Epoch: 6/9 Iteration: 1458 Training loss: 1.31369\n",
      "Epoch: 6/9 Iteration: 1459 Training loss: 2.02215\n",
      "Epoch: 6/9 Iteration: 1460 Training loss: 1.04783\n",
      "Epoch: 6/9 Iteration: 1461 Training loss: 1.57064\n",
      "Epoch: 6/9 Iteration: 1462 Training loss: 0.55459\n",
      "Epoch: 6/9 Iteration: 1463 Training loss: 0.76391\n",
      "Epoch: 6/9 Iteration: 1464 Training loss: 1.11621\n",
      "Epoch: 6/9 Iteration: 1465 Training loss: 1.35840\n",
      "Epoch: 6/9 Iteration: 1466 Training loss: 1.82548\n",
      "Epoch: 6/9 Iteration: 1467 Training loss: 0.87820\n",
      "Epoch: 6/9 Iteration: 1468 Training loss: 1.38829\n",
      "Epoch: 6/9 Iteration: 1469 Training loss: 2.21551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/9 Iteration: 1470 Training loss: 0.96308\n",
      "Epoch: 6/9 Iteration: 1471 Training loss: 0.98007\n",
      "Epoch: 6/9 Iteration: 1472 Training loss: 1.13795\n",
      "Epoch: 6/9 Iteration: 1473 Training loss: 0.62020\n",
      "Epoch: 6/9 Iteration: 1474 Training loss: 1.07188\n",
      "Epoch: 6/9 Iteration: 1475 Training loss: 1.47474\n",
      "Epoch: 6/9 Iteration: 1476 Training loss: 0.77979\n",
      "Epoch: 6/9 Iteration: 1477 Training loss: 1.02251\n",
      "Epoch: 6/9 Iteration: 1478 Training loss: 1.20012\n",
      "Epoch: 6/9 Iteration: 1479 Training loss: 0.57808\n",
      "Epoch: 6/9 Iteration: 1480 Training loss: 1.53513\n",
      "Epoch: 6/9 Iteration: 1481 Training loss: 0.71409\n",
      "Epoch: 6/9 Iteration: 1482 Training loss: 0.90721\n",
      "Epoch: 6/9 Iteration: 1483 Training loss: 1.39708\n",
      "Epoch: 6/9 Iteration: 1484 Training loss: 0.93482\n",
      "Epoch: 6/9 Iteration: 1485 Training loss: 1.17775\n",
      "Epoch: 6/9 Iteration: 1486 Training loss: 0.80180\n",
      "Epoch: 6/9 Iteration: 1487 Training loss: 1.12205\n",
      "Epoch: 6/9 Iteration: 1488 Training loss: 0.46571\n",
      "Epoch: 6/9 Iteration: 1489 Training loss: 0.67156\n",
      "Epoch: 6/9 Iteration: 1490 Training loss: 1.41544\n",
      "Epoch: 6/9 Iteration: 1491 Training loss: 1.04499\n",
      "Epoch: 6/9 Iteration: 1492 Training loss: 0.72055\n",
      "Epoch: 6/9 Iteration: 1493 Training loss: 0.68201\n",
      "Epoch: 6/9 Iteration: 1494 Training loss: 0.74633\n",
      "Epoch: 6/9 Iteration: 1495 Training loss: 0.38086\n",
      "Epoch: 6/9 Iteration: 1496 Training loss: 1.19217\n",
      "Epoch: 6/9 Iteration: 1497 Training loss: 1.29182\n",
      "Epoch: 6/9 Iteration: 1498 Training loss: 0.77603\n",
      "Epoch: 6/9 Iteration: 1499 Training loss: 0.96118\n",
      "Epoch: 6/9 Iteration: 1500 Training loss: 1.44399\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1500 Validation Acc: 0.7000\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1501 Training loss: 0.50025\n",
      "Epoch: 6/9 Iteration: 1502 Training loss: 0.67962\n",
      "Epoch: 6/9 Iteration: 1503 Training loss: 1.18637\n",
      "Epoch: 6/9 Iteration: 1504 Training loss: 1.03432\n",
      "Epoch: 6/9 Iteration: 1505 Training loss: 1.04681\n",
      "Epoch: 6/9 Iteration: 1506 Training loss: 1.01224\n",
      "Epoch: 6/9 Iteration: 1507 Training loss: 1.07708\n",
      "Epoch: 6/9 Iteration: 1508 Training loss: 0.56132\n",
      "Epoch: 6/9 Iteration: 1509 Training loss: 0.47291\n",
      "Epoch: 6/9 Iteration: 1510 Training loss: 2.55028\n",
      "Epoch: 6/9 Iteration: 1511 Training loss: 1.14715\n",
      "Epoch: 6/9 Iteration: 1512 Training loss: 1.34083\n",
      "Epoch: 6/9 Iteration: 1513 Training loss: 1.16364\n",
      "Epoch: 6/9 Iteration: 1514 Training loss: 0.79922\n",
      "Epoch: 6/9 Iteration: 1515 Training loss: 1.01919\n",
      "Epoch: 6/9 Iteration: 1516 Training loss: 1.33625\n",
      "Epoch: 6/9 Iteration: 1517 Training loss: 1.57803\n",
      "Epoch: 6/9 Iteration: 1518 Training loss: 0.79605\n",
      "Epoch: 6/9 Iteration: 1519 Training loss: 1.14597\n",
      "Epoch: 6/9 Iteration: 1520 Training loss: 1.13275\n",
      "Epoch: 6/9 Iteration: 1521 Training loss: 1.53258\n",
      "Epoch: 6/9 Iteration: 1522 Training loss: 0.97906\n",
      "Epoch: 6/9 Iteration: 1523 Training loss: 1.63177\n",
      "Epoch: 6/9 Iteration: 1524 Training loss: 1.75432\n",
      "Epoch: 6/9 Iteration: 1525 Training loss: 0.94655\n",
      "Epoch: 6/9 Iteration: 1526 Training loss: 0.96549\n",
      "Epoch: 6/9 Iteration: 1527 Training loss: 0.67244\n",
      "Epoch: 6/9 Iteration: 1528 Training loss: 1.18277\n",
      "Epoch: 6/9 Iteration: 1529 Training loss: 1.56687\n",
      "Epoch: 6/9 Iteration: 1530 Training loss: 1.04720\n",
      "Epoch: 6/9 Iteration: 1531 Training loss: 0.78297\n",
      "Epoch: 6/9 Iteration: 1532 Training loss: 0.66058\n",
      "Epoch: 6/9 Iteration: 1533 Training loss: 0.80447\n",
      "Epoch: 6/9 Iteration: 1534 Training loss: 1.30629\n",
      "Epoch: 6/9 Iteration: 1535 Training loss: 1.45470\n",
      "Epoch: 6/9 Iteration: 1536 Training loss: 1.14345\n",
      "Epoch: 6/9 Iteration: 1537 Training loss: 1.22200\n",
      "Epoch: 6/9 Iteration: 1538 Training loss: 0.50075\n",
      "Epoch: 6/9 Iteration: 1539 Training loss: 0.58450\n",
      "Epoch: 6/9 Iteration: 1540 Training loss: 1.17325\n",
      "Epoch: 6/9 Iteration: 1541 Training loss: 0.89628\n",
      "Epoch: 6/9 Iteration: 1542 Training loss: 1.03639\n",
      "Epoch: 6/9 Iteration: 1543 Training loss: 1.01805\n",
      "Epoch: 6/9 Iteration: 1544 Training loss: 0.48092\n",
      "Epoch: 6/9 Iteration: 1545 Training loss: 1.09749\n",
      "Epoch: 6/9 Iteration: 1546 Training loss: 0.91592\n",
      "Epoch: 6/9 Iteration: 1547 Training loss: 1.64731\n",
      "Epoch: 6/9 Iteration: 1548 Training loss: 0.88729\n",
      "Epoch: 6/9 Iteration: 1549 Training loss: 0.44138\n",
      "Epoch: 6/9 Iteration: 1550 Training loss: 0.57551\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1550 Validation Acc: 0.7167\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1551 Training loss: 0.47152\n",
      "Epoch: 6/9 Iteration: 1552 Training loss: 1.77297\n",
      "Epoch: 6/9 Iteration: 1553 Training loss: 1.26488\n",
      "Epoch: 6/9 Iteration: 1554 Training loss: 0.92729\n",
      "Epoch: 6/9 Iteration: 1555 Training loss: 0.91304\n",
      "Epoch: 6/9 Iteration: 1556 Training loss: 0.79543\n",
      "Epoch: 6/9 Iteration: 1557 Training loss: 0.69906\n",
      "Epoch: 6/9 Iteration: 1558 Training loss: 1.39195\n",
      "Epoch: 6/9 Iteration: 1559 Training loss: 0.67972\n",
      "Epoch: 6/9 Iteration: 1560 Training loss: 0.52058\n",
      "Epoch: 6/9 Iteration: 1561 Training loss: 0.80782\n",
      "Epoch: 6/9 Iteration: 1562 Training loss: 0.86541\n",
      "Epoch: 6/9 Iteration: 1563 Training loss: 0.46581\n",
      "Epoch: 6/9 Iteration: 1564 Training loss: 1.14173\n",
      "Epoch: 6/9 Iteration: 1565 Training loss: 1.28615\n",
      "Epoch: 6/9 Iteration: 1566 Training loss: 0.68797\n",
      "Epoch: 6/9 Iteration: 1567 Training loss: 1.34174\n",
      "Epoch: 6/9 Iteration: 1568 Training loss: 1.17481\n",
      "Epoch: 6/9 Iteration: 1569 Training loss: 0.67883\n",
      "Epoch: 6/9 Iteration: 1570 Training loss: 1.14711\n",
      "Epoch: 6/9 Iteration: 1571 Training loss: 1.50902\n",
      "Epoch: 6/9 Iteration: 1572 Training loss: 0.92559\n",
      "Epoch: 6/9 Iteration: 1573 Training loss: 0.98505\n",
      "Epoch: 6/9 Iteration: 1574 Training loss: 0.43170\n",
      "Epoch: 6/9 Iteration: 1575 Training loss: 0.47475\n",
      "Epoch: 6/9 Iteration: 1576 Training loss: 1.34853\n",
      "Epoch: 6/9 Iteration: 1577 Training loss: 1.52239\n",
      "Epoch: 6/9 Iteration: 1578 Training loss: 0.29365\n",
      "Epoch: 6/9 Iteration: 1579 Training loss: 0.96804\n",
      "Epoch: 6/9 Iteration: 1580 Training loss: 1.68753\n",
      "Epoch: 6/9 Iteration: 1581 Training loss: 0.53026\n",
      "Epoch: 6/9 Iteration: 1582 Training loss: 0.75983\n",
      "Epoch: 6/9 Iteration: 1583 Training loss: 0.73586\n",
      "Epoch: 6/9 Iteration: 1584 Training loss: 0.35926\n",
      "Epoch: 6/9 Iteration: 1585 Training loss: 1.16789\n",
      "Epoch: 6/9 Iteration: 1586 Training loss: 0.88492\n",
      "Epoch: 6/9 Iteration: 1587 Training loss: 1.57336\n",
      "Epoch: 6/9 Iteration: 1588 Training loss: 0.51431\n",
      "Epoch: 6/9 Iteration: 1589 Training loss: 0.32832\n",
      "Epoch: 6/9 Iteration: 1590 Training loss: 1.53434\n",
      "Epoch: 6/9 Iteration: 1591 Training loss: 0.82619\n",
      "Epoch: 6/9 Iteration: 1592 Training loss: 0.86728\n",
      "Epoch: 6/9 Iteration: 1593 Training loss: 0.91129\n",
      "Epoch: 6/9 Iteration: 1594 Training loss: 1.78526\n",
      "Epoch: 6/9 Iteration: 1595 Training loss: 0.63726\n",
      "Epoch: 6/9 Iteration: 1596 Training loss: 1.06931\n",
      "Epoch: 6/9 Iteration: 1597 Training loss: 0.72746\n",
      "Epoch: 6/9 Iteration: 1598 Training loss: 0.68243\n",
      "Epoch: 6/9 Iteration: 1599 Training loss: 0.74829\n",
      "Epoch: 6/9 Iteration: 1600 Training loss: 0.80394\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1600 Validation Acc: 0.6800\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1601 Training loss: 0.63328\n",
      "Epoch: 6/9 Iteration: 1602 Training loss: 0.60597\n",
      "Epoch: 6/9 Iteration: 1603 Training loss: 0.88315\n",
      "Epoch: 6/9 Iteration: 1604 Training loss: 1.16983\n",
      "Epoch: 6/9 Iteration: 1605 Training loss: 0.94215\n",
      "Epoch: 6/9 Iteration: 1606 Training loss: 1.58357\n",
      "Epoch: 6/9 Iteration: 1607 Training loss: 0.47676\n",
      "Epoch: 6/9 Iteration: 1608 Training loss: 0.71022\n",
      "Epoch: 6/9 Iteration: 1609 Training loss: 1.39326\n",
      "Epoch: 6/9 Iteration: 1610 Training loss: 0.42559\n",
      "Epoch: 6/9 Iteration: 1611 Training loss: 0.86590\n",
      "Epoch: 6/9 Iteration: 1612 Training loss: 1.26361\n",
      "Epoch: 6/9 Iteration: 1613 Training loss: 1.27288\n",
      "Epoch: 6/9 Iteration: 1614 Training loss: 0.83833\n",
      "Epoch: 6/9 Iteration: 1615 Training loss: 1.78640\n",
      "Epoch: 6/9 Iteration: 1616 Training loss: 1.80803\n",
      "Epoch: 6/9 Iteration: 1617 Training loss: 1.72439\n",
      "Epoch: 6/9 Iteration: 1618 Training loss: 1.25262\n",
      "Epoch: 6/9 Iteration: 1619 Training loss: 1.05097\n",
      "Epoch: 6/9 Iteration: 1620 Training loss: 0.73543\n",
      "Epoch: 6/9 Iteration: 1621 Training loss: 0.74573\n",
      "Epoch: 6/9 Iteration: 1622 Training loss: 0.54324\n",
      "Epoch: 6/9 Iteration: 1623 Training loss: 1.08383\n",
      "Epoch: 6/9 Iteration: 1624 Training loss: 1.47332\n",
      "Epoch: 6/9 Iteration: 1625 Training loss: 2.25553\n",
      "Epoch: 6/9 Iteration: 1626 Training loss: 1.16873\n",
      "Epoch: 6/9 Iteration: 1627 Training loss: 0.89793\n",
      "Epoch: 6/9 Iteration: 1628 Training loss: 0.99720\n",
      "Epoch: 6/9 Iteration: 1629 Training loss: 1.12206\n",
      "Epoch: 6/9 Iteration: 1630 Training loss: 0.99081\n",
      "Epoch: 6/9 Iteration: 1631 Training loss: 0.95847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/9 Iteration: 1632 Training loss: 0.67933\n",
      "Epoch: 6/9 Iteration: 1633 Training loss: 0.88349\n",
      "Epoch: 6/9 Iteration: 1634 Training loss: 1.14237\n",
      "Epoch: 6/9 Iteration: 1635 Training loss: 1.35627\n",
      "Epoch: 6/9 Iteration: 1636 Training loss: 1.68715\n",
      "Epoch: 6/9 Iteration: 1637 Training loss: 0.85792\n",
      "Epoch: 6/9 Iteration: 1638 Training loss: 0.63887\n",
      "Epoch: 6/9 Iteration: 1639 Training loss: 0.70394\n",
      "Epoch: 6/9 Iteration: 1640 Training loss: 1.28119\n",
      "Epoch: 6/9 Iteration: 1641 Training loss: 1.09602\n",
      "Epoch: 6/9 Iteration: 1642 Training loss: 1.69469\n",
      "Epoch: 6/9 Iteration: 1643 Training loss: 0.90107\n",
      "Epoch: 6/9 Iteration: 1644 Training loss: 1.68536\n",
      "Epoch: 6/9 Iteration: 1645 Training loss: 1.83995\n",
      "Epoch: 6/9 Iteration: 1646 Training loss: 0.74563\n",
      "Epoch: 6/9 Iteration: 1647 Training loss: 0.76344\n",
      "Epoch: 6/9 Iteration: 1648 Training loss: 1.16938\n",
      "Epoch: 6/9 Iteration: 1649 Training loss: 0.62482\n",
      "Epoch: 6/9 Iteration: 1650 Training loss: 1.18080\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1650 Validation Acc: 0.7000\n",
      "***\n",
      "Epoch: 6/9 Iteration: 1651 Training loss: 0.63441\n",
      "Epoch: 6/9 Iteration: 1652 Training loss: 1.79943\n",
      "Epoch: 6/9 Iteration: 1653 Training loss: 0.96097\n",
      "Epoch: 6/9 Iteration: 1654 Training loss: 1.12308\n",
      "Epoch: 6/9 Iteration: 1655 Training loss: 0.95832\n",
      "Epoch: 6/9 Iteration: 1656 Training loss: 1.01649\n",
      "Epoch: 6/9 Iteration: 1657 Training loss: 0.80113\n",
      "Epoch: 6/9 Iteration: 1658 Training loss: 0.90662\n",
      "Epoch: 6/9 Iteration: 1659 Training loss: 0.83658\n",
      "Epoch: 6/9 Iteration: 1660 Training loss: 0.65793\n",
      "Epoch: 6/9 Iteration: 1661 Training loss: 1.42121\n",
      "Epoch: 6/9 Iteration: 1662 Training loss: 1.32392\n",
      "Epoch: 6/9 Iteration: 1663 Training loss: 1.10070\n",
      "Epoch: 6/9 Iteration: 1664 Training loss: 0.52473\n",
      "Epoch: 6/9 Iteration: 1665 Training loss: 0.57779\n",
      "Epoch: 6/9 Iteration: 1666 Training loss: 1.71748\n",
      "Epoch: 6/9 Iteration: 1667 Training loss: 0.86377\n",
      "Epoch: 6/9 Iteration: 1668 Training loss: 0.81083\n",
      "Epoch: 6/9 Iteration: 1669 Training loss: 0.87383\n",
      "Epoch: 6/9 Iteration: 1670 Training loss: 1.10397\n",
      "Epoch: 6/9 Iteration: 1671 Training loss: 0.87581\n",
      "Epoch: 6/9 Iteration: 1672 Training loss: 1.33481\n",
      "Epoch: 6/9 Iteration: 1673 Training loss: 1.40743\n",
      "Epoch: 6/9 Iteration: 1674 Training loss: 0.58573\n",
      "Epoch: 6/9 Iteration: 1675 Training loss: 1.53997\n",
      "Epoch: 6/9 Iteration: 1676 Training loss: 1.43940\n",
      "Epoch: 6/9 Iteration: 1677 Training loss: 1.08212\n",
      "Epoch: 6/9 Iteration: 1678 Training loss: 1.70110\n",
      "Epoch: 6/9 Iteration: 1679 Training loss: 1.23296\n",
      "Epoch: 7/9 Iteration: 1680 Training loss: 0.94757\n",
      "Epoch: 7/9 Iteration: 1681 Training loss: 1.20028\n",
      "Epoch: 7/9 Iteration: 1682 Training loss: 1.62323\n",
      "Epoch: 7/9 Iteration: 1683 Training loss: 0.09930\n",
      "Epoch: 7/9 Iteration: 1684 Training loss: 2.15227\n",
      "Epoch: 7/9 Iteration: 1685 Training loss: 0.93500\n",
      "Epoch: 7/9 Iteration: 1686 Training loss: 1.08311\n",
      "Epoch: 7/9 Iteration: 1687 Training loss: 1.99760\n",
      "Epoch: 7/9 Iteration: 1688 Training loss: 1.07238\n",
      "Epoch: 7/9 Iteration: 1689 Training loss: 1.24762\n",
      "Epoch: 7/9 Iteration: 1690 Training loss: 1.36484\n",
      "Epoch: 7/9 Iteration: 1691 Training loss: 0.67633\n",
      "Epoch: 7/9 Iteration: 1692 Training loss: 0.99442\n",
      "Epoch: 7/9 Iteration: 1693 Training loss: 1.08036\n",
      "Epoch: 7/9 Iteration: 1694 Training loss: 0.62575\n",
      "Epoch: 7/9 Iteration: 1695 Training loss: 1.37529\n",
      "Epoch: 7/9 Iteration: 1696 Training loss: 1.17291\n",
      "Epoch: 7/9 Iteration: 1697 Training loss: 0.87612\n",
      "Epoch: 7/9 Iteration: 1698 Training loss: 1.78852\n",
      "Epoch: 7/9 Iteration: 1699 Training loss: 1.25856\n",
      "Epoch: 7/9 Iteration: 1700 Training loss: 1.48091\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1700 Validation Acc: 0.6500\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1701 Training loss: 0.97528\n",
      "Epoch: 7/9 Iteration: 1702 Training loss: 0.80570\n",
      "Epoch: 7/9 Iteration: 1703 Training loss: 1.04835\n",
      "Epoch: 7/9 Iteration: 1704 Training loss: 1.12399\n",
      "Epoch: 7/9 Iteration: 1705 Training loss: 0.68995\n",
      "Epoch: 7/9 Iteration: 1706 Training loss: 1.34095\n",
      "Epoch: 7/9 Iteration: 1707 Training loss: 1.04462\n",
      "Epoch: 7/9 Iteration: 1708 Training loss: 1.27765\n",
      "Epoch: 7/9 Iteration: 1709 Training loss: 1.10061\n",
      "Epoch: 7/9 Iteration: 1710 Training loss: 0.85593\n",
      "Epoch: 7/9 Iteration: 1711 Training loss: 1.38889\n",
      "Epoch: 7/9 Iteration: 1712 Training loss: 1.03222\n",
      "Epoch: 7/9 Iteration: 1713 Training loss: 0.36755\n",
      "Epoch: 7/9 Iteration: 1714 Training loss: 1.23936\n",
      "Epoch: 7/9 Iteration: 1715 Training loss: 1.22583\n",
      "Epoch: 7/9 Iteration: 1716 Training loss: 0.39888\n",
      "Epoch: 7/9 Iteration: 1717 Training loss: 0.87397\n",
      "Epoch: 7/9 Iteration: 1718 Training loss: 1.06677\n",
      "Epoch: 7/9 Iteration: 1719 Training loss: 0.85571\n",
      "Epoch: 7/9 Iteration: 1720 Training loss: 1.02436\n",
      "Epoch: 7/9 Iteration: 1721 Training loss: 1.81455\n",
      "Epoch: 7/9 Iteration: 1722 Training loss: 1.50593\n",
      "Epoch: 7/9 Iteration: 1723 Training loss: 1.56143\n",
      "Epoch: 7/9 Iteration: 1724 Training loss: 1.24169\n",
      "Epoch: 7/9 Iteration: 1725 Training loss: 0.94624\n",
      "Epoch: 7/9 Iteration: 1726 Training loss: 0.81351\n",
      "Epoch: 7/9 Iteration: 1727 Training loss: 0.62832\n",
      "Epoch: 7/9 Iteration: 1728 Training loss: 0.77344\n",
      "Epoch: 7/9 Iteration: 1729 Training loss: 0.97321\n",
      "Epoch: 7/9 Iteration: 1730 Training loss: 1.42942\n",
      "Epoch: 7/9 Iteration: 1731 Training loss: 1.32977\n",
      "Epoch: 7/9 Iteration: 1732 Training loss: 0.66634\n",
      "Epoch: 7/9 Iteration: 1733 Training loss: 0.89780\n",
      "Epoch: 7/9 Iteration: 1734 Training loss: 0.67725\n",
      "Epoch: 7/9 Iteration: 1735 Training loss: 0.64182\n",
      "Epoch: 7/9 Iteration: 1736 Training loss: 0.97681\n",
      "Epoch: 7/9 Iteration: 1737 Training loss: 0.99979\n",
      "Epoch: 7/9 Iteration: 1738 Training loss: 0.72842\n",
      "Epoch: 7/9 Iteration: 1739 Training loss: 0.63722\n",
      "Epoch: 7/9 Iteration: 1740 Training loss: 1.55090\n",
      "Epoch: 7/9 Iteration: 1741 Training loss: 0.89072\n",
      "Epoch: 7/9 Iteration: 1742 Training loss: 1.19059\n",
      "Epoch: 7/9 Iteration: 1743 Training loss: 1.11485\n",
      "Epoch: 7/9 Iteration: 1744 Training loss: 0.97948\n",
      "Epoch: 7/9 Iteration: 1745 Training loss: 0.83660\n",
      "Epoch: 7/9 Iteration: 1746 Training loss: 1.33869\n",
      "Epoch: 7/9 Iteration: 1747 Training loss: 0.73955\n",
      "Epoch: 7/9 Iteration: 1748 Training loss: 0.97421\n",
      "Epoch: 7/9 Iteration: 1749 Training loss: 0.77406\n",
      "Epoch: 7/9 Iteration: 1750 Training loss: 1.68474\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1750 Validation Acc: 0.6800\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1751 Training loss: 1.67860\n",
      "Epoch: 7/9 Iteration: 1752 Training loss: 1.01820\n",
      "Epoch: 7/9 Iteration: 1753 Training loss: 0.96105\n",
      "Epoch: 7/9 Iteration: 1754 Training loss: 0.52682\n",
      "Epoch: 7/9 Iteration: 1755 Training loss: 0.91292\n",
      "Epoch: 7/9 Iteration: 1756 Training loss: 1.13341\n",
      "Epoch: 7/9 Iteration: 1757 Training loss: 1.15458\n",
      "Epoch: 7/9 Iteration: 1758 Training loss: 0.93030\n",
      "Epoch: 7/9 Iteration: 1759 Training loss: 1.11929\n",
      "Epoch: 7/9 Iteration: 1760 Training loss: 1.07142\n",
      "Epoch: 7/9 Iteration: 1761 Training loss: 1.65004\n",
      "Epoch: 7/9 Iteration: 1762 Training loss: 1.11977\n",
      "Epoch: 7/9 Iteration: 1763 Training loss: 0.42199\n",
      "Epoch: 7/9 Iteration: 1764 Training loss: 1.47352\n",
      "Epoch: 7/9 Iteration: 1765 Training loss: 0.38832\n",
      "Epoch: 7/9 Iteration: 1766 Training loss: 1.06592\n",
      "Epoch: 7/9 Iteration: 1767 Training loss: 1.22253\n",
      "Epoch: 7/9 Iteration: 1768 Training loss: 0.84200\n",
      "Epoch: 7/9 Iteration: 1769 Training loss: 1.44015\n",
      "Epoch: 7/9 Iteration: 1770 Training loss: 0.94179\n",
      "Epoch: 7/9 Iteration: 1771 Training loss: 0.98740\n",
      "Epoch: 7/9 Iteration: 1772 Training loss: 1.00994\n",
      "Epoch: 7/9 Iteration: 1773 Training loss: 1.32257\n",
      "Epoch: 7/9 Iteration: 1774 Training loss: 1.58293\n",
      "Epoch: 7/9 Iteration: 1775 Training loss: 0.90621\n",
      "Epoch: 7/9 Iteration: 1776 Training loss: 1.15868\n",
      "Epoch: 7/9 Iteration: 1777 Training loss: 0.97063\n",
      "Epoch: 7/9 Iteration: 1778 Training loss: 0.60123\n",
      "Epoch: 7/9 Iteration: 1779 Training loss: 0.61628\n",
      "Epoch: 7/9 Iteration: 1780 Training loss: 1.86222\n",
      "Epoch: 7/9 Iteration: 1781 Training loss: 0.78604\n",
      "Epoch: 7/9 Iteration: 1782 Training loss: 1.31298\n",
      "Epoch: 7/9 Iteration: 1783 Training loss: 0.92308\n",
      "Epoch: 7/9 Iteration: 1784 Training loss: 0.99545\n",
      "Epoch: 7/9 Iteration: 1785 Training loss: 0.96402\n",
      "Epoch: 7/9 Iteration: 1786 Training loss: 0.78585\n",
      "Epoch: 7/9 Iteration: 1787 Training loss: 1.46664\n",
      "Epoch: 7/9 Iteration: 1788 Training loss: 1.54646\n",
      "Epoch: 7/9 Iteration: 1789 Training loss: 0.80685\n",
      "Epoch: 7/9 Iteration: 1790 Training loss: 0.87329\n",
      "Epoch: 7/9 Iteration: 1791 Training loss: 0.84562\n",
      "Epoch: 7/9 Iteration: 1792 Training loss: 1.30179\n",
      "Epoch: 7/9 Iteration: 1793 Training loss: 0.65578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/9 Iteration: 1794 Training loss: 0.90617\n",
      "Epoch: 7/9 Iteration: 1795 Training loss: 0.96672\n",
      "Epoch: 7/9 Iteration: 1796 Training loss: 1.01377\n",
      "Epoch: 7/9 Iteration: 1797 Training loss: 1.00290\n",
      "Epoch: 7/9 Iteration: 1798 Training loss: 0.99580\n",
      "Epoch: 7/9 Iteration: 1799 Training loss: 1.53366\n",
      "Epoch: 7/9 Iteration: 1800 Training loss: 0.50145\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1800 Validation Acc: 0.7033\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1801 Training loss: 1.09553\n",
      "Epoch: 7/9 Iteration: 1802 Training loss: 0.57660\n",
      "Epoch: 7/9 Iteration: 1803 Training loss: 0.35727\n",
      "Epoch: 7/9 Iteration: 1804 Training loss: 1.12583\n",
      "Epoch: 7/9 Iteration: 1805 Training loss: 1.48886\n",
      "Epoch: 7/9 Iteration: 1806 Training loss: 0.77460\n",
      "Epoch: 7/9 Iteration: 1807 Training loss: 0.74866\n",
      "Epoch: 7/9 Iteration: 1808 Training loss: 0.77306\n",
      "Epoch: 7/9 Iteration: 1809 Training loss: 0.79051\n",
      "Epoch: 7/9 Iteration: 1810 Training loss: 1.24002\n",
      "Epoch: 7/9 Iteration: 1811 Training loss: 0.99443\n",
      "Epoch: 7/9 Iteration: 1812 Training loss: 0.91171\n",
      "Epoch: 7/9 Iteration: 1813 Training loss: 0.87750\n",
      "Epoch: 7/9 Iteration: 1814 Training loss: 0.55871\n",
      "Epoch: 7/9 Iteration: 1815 Training loss: 0.47508\n",
      "Epoch: 7/9 Iteration: 1816 Training loss: 2.12932\n",
      "Epoch: 7/9 Iteration: 1817 Training loss: 1.76049\n",
      "Epoch: 7/9 Iteration: 1818 Training loss: 0.63209\n",
      "Epoch: 7/9 Iteration: 1819 Training loss: 1.05271\n",
      "Epoch: 7/9 Iteration: 1820 Training loss: 1.66256\n",
      "Epoch: 7/9 Iteration: 1821 Training loss: 0.55287\n",
      "Epoch: 7/9 Iteration: 1822 Training loss: 2.14475\n",
      "Epoch: 7/9 Iteration: 1823 Training loss: 1.62050\n",
      "Epoch: 7/9 Iteration: 1824 Training loss: 1.16405\n",
      "Epoch: 7/9 Iteration: 1825 Training loss: 1.05836\n",
      "Epoch: 7/9 Iteration: 1826 Training loss: 1.13162\n",
      "Epoch: 7/9 Iteration: 1827 Training loss: 1.27548\n",
      "Epoch: 7/9 Iteration: 1828 Training loss: 0.71716\n",
      "Epoch: 7/9 Iteration: 1829 Training loss: 0.49148\n",
      "Epoch: 7/9 Iteration: 1830 Training loss: 0.83950\n",
      "Epoch: 7/9 Iteration: 1831 Training loss: 0.92449\n",
      "Epoch: 7/9 Iteration: 1832 Training loss: 0.75033\n",
      "Epoch: 7/9 Iteration: 1833 Training loss: 0.45051\n",
      "Epoch: 7/9 Iteration: 1834 Training loss: 1.13850\n",
      "Epoch: 7/9 Iteration: 1835 Training loss: 0.62353\n",
      "Epoch: 7/9 Iteration: 1836 Training loss: 1.12273\n",
      "Epoch: 7/9 Iteration: 1837 Training loss: 0.68994\n",
      "Epoch: 7/9 Iteration: 1838 Training loss: 0.59288\n",
      "Epoch: 7/9 Iteration: 1839 Training loss: 0.86407\n",
      "Epoch: 7/9 Iteration: 1840 Training loss: 0.82980\n",
      "Epoch: 7/9 Iteration: 1841 Training loss: 0.48203\n",
      "Epoch: 7/9 Iteration: 1842 Training loss: 0.56999\n",
      "Epoch: 7/9 Iteration: 1843 Training loss: 0.81475\n",
      "Epoch: 7/9 Iteration: 1844 Training loss: 1.07478\n",
      "Epoch: 7/9 Iteration: 1845 Training loss: 0.65413\n",
      "Epoch: 7/9 Iteration: 1846 Training loss: 1.87225\n",
      "Epoch: 7/9 Iteration: 1847 Training loss: 0.43049\n",
      "Epoch: 7/9 Iteration: 1848 Training loss: 0.28500\n",
      "Epoch: 7/9 Iteration: 1849 Training loss: 0.97411\n",
      "Epoch: 7/9 Iteration: 1850 Training loss: 0.49611\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1850 Validation Acc: 0.7233\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1851 Training loss: 0.38792\n",
      "Epoch: 7/9 Iteration: 1852 Training loss: 0.92629\n",
      "Epoch: 7/9 Iteration: 1853 Training loss: 1.33146\n",
      "Epoch: 7/9 Iteration: 1854 Training loss: 0.98896\n",
      "Epoch: 7/9 Iteration: 1855 Training loss: 1.73706\n",
      "Epoch: 7/9 Iteration: 1856 Training loss: 1.13111\n",
      "Epoch: 7/9 Iteration: 1857 Training loss: 2.06688\n",
      "Epoch: 7/9 Iteration: 1858 Training loss: 0.85227\n",
      "Epoch: 7/9 Iteration: 1859 Training loss: 0.81302\n",
      "Epoch: 7/9 Iteration: 1860 Training loss: 0.65902\n",
      "Epoch: 7/9 Iteration: 1861 Training loss: 1.11782\n",
      "Epoch: 7/9 Iteration: 1862 Training loss: 0.48227\n",
      "Epoch: 7/9 Iteration: 1863 Training loss: 0.62100\n",
      "Epoch: 7/9 Iteration: 1864 Training loss: 1.71373\n",
      "Epoch: 7/9 Iteration: 1865 Training loss: 1.56343\n",
      "Epoch: 7/9 Iteration: 1866 Training loss: 1.23653\n",
      "Epoch: 7/9 Iteration: 1867 Training loss: 0.90239\n",
      "Epoch: 7/9 Iteration: 1868 Training loss: 0.83678\n",
      "Epoch: 7/9 Iteration: 1869 Training loss: 1.35364\n",
      "Epoch: 7/9 Iteration: 1870 Training loss: 0.99570\n",
      "Epoch: 7/9 Iteration: 1871 Training loss: 1.89463\n",
      "Epoch: 7/9 Iteration: 1872 Training loss: 0.95235\n",
      "Epoch: 7/9 Iteration: 1873 Training loss: 1.34270\n",
      "Epoch: 7/9 Iteration: 1874 Training loss: 0.48334\n",
      "Epoch: 7/9 Iteration: 1875 Training loss: 0.81978\n",
      "Epoch: 7/9 Iteration: 1876 Training loss: 1.27034\n",
      "Epoch: 7/9 Iteration: 1877 Training loss: 1.02255\n",
      "Epoch: 7/9 Iteration: 1878 Training loss: 1.43258\n",
      "Epoch: 7/9 Iteration: 1879 Training loss: 0.84264\n",
      "Epoch: 7/9 Iteration: 1880 Training loss: 1.23784\n",
      "Epoch: 7/9 Iteration: 1881 Training loss: 1.19970\n",
      "Epoch: 7/9 Iteration: 1882 Training loss: 1.21180\n",
      "Epoch: 7/9 Iteration: 1883 Training loss: 1.09614\n",
      "Epoch: 7/9 Iteration: 1884 Training loss: 1.58717\n",
      "Epoch: 7/9 Iteration: 1885 Training loss: 1.17737\n",
      "Epoch: 7/9 Iteration: 1886 Training loss: 0.50712\n",
      "Epoch: 7/9 Iteration: 1887 Training loss: 0.92638\n",
      "Epoch: 7/9 Iteration: 1888 Training loss: 0.88619\n",
      "Epoch: 7/9 Iteration: 1889 Training loss: 0.24863\n",
      "Epoch: 7/9 Iteration: 1890 Training loss: 1.41145\n",
      "Epoch: 7/9 Iteration: 1891 Training loss: 1.17296\n",
      "Epoch: 7/9 Iteration: 1892 Training loss: 1.66295\n",
      "Epoch: 7/9 Iteration: 1893 Training loss: 1.06827\n",
      "Epoch: 7/9 Iteration: 1894 Training loss: 1.73002\n",
      "Epoch: 7/9 Iteration: 1895 Training loss: 1.94230\n",
      "Epoch: 7/9 Iteration: 1896 Training loss: 1.69763\n",
      "Epoch: 7/9 Iteration: 1897 Training loss: 0.99338\n",
      "Epoch: 7/9 Iteration: 1898 Training loss: 0.12153\n",
      "Epoch: 7/9 Iteration: 1899 Training loss: 0.69541\n",
      "Epoch: 7/9 Iteration: 1900 Training loss: 0.15106\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1900 Validation Acc: 0.6667\n",
      "***\n",
      "Epoch: 7/9 Iteration: 1901 Training loss: 1.68444\n",
      "Epoch: 7/9 Iteration: 1902 Training loss: 1.08622\n",
      "Epoch: 7/9 Iteration: 1903 Training loss: 1.31954\n",
      "Epoch: 7/9 Iteration: 1904 Training loss: 0.98032\n",
      "Epoch: 7/9 Iteration: 1905 Training loss: 0.86548\n",
      "Epoch: 7/9 Iteration: 1906 Training loss: 1.22256\n",
      "Epoch: 7/9 Iteration: 1907 Training loss: 0.80097\n",
      "Epoch: 7/9 Iteration: 1908 Training loss: 1.01404\n",
      "Epoch: 7/9 Iteration: 1909 Training loss: 0.78838\n",
      "Epoch: 7/9 Iteration: 1910 Training loss: 1.42684\n",
      "Epoch: 7/9 Iteration: 1911 Training loss: 0.55730\n",
      "Epoch: 7/9 Iteration: 1912 Training loss: 1.58557\n",
      "Epoch: 7/9 Iteration: 1913 Training loss: 0.95005\n",
      "Epoch: 7/9 Iteration: 1914 Training loss: 0.89312\n",
      "Epoch: 7/9 Iteration: 1915 Training loss: 0.57932\n",
      "Epoch: 7/9 Iteration: 1916 Training loss: 1.20997\n",
      "Epoch: 7/9 Iteration: 1917 Training loss: 1.16391\n",
      "Epoch: 7/9 Iteration: 1918 Training loss: 1.48159\n",
      "Epoch: 7/9 Iteration: 1919 Training loss: 1.56278\n",
      "Epoch: 8/9 Iteration: 1920 Training loss: 0.99303\n",
      "Epoch: 8/9 Iteration: 1921 Training loss: 0.72383\n",
      "Epoch: 8/9 Iteration: 1922 Training loss: 1.94321\n",
      "Epoch: 8/9 Iteration: 1923 Training loss: 0.60195\n",
      "Epoch: 8/9 Iteration: 1924 Training loss: 1.36308\n",
      "Epoch: 8/9 Iteration: 1925 Training loss: 1.27326\n",
      "Epoch: 8/9 Iteration: 1926 Training loss: 0.94522\n",
      "Epoch: 8/9 Iteration: 1927 Training loss: 1.79513\n",
      "Epoch: 8/9 Iteration: 1928 Training loss: 1.39137\n",
      "Epoch: 8/9 Iteration: 1929 Training loss: 0.94984\n",
      "Epoch: 8/9 Iteration: 1930 Training loss: 1.68365\n",
      "Epoch: 8/9 Iteration: 1931 Training loss: 0.85710\n",
      "Epoch: 8/9 Iteration: 1932 Training loss: 1.26293\n",
      "Epoch: 8/9 Iteration: 1933 Training loss: 1.07891\n",
      "Epoch: 8/9 Iteration: 1934 Training loss: 0.74899\n",
      "Epoch: 8/9 Iteration: 1935 Training loss: 1.10509\n",
      "Epoch: 8/9 Iteration: 1936 Training loss: 2.16197\n",
      "Epoch: 8/9 Iteration: 1937 Training loss: 0.91839\n",
      "Epoch: 8/9 Iteration: 1938 Training loss: 1.27089\n",
      "Epoch: 8/9 Iteration: 1939 Training loss: 1.85514\n",
      "Epoch: 8/9 Iteration: 1940 Training loss: 1.73344\n",
      "Epoch: 8/9 Iteration: 1941 Training loss: 1.42786\n",
      "Epoch: 8/9 Iteration: 1942 Training loss: 0.91904\n",
      "Epoch: 8/9 Iteration: 1943 Training loss: 1.35565\n",
      "Epoch: 8/9 Iteration: 1944 Training loss: 0.85733\n",
      "Epoch: 8/9 Iteration: 1945 Training loss: 0.72841\n",
      "Epoch: 8/9 Iteration: 1946 Training loss: 1.71442\n",
      "Epoch: 8/9 Iteration: 1947 Training loss: 1.30228\n",
      "Epoch: 8/9 Iteration: 1948 Training loss: 0.76971\n",
      "Epoch: 8/9 Iteration: 1949 Training loss: 1.30687\n",
      "Epoch: 8/9 Iteration: 1950 Training loss: 0.72202\n",
      "***\n",
      "Epoch: 8/9 Iteration: 1950 Validation Acc: 0.6200\n",
      "***\n",
      "Epoch: 8/9 Iteration: 1951 Training loss: 1.19751\n",
      "Epoch: 8/9 Iteration: 1952 Training loss: 0.87749\n",
      "Epoch: 8/9 Iteration: 1953 Training loss: 0.35827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/9 Iteration: 1954 Training loss: 1.41694\n",
      "Epoch: 8/9 Iteration: 1955 Training loss: 1.91884\n",
      "Epoch: 8/9 Iteration: 1956 Training loss: 1.01031\n",
      "Epoch: 8/9 Iteration: 1957 Training loss: 0.84703\n",
      "Epoch: 8/9 Iteration: 1958 Training loss: 0.50242\n",
      "Epoch: 8/9 Iteration: 1959 Training loss: 1.27591\n",
      "Epoch: 8/9 Iteration: 1960 Training loss: 0.99921\n",
      "Epoch: 8/9 Iteration: 1961 Training loss: 0.73406\n",
      "Epoch: 8/9 Iteration: 1962 Training loss: 1.05228\n",
      "Epoch: 8/9 Iteration: 1963 Training loss: 1.65015\n",
      "Epoch: 8/9 Iteration: 1964 Training loss: 1.13223\n",
      "Epoch: 8/9 Iteration: 1965 Training loss: 1.00675\n",
      "Epoch: 8/9 Iteration: 1966 Training loss: 0.59819\n",
      "Epoch: 8/9 Iteration: 1967 Training loss: 1.61781\n",
      "Epoch: 8/9 Iteration: 1968 Training loss: 0.72540\n",
      "Epoch: 8/9 Iteration: 1969 Training loss: 1.15312\n",
      "Epoch: 8/9 Iteration: 1970 Training loss: 2.12054\n",
      "Epoch: 8/9 Iteration: 1971 Training loss: 1.70951\n",
      "Epoch: 8/9 Iteration: 1972 Training loss: 0.98468\n",
      "Epoch: 8/9 Iteration: 1973 Training loss: 0.67556\n",
      "Epoch: 8/9 Iteration: 1974 Training loss: 1.05132\n",
      "Epoch: 8/9 Iteration: 1975 Training loss: 0.75310\n",
      "Epoch: 8/9 Iteration: 1976 Training loss: 1.11453\n",
      "Epoch: 8/9 Iteration: 1977 Training loss: 1.17961\n",
      "Epoch: 8/9 Iteration: 1978 Training loss: 0.70252\n",
      "Epoch: 8/9 Iteration: 1979 Training loss: 0.93188\n",
      "Epoch: 8/9 Iteration: 1980 Training loss: 1.34600\n",
      "Epoch: 8/9 Iteration: 1981 Training loss: 1.32436\n",
      "Epoch: 8/9 Iteration: 1982 Training loss: 0.31821\n",
      "Epoch: 8/9 Iteration: 1983 Training loss: 1.16676\n",
      "Epoch: 8/9 Iteration: 1984 Training loss: 1.04677\n",
      "Epoch: 8/9 Iteration: 1985 Training loss: 0.80611\n",
      "Epoch: 8/9 Iteration: 1986 Training loss: 0.94906\n",
      "Epoch: 8/9 Iteration: 1987 Training loss: 1.27750\n",
      "Epoch: 8/9 Iteration: 1988 Training loss: 1.81664\n",
      "Epoch: 8/9 Iteration: 1989 Training loss: 1.20334\n",
      "Epoch: 8/9 Iteration: 1990 Training loss: 1.63127\n",
      "Epoch: 8/9 Iteration: 1991 Training loss: 0.87577\n",
      "Epoch: 8/9 Iteration: 1992 Training loss: 1.50788\n",
      "Epoch: 8/9 Iteration: 1993 Training loss: 0.75772\n",
      "Epoch: 8/9 Iteration: 1994 Training loss: 1.11768\n",
      "Epoch: 8/9 Iteration: 1995 Training loss: 1.11047\n",
      "Epoch: 8/9 Iteration: 1996 Training loss: 1.61043\n",
      "Epoch: 8/9 Iteration: 1997 Training loss: 1.59603\n",
      "Epoch: 8/9 Iteration: 1998 Training loss: 0.87235\n",
      "Epoch: 8/9 Iteration: 1999 Training loss: 1.27801\n",
      "Epoch: 8/9 Iteration: 2000 Training loss: 0.78243\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2000 Validation Acc: 0.6900\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2001 Training loss: 1.51894\n",
      "Epoch: 8/9 Iteration: 2002 Training loss: 1.13618\n",
      "Epoch: 8/9 Iteration: 2003 Training loss: 0.85588\n",
      "Epoch: 8/9 Iteration: 2004 Training loss: 1.13442\n",
      "Epoch: 8/9 Iteration: 2005 Training loss: 0.82986\n",
      "Epoch: 8/9 Iteration: 2006 Training loss: 0.89077\n",
      "Epoch: 8/9 Iteration: 2007 Training loss: 0.98762\n",
      "Epoch: 8/9 Iteration: 2008 Training loss: 1.09284\n",
      "Epoch: 8/9 Iteration: 2009 Training loss: 1.16469\n",
      "Epoch: 8/9 Iteration: 2010 Training loss: 1.21544\n",
      "Epoch: 8/9 Iteration: 2011 Training loss: 0.81998\n",
      "Epoch: 8/9 Iteration: 2012 Training loss: 1.32798\n",
      "Epoch: 8/9 Iteration: 2013 Training loss: 0.71515\n",
      "Epoch: 8/9 Iteration: 2014 Training loss: 0.97529\n",
      "Epoch: 8/9 Iteration: 2015 Training loss: 1.64001\n",
      "Epoch: 8/9 Iteration: 2016 Training loss: 0.87221\n",
      "Epoch: 8/9 Iteration: 2017 Training loss: 1.30028\n",
      "Epoch: 8/9 Iteration: 2018 Training loss: 0.69802\n",
      "Epoch: 8/9 Iteration: 2019 Training loss: 0.76823\n",
      "Epoch: 8/9 Iteration: 2020 Training loss: 1.20241\n",
      "Epoch: 8/9 Iteration: 2021 Training loss: 1.46058\n",
      "Epoch: 8/9 Iteration: 2022 Training loss: 0.59858\n",
      "Epoch: 8/9 Iteration: 2023 Training loss: 1.13095\n",
      "Epoch: 8/9 Iteration: 2024 Training loss: 1.17380\n",
      "Epoch: 8/9 Iteration: 2025 Training loss: 1.24083\n",
      "Epoch: 8/9 Iteration: 2026 Training loss: 1.20021\n",
      "Epoch: 8/9 Iteration: 2027 Training loss: 0.85147\n",
      "Epoch: 8/9 Iteration: 2028 Training loss: 0.60411\n",
      "Epoch: 8/9 Iteration: 2029 Training loss: 0.72190\n",
      "Epoch: 8/9 Iteration: 2030 Training loss: 1.00472\n",
      "Epoch: 8/9 Iteration: 2031 Training loss: 0.72253\n",
      "Epoch: 8/9 Iteration: 2032 Training loss: 1.27273\n",
      "Epoch: 8/9 Iteration: 2033 Training loss: 0.73925\n",
      "Epoch: 8/9 Iteration: 2034 Training loss: 0.49195\n",
      "Epoch: 8/9 Iteration: 2035 Training loss: 1.21632\n",
      "Epoch: 8/9 Iteration: 2036 Training loss: 0.50457\n",
      "Epoch: 8/9 Iteration: 2037 Training loss: 1.16837\n",
      "Epoch: 8/9 Iteration: 2038 Training loss: 0.75193\n",
      "Epoch: 8/9 Iteration: 2039 Training loss: 1.56761\n",
      "Epoch: 8/9 Iteration: 2040 Training loss: 0.82589\n",
      "Epoch: 8/9 Iteration: 2041 Training loss: 1.25565\n",
      "Epoch: 8/9 Iteration: 2042 Training loss: 0.36115\n",
      "Epoch: 8/9 Iteration: 2043 Training loss: 0.56595\n",
      "Epoch: 8/9 Iteration: 2044 Training loss: 1.20536\n",
      "Epoch: 8/9 Iteration: 2045 Training loss: 1.40426\n",
      "Epoch: 8/9 Iteration: 2046 Training loss: 2.30502\n",
      "Epoch: 8/9 Iteration: 2047 Training loss: 1.47163\n",
      "Epoch: 8/9 Iteration: 2048 Training loss: 0.81046\n",
      "Epoch: 8/9 Iteration: 2049 Training loss: 1.07516\n",
      "Epoch: 8/9 Iteration: 2050 Training loss: 1.70345\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2050 Validation Acc: 0.7200\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2051 Training loss: 1.36939\n",
      "Epoch: 8/9 Iteration: 2052 Training loss: 0.60532\n",
      "Epoch: 8/9 Iteration: 2053 Training loss: 1.56576\n",
      "Epoch: 8/9 Iteration: 2054 Training loss: 1.64949\n",
      "Epoch: 8/9 Iteration: 2055 Training loss: 0.52182\n",
      "Epoch: 8/9 Iteration: 2056 Training loss: 1.56348\n",
      "Epoch: 8/9 Iteration: 2057 Training loss: 1.14137\n",
      "Epoch: 8/9 Iteration: 2058 Training loss: 0.72345\n",
      "Epoch: 8/9 Iteration: 2059 Training loss: 1.07419\n",
      "Epoch: 8/9 Iteration: 2060 Training loss: 1.34554\n",
      "Epoch: 8/9 Iteration: 2061 Training loss: 0.79922\n",
      "Epoch: 8/9 Iteration: 2062 Training loss: 1.71191\n",
      "Epoch: 8/9 Iteration: 2063 Training loss: 0.89061\n",
      "Epoch: 8/9 Iteration: 2064 Training loss: 0.71444\n",
      "Epoch: 8/9 Iteration: 2065 Training loss: 1.19336\n",
      "Epoch: 8/9 Iteration: 2066 Training loss: 1.69003\n",
      "Epoch: 8/9 Iteration: 2067 Training loss: 1.10735\n",
      "Epoch: 8/9 Iteration: 2068 Training loss: 0.73053\n",
      "Epoch: 8/9 Iteration: 2069 Training loss: 0.51276\n",
      "Epoch: 8/9 Iteration: 2070 Training loss: 0.95340\n",
      "Epoch: 8/9 Iteration: 2071 Training loss: 0.87647\n",
      "Epoch: 8/9 Iteration: 2072 Training loss: 1.07377\n",
      "Epoch: 8/9 Iteration: 2073 Training loss: 1.20940\n",
      "Epoch: 8/9 Iteration: 2074 Training loss: 1.17034\n",
      "Epoch: 8/9 Iteration: 2075 Training loss: 0.52987\n",
      "Epoch: 8/9 Iteration: 2076 Training loss: 0.93684\n",
      "Epoch: 8/9 Iteration: 2077 Training loss: 0.91378\n",
      "Epoch: 8/9 Iteration: 2078 Training loss: 0.84534\n",
      "Epoch: 8/9 Iteration: 2079 Training loss: 1.16729\n",
      "Epoch: 8/9 Iteration: 2080 Training loss: 0.92337\n",
      "Epoch: 8/9 Iteration: 2081 Training loss: 0.52949\n",
      "Epoch: 8/9 Iteration: 2082 Training loss: 0.72700\n",
      "Epoch: 8/9 Iteration: 2083 Training loss: 0.69765\n",
      "Epoch: 8/9 Iteration: 2084 Training loss: 0.78579\n",
      "Epoch: 8/9 Iteration: 2085 Training loss: 1.04110\n",
      "Epoch: 8/9 Iteration: 2086 Training loss: 1.59468\n",
      "Epoch: 8/9 Iteration: 2087 Training loss: 0.81028\n",
      "Epoch: 8/9 Iteration: 2088 Training loss: 0.53586\n",
      "Epoch: 8/9 Iteration: 2089 Training loss: 0.72652\n",
      "Epoch: 8/9 Iteration: 2090 Training loss: 0.33577\n",
      "Epoch: 8/9 Iteration: 2091 Training loss: 0.74959\n",
      "Epoch: 8/9 Iteration: 2092 Training loss: 1.28995\n",
      "Epoch: 8/9 Iteration: 2093 Training loss: 0.85837\n",
      "Epoch: 8/9 Iteration: 2094 Training loss: 1.16583\n",
      "Epoch: 8/9 Iteration: 2095 Training loss: 1.99561\n",
      "Epoch: 8/9 Iteration: 2096 Training loss: 0.86372\n",
      "Epoch: 8/9 Iteration: 2097 Training loss: 1.32804\n",
      "Epoch: 8/9 Iteration: 2098 Training loss: 0.56838\n",
      "Epoch: 8/9 Iteration: 2099 Training loss: 0.65457\n",
      "Epoch: 8/9 Iteration: 2100 Training loss: 0.43160\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2100 Validation Acc: 0.7000\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2101 Training loss: 1.00807\n",
      "Epoch: 8/9 Iteration: 2102 Training loss: 0.66493\n",
      "Epoch: 8/9 Iteration: 2103 Training loss: 0.76371\n",
      "Epoch: 8/9 Iteration: 2104 Training loss: 1.63207\n",
      "Epoch: 8/9 Iteration: 2105 Training loss: 1.52471\n",
      "Epoch: 8/9 Iteration: 2106 Training loss: 0.68917\n",
      "Epoch: 8/9 Iteration: 2107 Training loss: 0.85147\n",
      "Epoch: 8/9 Iteration: 2108 Training loss: 0.74035\n",
      "Epoch: 8/9 Iteration: 2109 Training loss: 1.75495\n",
      "Epoch: 8/9 Iteration: 2110 Training loss: 0.64193\n",
      "Epoch: 8/9 Iteration: 2111 Training loss: 0.72703\n",
      "Epoch: 8/9 Iteration: 2112 Training loss: 1.11530\n",
      "Epoch: 8/9 Iteration: 2113 Training loss: 1.10383\n",
      "Epoch: 8/9 Iteration: 2114 Training loss: 0.84855\n",
      "Epoch: 8/9 Iteration: 2115 Training loss: 1.39486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/9 Iteration: 2116 Training loss: 1.25287\n",
      "Epoch: 8/9 Iteration: 2117 Training loss: 0.63947\n",
      "Epoch: 8/9 Iteration: 2118 Training loss: 0.80807\n",
      "Epoch: 8/9 Iteration: 2119 Training loss: 0.56644\n",
      "Epoch: 8/9 Iteration: 2120 Training loss: 1.31642\n",
      "Epoch: 8/9 Iteration: 2121 Training loss: 1.06178\n",
      "Epoch: 8/9 Iteration: 2122 Training loss: 1.36017\n",
      "Epoch: 8/9 Iteration: 2123 Training loss: 0.83446\n",
      "Epoch: 8/9 Iteration: 2124 Training loss: 1.45874\n",
      "Epoch: 8/9 Iteration: 2125 Training loss: 1.18141\n",
      "Epoch: 8/9 Iteration: 2126 Training loss: 0.77410\n",
      "Epoch: 8/9 Iteration: 2127 Training loss: 1.84368\n",
      "Epoch: 8/9 Iteration: 2128 Training loss: 1.01716\n",
      "Epoch: 8/9 Iteration: 2129 Training loss: 0.55789\n",
      "Epoch: 8/9 Iteration: 2130 Training loss: 1.42384\n",
      "Epoch: 8/9 Iteration: 2131 Training loss: 0.89586\n",
      "Epoch: 8/9 Iteration: 2132 Training loss: 2.10923\n",
      "Epoch: 8/9 Iteration: 2133 Training loss: 1.69345\n",
      "Epoch: 8/9 Iteration: 2134 Training loss: 1.37196\n",
      "Epoch: 8/9 Iteration: 2135 Training loss: 1.23714\n",
      "Epoch: 8/9 Iteration: 2136 Training loss: 1.17088\n",
      "Epoch: 8/9 Iteration: 2137 Training loss: 1.93584\n",
      "Epoch: 8/9 Iteration: 2138 Training loss: 0.18311\n",
      "Epoch: 8/9 Iteration: 2139 Training loss: 1.14496\n",
      "Epoch: 8/9 Iteration: 2140 Training loss: 0.76365\n",
      "Epoch: 8/9 Iteration: 2141 Training loss: 1.34058\n",
      "Epoch: 8/9 Iteration: 2142 Training loss: 0.83878\n",
      "Epoch: 8/9 Iteration: 2143 Training loss: 1.25460\n",
      "Epoch: 8/9 Iteration: 2144 Training loss: 0.76011\n",
      "Epoch: 8/9 Iteration: 2145 Training loss: 0.98094\n",
      "Epoch: 8/9 Iteration: 2146 Training loss: 1.35333\n",
      "Epoch: 8/9 Iteration: 2147 Training loss: 1.10824\n",
      "Epoch: 8/9 Iteration: 2148 Training loss: 0.80646\n",
      "Epoch: 8/9 Iteration: 2149 Training loss: 1.00660\n",
      "Epoch: 8/9 Iteration: 2150 Training loss: 1.07820\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2150 Validation Acc: 0.6800\n",
      "***\n",
      "Epoch: 8/9 Iteration: 2151 Training loss: 0.69415\n",
      "Epoch: 8/9 Iteration: 2152 Training loss: 1.86282\n",
      "Epoch: 8/9 Iteration: 2153 Training loss: 1.20203\n",
      "Epoch: 8/9 Iteration: 2154 Training loss: 1.50469\n",
      "Epoch: 8/9 Iteration: 2155 Training loss: 0.47325\n",
      "Epoch: 8/9 Iteration: 2156 Training loss: 1.38029\n",
      "Epoch: 8/9 Iteration: 2157 Training loss: 0.60281\n",
      "Epoch: 8/9 Iteration: 2158 Training loss: 1.31644\n",
      "Epoch: 8/9 Iteration: 2159 Training loss: 1.05426\n",
      "Epoch: 9/9 Iteration: 2160 Training loss: 1.00800\n",
      "Epoch: 9/9 Iteration: 2161 Training loss: 0.93018\n",
      "Epoch: 9/9 Iteration: 2162 Training loss: 0.95323\n",
      "Epoch: 9/9 Iteration: 2163 Training loss: 0.31047\n",
      "Epoch: 9/9 Iteration: 2164 Training loss: 1.43287\n",
      "Epoch: 9/9 Iteration: 2165 Training loss: 1.03097\n",
      "Epoch: 9/9 Iteration: 2166 Training loss: 1.13071\n",
      "Epoch: 9/9 Iteration: 2167 Training loss: 1.18215\n",
      "Epoch: 9/9 Iteration: 2168 Training loss: 1.00012\n",
      "Epoch: 9/9 Iteration: 2169 Training loss: 1.54912\n",
      "Epoch: 9/9 Iteration: 2170 Training loss: 1.33163\n",
      "Epoch: 9/9 Iteration: 2171 Training loss: 0.89225\n",
      "Epoch: 9/9 Iteration: 2172 Training loss: 0.72375\n",
      "Epoch: 9/9 Iteration: 2173 Training loss: 0.63263\n",
      "Epoch: 9/9 Iteration: 2174 Training loss: 0.57723\n",
      "Epoch: 9/9 Iteration: 2175 Training loss: 1.20001\n",
      "Epoch: 9/9 Iteration: 2176 Training loss: 0.64478\n",
      "Epoch: 9/9 Iteration: 2177 Training loss: 0.68782\n",
      "Epoch: 9/9 Iteration: 2178 Training loss: 0.91409\n",
      "Epoch: 9/9 Iteration: 2179 Training loss: 1.91440\n",
      "Epoch: 9/9 Iteration: 2180 Training loss: 1.35357\n",
      "Epoch: 9/9 Iteration: 2181 Training loss: 1.22014\n",
      "Epoch: 9/9 Iteration: 2182 Training loss: 0.62170\n",
      "Epoch: 9/9 Iteration: 2183 Training loss: 1.54466\n",
      "Epoch: 9/9 Iteration: 2184 Training loss: 1.27192\n",
      "Epoch: 9/9 Iteration: 2185 Training loss: 0.70419\n",
      "Epoch: 9/9 Iteration: 2186 Training loss: 1.24933\n",
      "Epoch: 9/9 Iteration: 2187 Training loss: 1.07329\n",
      "Epoch: 9/9 Iteration: 2188 Training loss: 1.13131\n",
      "Epoch: 9/9 Iteration: 2189 Training loss: 1.29155\n",
      "Epoch: 9/9 Iteration: 2190 Training loss: 0.99726\n",
      "Epoch: 9/9 Iteration: 2191 Training loss: 1.26976\n",
      "Epoch: 9/9 Iteration: 2192 Training loss: 0.94198\n",
      "Epoch: 9/9 Iteration: 2193 Training loss: 0.17784\n",
      "Epoch: 9/9 Iteration: 2194 Training loss: 0.92499\n",
      "Epoch: 9/9 Iteration: 2195 Training loss: 1.45423\n",
      "Epoch: 9/9 Iteration: 2196 Training loss: 0.57293\n",
      "Epoch: 9/9 Iteration: 2197 Training loss: 0.29287\n",
      "Epoch: 9/9 Iteration: 2198 Training loss: 0.78507\n",
      "Epoch: 9/9 Iteration: 2199 Training loss: 1.44125\n",
      "Epoch: 9/9 Iteration: 2200 Training loss: 0.84434\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2200 Validation Acc: 0.6933\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2201 Training loss: 0.94319\n",
      "Epoch: 9/9 Iteration: 2202 Training loss: 0.93581\n",
      "Epoch: 9/9 Iteration: 2203 Training loss: 2.05776\n",
      "Epoch: 9/9 Iteration: 2204 Training loss: 0.59301\n",
      "Epoch: 9/9 Iteration: 2205 Training loss: 0.63561\n",
      "Epoch: 9/9 Iteration: 2206 Training loss: 0.87809\n",
      "Epoch: 9/9 Iteration: 2207 Training loss: 1.15768\n",
      "Epoch: 9/9 Iteration: 2208 Training loss: 0.70487\n",
      "Epoch: 9/9 Iteration: 2209 Training loss: 0.69712\n",
      "Epoch: 9/9 Iteration: 2210 Training loss: 1.28447\n",
      "Epoch: 9/9 Iteration: 2211 Training loss: 0.90539\n",
      "Epoch: 9/9 Iteration: 2212 Training loss: 1.18020\n",
      "Epoch: 9/9 Iteration: 2213 Training loss: 0.25893\n",
      "Epoch: 9/9 Iteration: 2214 Training loss: 0.61316\n",
      "Epoch: 9/9 Iteration: 2215 Training loss: 1.12740\n",
      "Epoch: 9/9 Iteration: 2216 Training loss: 1.15500\n",
      "Epoch: 9/9 Iteration: 2217 Training loss: 0.77836\n",
      "Epoch: 9/9 Iteration: 2218 Training loss: 0.72591\n",
      "Epoch: 9/9 Iteration: 2219 Training loss: 0.84094\n",
      "Epoch: 9/9 Iteration: 2220 Training loss: 1.16603\n",
      "Epoch: 9/9 Iteration: 2221 Training loss: 0.56852\n",
      "Epoch: 9/9 Iteration: 2222 Training loss: 0.50271\n",
      "Epoch: 9/9 Iteration: 2223 Training loss: 1.10059\n",
      "Epoch: 9/9 Iteration: 2224 Training loss: 0.46249\n",
      "Epoch: 9/9 Iteration: 2225 Training loss: 1.15257\n",
      "Epoch: 9/9 Iteration: 2226 Training loss: 1.55130\n",
      "Epoch: 9/9 Iteration: 2227 Training loss: 1.34300\n",
      "Epoch: 9/9 Iteration: 2228 Training loss: 1.99044\n",
      "Epoch: 9/9 Iteration: 2229 Training loss: 1.23876\n",
      "Epoch: 9/9 Iteration: 2230 Training loss: 2.20480\n",
      "Epoch: 9/9 Iteration: 2231 Training loss: 1.12959\n",
      "Epoch: 9/9 Iteration: 2232 Training loss: 1.15926\n",
      "Epoch: 9/9 Iteration: 2233 Training loss: 0.45300\n",
      "Epoch: 9/9 Iteration: 2234 Training loss: 1.02200\n",
      "Epoch: 9/9 Iteration: 2235 Training loss: 0.65689\n",
      "Epoch: 9/9 Iteration: 2236 Training loss: 1.19109\n",
      "Epoch: 9/9 Iteration: 2237 Training loss: 1.14026\n",
      "Epoch: 9/9 Iteration: 2238 Training loss: 1.16483\n",
      "Epoch: 9/9 Iteration: 2239 Training loss: 0.92701\n",
      "Epoch: 9/9 Iteration: 2240 Training loss: 1.22893\n",
      "Epoch: 9/9 Iteration: 2241 Training loss: 1.85836\n",
      "Epoch: 9/9 Iteration: 2242 Training loss: 1.03509\n",
      "Epoch: 9/9 Iteration: 2243 Training loss: 0.88048\n",
      "Epoch: 9/9 Iteration: 2244 Training loss: 1.28146\n",
      "Epoch: 9/9 Iteration: 2245 Training loss: 1.07574\n",
      "Epoch: 9/9 Iteration: 2246 Training loss: 0.86058\n",
      "Epoch: 9/9 Iteration: 2247 Training loss: 1.27911\n",
      "Epoch: 9/9 Iteration: 2248 Training loss: 0.97279\n",
      "Epoch: 9/9 Iteration: 2249 Training loss: 2.47122\n",
      "Epoch: 9/9 Iteration: 2250 Training loss: 0.76521\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2250 Validation Acc: 0.7100\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2251 Training loss: 0.94089\n",
      "Epoch: 9/9 Iteration: 2252 Training loss: 0.87859\n",
      "Epoch: 9/9 Iteration: 2253 Training loss: 1.04726\n",
      "Epoch: 9/9 Iteration: 2254 Training loss: 1.40476\n",
      "Epoch: 9/9 Iteration: 2255 Training loss: 1.24496\n",
      "Epoch: 9/9 Iteration: 2256 Training loss: 1.11449\n",
      "Epoch: 9/9 Iteration: 2257 Training loss: 1.20820\n",
      "Epoch: 9/9 Iteration: 2258 Training loss: 0.81313\n",
      "Epoch: 9/9 Iteration: 2259 Training loss: 0.68300\n",
      "Epoch: 9/9 Iteration: 2260 Training loss: 1.34495\n",
      "Epoch: 9/9 Iteration: 2261 Training loss: 1.18319\n",
      "Epoch: 9/9 Iteration: 2262 Training loss: 0.90146\n",
      "Epoch: 9/9 Iteration: 2263 Training loss: 1.06628\n",
      "Epoch: 9/9 Iteration: 2264 Training loss: 0.56286\n",
      "Epoch: 9/9 Iteration: 2265 Training loss: 1.49228\n",
      "Epoch: 9/9 Iteration: 2266 Training loss: 1.09768\n",
      "Epoch: 9/9 Iteration: 2267 Training loss: 1.17930\n",
      "Epoch: 9/9 Iteration: 2268 Training loss: 0.86657\n",
      "Epoch: 9/9 Iteration: 2269 Training loss: 0.98577\n",
      "Epoch: 9/9 Iteration: 2270 Training loss: 0.83434\n",
      "Epoch: 9/9 Iteration: 2271 Training loss: 0.92088\n",
      "Epoch: 9/9 Iteration: 2272 Training loss: 1.29621\n",
      "Epoch: 9/9 Iteration: 2273 Training loss: 0.81742\n",
      "Epoch: 9/9 Iteration: 2274 Training loss: 0.80562\n",
      "Epoch: 9/9 Iteration: 2275 Training loss: 0.85497\n",
      "Epoch: 9/9 Iteration: 2276 Training loss: 0.89060\n",
      "Epoch: 9/9 Iteration: 2277 Training loss: 1.25230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/9 Iteration: 2278 Training loss: 0.76065\n",
      "Epoch: 9/9 Iteration: 2279 Training loss: 1.18770\n",
      "Epoch: 9/9 Iteration: 2280 Training loss: 0.47276\n",
      "Epoch: 9/9 Iteration: 2281 Training loss: 1.53274\n",
      "Epoch: 9/9 Iteration: 2282 Training loss: 1.10833\n",
      "Epoch: 9/9 Iteration: 2283 Training loss: 0.77132\n",
      "Epoch: 9/9 Iteration: 2284 Training loss: 1.05647\n",
      "Epoch: 9/9 Iteration: 2285 Training loss: 1.89725\n",
      "Epoch: 9/9 Iteration: 2286 Training loss: 0.77870\n",
      "Epoch: 9/9 Iteration: 2287 Training loss: 1.20205\n",
      "Epoch: 9/9 Iteration: 2288 Training loss: 0.62336\n",
      "Epoch: 9/9 Iteration: 2289 Training loss: 1.08538\n",
      "Epoch: 9/9 Iteration: 2290 Training loss: 0.88505\n",
      "Epoch: 9/9 Iteration: 2291 Training loss: 1.61610\n",
      "Epoch: 9/9 Iteration: 2292 Training loss: 0.81576\n",
      "Epoch: 9/9 Iteration: 2293 Training loss: 0.78667\n",
      "Epoch: 9/9 Iteration: 2294 Training loss: 0.58650\n",
      "Epoch: 9/9 Iteration: 2295 Training loss: 0.63198\n",
      "Epoch: 9/9 Iteration: 2296 Training loss: 1.31269\n",
      "Epoch: 9/9 Iteration: 2297 Training loss: 1.49654\n",
      "Epoch: 9/9 Iteration: 2298 Training loss: 0.74986\n",
      "Epoch: 9/9 Iteration: 2299 Training loss: 0.96649\n",
      "Epoch: 9/9 Iteration: 2300 Training loss: 1.65428\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2300 Validation Acc: 0.7100\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2301 Training loss: 0.78664\n",
      "Epoch: 9/9 Iteration: 2302 Training loss: 1.23632\n",
      "Epoch: 9/9 Iteration: 2303 Training loss: 0.59114\n",
      "Epoch: 9/9 Iteration: 2304 Training loss: 0.80604\n",
      "Epoch: 9/9 Iteration: 2305 Training loss: 1.06853\n",
      "Epoch: 9/9 Iteration: 2306 Training loss: 0.51065\n",
      "Epoch: 9/9 Iteration: 2307 Training loss: 1.17569\n",
      "Epoch: 9/9 Iteration: 2308 Training loss: 1.29128\n",
      "Epoch: 9/9 Iteration: 2309 Training loss: 0.69831\n",
      "Epoch: 9/9 Iteration: 2310 Training loss: 0.86345\n",
      "Epoch: 9/9 Iteration: 2311 Training loss: 1.12732\n",
      "Epoch: 9/9 Iteration: 2312 Training loss: 1.47025\n",
      "Epoch: 9/9 Iteration: 2313 Training loss: 0.97914\n",
      "Epoch: 9/9 Iteration: 2314 Training loss: 0.61211\n",
      "Epoch: 9/9 Iteration: 2315 Training loss: 0.44677\n",
      "Epoch: 9/9 Iteration: 2316 Training loss: 0.70379\n",
      "Epoch: 9/9 Iteration: 2317 Training loss: 1.09025\n",
      "Epoch: 9/9 Iteration: 2318 Training loss: 0.78459\n",
      "Epoch: 9/9 Iteration: 2319 Training loss: 0.99670\n",
      "Epoch: 9/9 Iteration: 2320 Training loss: 0.84036\n",
      "Epoch: 9/9 Iteration: 2321 Training loss: 0.75480\n",
      "Epoch: 9/9 Iteration: 2322 Training loss: 0.42562\n",
      "Epoch: 9/9 Iteration: 2323 Training loss: 0.92068\n",
      "Epoch: 9/9 Iteration: 2324 Training loss: 0.91210\n",
      "Epoch: 9/9 Iteration: 2325 Training loss: 0.91975\n",
      "Epoch: 9/9 Iteration: 2326 Training loss: 1.64749\n",
      "Epoch: 9/9 Iteration: 2327 Training loss: 1.01663\n",
      "Epoch: 9/9 Iteration: 2328 Training loss: 0.93637\n",
      "Epoch: 9/9 Iteration: 2329 Training loss: 1.06905\n",
      "Epoch: 9/9 Iteration: 2330 Training loss: 0.48674\n",
      "Epoch: 9/9 Iteration: 2331 Training loss: 0.44218\n",
      "Epoch: 9/9 Iteration: 2332 Training loss: 1.38349\n",
      "Epoch: 9/9 Iteration: 2333 Training loss: 0.95393\n",
      "Epoch: 9/9 Iteration: 2334 Training loss: 0.79790\n",
      "Epoch: 9/9 Iteration: 2335 Training loss: 1.38725\n",
      "Epoch: 9/9 Iteration: 2336 Training loss: 0.64814\n",
      "Epoch: 9/9 Iteration: 2337 Training loss: 0.95069\n",
      "Epoch: 9/9 Iteration: 2338 Training loss: 1.14325\n",
      "Epoch: 9/9 Iteration: 2339 Training loss: 0.88089\n",
      "Epoch: 9/9 Iteration: 2340 Training loss: 0.49660\n",
      "Epoch: 9/9 Iteration: 2341 Training loss: 1.16192\n",
      "Epoch: 9/9 Iteration: 2342 Training loss: 0.47694\n",
      "Epoch: 9/9 Iteration: 2343 Training loss: 0.88513\n",
      "Epoch: 9/9 Iteration: 2344 Training loss: 1.38035\n",
      "Epoch: 9/9 Iteration: 2345 Training loss: 1.12366\n",
      "Epoch: 9/9 Iteration: 2346 Training loss: 0.41088\n",
      "Epoch: 9/9 Iteration: 2347 Training loss: 1.07975\n",
      "Epoch: 9/9 Iteration: 2348 Training loss: 1.06643\n",
      "Epoch: 9/9 Iteration: 2349 Training loss: 0.55051\n",
      "Epoch: 9/9 Iteration: 2350 Training loss: 0.34057\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2350 Validation Acc: 0.7100\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2351 Training loss: 0.57304\n",
      "Epoch: 9/9 Iteration: 2352 Training loss: 0.87150\n",
      "Epoch: 9/9 Iteration: 2353 Training loss: 1.90903\n",
      "Epoch: 9/9 Iteration: 2354 Training loss: 0.77001\n",
      "Epoch: 9/9 Iteration: 2355 Training loss: 0.46131\n",
      "Epoch: 9/9 Iteration: 2356 Training loss: 0.99265\n",
      "Epoch: 9/9 Iteration: 2357 Training loss: 0.50189\n",
      "Epoch: 9/9 Iteration: 2358 Training loss: 1.07323\n",
      "Epoch: 9/9 Iteration: 2359 Training loss: 0.50199\n",
      "Epoch: 9/9 Iteration: 2360 Training loss: 1.33059\n",
      "Epoch: 9/9 Iteration: 2361 Training loss: 0.72412\n",
      "Epoch: 9/9 Iteration: 2362 Training loss: 1.91291\n",
      "Epoch: 9/9 Iteration: 2363 Training loss: 1.25852\n",
      "Epoch: 9/9 Iteration: 2364 Training loss: 1.03321\n",
      "Epoch: 9/9 Iteration: 2365 Training loss: 1.24417\n",
      "Epoch: 9/9 Iteration: 2366 Training loss: 0.70974\n",
      "Epoch: 9/9 Iteration: 2367 Training loss: 0.86621\n",
      "Epoch: 9/9 Iteration: 2368 Training loss: 0.92799\n",
      "Epoch: 9/9 Iteration: 2369 Training loss: 0.13722\n",
      "Epoch: 9/9 Iteration: 2370 Training loss: 1.22162\n",
      "Epoch: 9/9 Iteration: 2371 Training loss: 0.80652\n",
      "Epoch: 9/9 Iteration: 2372 Training loss: 1.55386\n",
      "Epoch: 9/9 Iteration: 2373 Training loss: 0.68494\n",
      "Epoch: 9/9 Iteration: 2374 Training loss: 0.98992\n",
      "Epoch: 9/9 Iteration: 2375 Training loss: 1.48778\n",
      "Epoch: 9/9 Iteration: 2376 Training loss: 0.88951\n",
      "Epoch: 9/9 Iteration: 2377 Training loss: 0.72085\n",
      "Epoch: 9/9 Iteration: 2378 Training loss: 0.64610\n",
      "Epoch: 9/9 Iteration: 2379 Training loss: 0.88419\n",
      "Epoch: 9/9 Iteration: 2380 Training loss: 0.52050\n",
      "Epoch: 9/9 Iteration: 2381 Training loss: 1.14383\n",
      "Epoch: 9/9 Iteration: 2382 Training loss: 0.89021\n",
      "Epoch: 9/9 Iteration: 2383 Training loss: 0.94663\n",
      "Epoch: 9/9 Iteration: 2384 Training loss: 0.73120\n",
      "Epoch: 9/9 Iteration: 2385 Training loss: 0.54579\n",
      "Epoch: 9/9 Iteration: 2386 Training loss: 1.71121\n",
      "Epoch: 9/9 Iteration: 2387 Training loss: 0.64269\n",
      "Epoch: 9/9 Iteration: 2388 Training loss: 0.95626\n",
      "Epoch: 9/9 Iteration: 2389 Training loss: 0.55351\n",
      "Epoch: 9/9 Iteration: 2390 Training loss: 1.17015\n",
      "Epoch: 9/9 Iteration: 2391 Training loss: 0.24529\n",
      "Epoch: 9/9 Iteration: 2392 Training loss: 1.60495\n",
      "Epoch: 9/9 Iteration: 2393 Training loss: 1.35909\n",
      "Epoch: 9/9 Iteration: 2394 Training loss: 0.68278\n",
      "Epoch: 9/9 Iteration: 2395 Training loss: 1.56892\n",
      "Epoch: 9/9 Iteration: 2396 Training loss: 1.23464\n",
      "Epoch: 9/9 Iteration: 2397 Training loss: 0.46663\n",
      "Epoch: 9/9 Iteration: 2398 Training loss: 1.55098\n",
      "Epoch: 9/9 Iteration: 2399 Training loss: 0.92458\n",
      "***\n",
      "Epoch: 9/9 Iteration: 2399 Validation Acc: 0.6567\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 10\n",
    "iteration = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for x, y in batch_creator(train_x, train_y, batch_size=batch_size, flatted=True):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 0.5\n",
    "                   }\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs - 1),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training loss: {:.5f}\".format(loss))\n",
    "            \n",
    "            if iteration % 50 == 0 or iteration == (train_x.shape[0] - 1):\n",
    "                feed = {inputs_: val_x,\n",
    "                        labels_: val_y,\n",
    "                        keep_prob: 1.0\n",
    "                       }\n",
    "                val_acc = sess.run(accuracy, feed_dict=feed)\n",
    "                print(\"***\\nEpoch: {}/{}\".format(e, epochs - 1),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation Acc: {:.4f}\\n***\".format(val_acc))\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    saver.save(sess, \"checkpoints/svhn.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 68.33%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y,\n",
    "            keep_prob: 1.0\n",
    "           }\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:3.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_idx = np.random.randint(0, labels.shape[0])\n",
    "sample_image = features[:,:,:,sample_image_idx]\n",
    "sample_label = labels[sample_image_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAAOzCAYAAAAROpOjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12762fe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample_images(features, labels, 1, test_data=sample_image_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    resized_image = resize_images(sample_image[:,:,:,np.newaxis])\n",
    "    feed_dict = {images: resized_image}\n",
    "    code = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: code, keep_prob: 1.0}\n",
    "    prediction = sess.run(predicted, feed_dict=feed).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhjoktSI/wP1/T7xn1aCawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a145e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(np.arange(10), prediction)\n",
    "plt.title('True {} | Predicted {}'.format(sample_label[0], np.argmax(prediction) + 1))\n",
    "plt.yticks(np.arange(10), np.unique(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
